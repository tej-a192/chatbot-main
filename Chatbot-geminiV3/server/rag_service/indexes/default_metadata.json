[
  {
    "content": "C h a p t e r 6\nD e e p F e e d f orw ard N e t w orks\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.\nThegoalofafeedforwardnetworkistoapproximatesomefunction f∗.Forexample,\nforaclassiﬁer, y= f∗(x)mapsaninputxtoacategory y.Afeedforwardnetwork\ndeﬁnesamappingy= f(x;θ)andlearnsthevalueoftheparametersθthatresult\ninthebestfunctionapproximation.\nThesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "functionbeingevaluatedfromx,throughtheintermediate computations usedto\ndeﬁne f,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhich\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\nnetworks,presentedinchapter.10\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "example,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\nlanguageapplications.\nFeedforwardneuralnetworksarecallednetworksbecausetheyaretypically\nrepresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "together.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheﬁrst\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\n168",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\nthename“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalled\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f∗(x).\nThetrainingdataprovidesuswithnoisy,approximateexamplesof f∗(x) evaluated\natdiﬀerenttrainingpoints.Eachexamplexisaccompanied byalabel y f≈∗(x).\nThetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "x;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\nnotdirectlyspeciﬁedbythetrainingdata. Thelearningalgorithmmustdecide\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\ndecidehowtousetheselayerstobestimplementanapproximation of f∗.Because\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\nlayersarecalledhiddenlayers.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "layersarecalledhiddenlayers.\nFinally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\nRatherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\nactivationvalue. Theideaofusingmanylayersofvector-valuedrepresentation\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\ntheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsabout\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "goalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\nknowaboutthebrain,ratherthanasmodelsofbrainfunction.\nOnewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\nandconsiderhowtoovercometheirlimitations. Linearmodels,suchaslogistic\nregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃciently",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "andreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\nToextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\nthelinearmodelnottoxitselfbuttoatransformedinput φ(x),where φisa\n1 6 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\nthe φmapping.Wecanthinkof φasprovidingasetoffeaturesdescribingx,or\nasprovidinganewrepresentationfor.x\nThequestionisthenhowtochoosethemapping. φ\n1.Oneoptionistouseaverygeneric φ,suchastheinﬁnite-dimens ional φthat\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel. If φ(x)is",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "ofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthe\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\nproblems.\n2.Anotheroptionistomanuallyengineer φ.Untiltheadventofdeeplearning,\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\neﬀortfor eachseparate task, withpractitioners specializing in diﬀerent",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "domainssuchasspeech recognition or computer vision, and with little\ntransferbetweendomains.\n3.Thestrategyofdeeplearningistolearn φ.Inthisapproach,wehaveamodel\ny= f(x;θw ,) = φ(x;θ)w.Wenowhaveparametersθthatweusetolearn\nφfromabroadclassoffunctions,andparameterswthatmapfrom φ(x)to\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\nφdeﬁningahiddenlayer. Thisapproachistheonlyoneofthethreethat\ngivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweigh",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "theharms.Inthisapproach,weparametrizetherepresentationas φ(x;θ)\nandusetheoptimization algorithmtoﬁndtheθthatcorrespondstoagood\nrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrst\napproachbybeinghighlygeneric—wedosobyusingaverybroadfamily\nφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach.\nHumanpractitioners canencodetheirknowledgetohelpgeneralization by\ndesigningfamilies φ(x;θ)thattheyexpectwillperformwell.Theadvantage",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "isthatthehumandesigneronlyneedstoﬁndtherightgeneralfunction\nfamilyratherthanﬁndingpreciselytherightfunction.\nThisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.\nFeedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\n1 7 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmappingsfromxtoythatlackfeedbackconnections. Othermodelspresented\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector.\nWebeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\nFirst,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "decisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "layersshould beconnectedto each other, and howmanyunitsshould bein\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\nmoderngeneralizations ,whichcanbeusedtoeﬃcientlycomputethesegradients.\nFinally,weclosewithsomehistoricalperspective.\n6. 1 E x am p l e: L earni n g X O R\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "theXORfunction.\nTheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues, x 1\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\ny= f∗(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;θ)and\nourlearningalgorithmwilladapttheparametersθtomake fassimilaraspossible\nto f∗.\nInthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0],[0 ,1],\n[1 ,0],and[1 ,1]}. Wewilltrainthenetworkonallfourofthesepoints. The\nonlychallengeistoﬁtthetrainingset.\nWecantreatthisproblemasaregressionproblemanduseameansquared\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\n1 7 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\naredescribedinsection.6.2.2.2\nEvaluatedonourwholetrainingset,theMSElossfunctionis\nJ() =θ1\n4\nx∈ X( f∗() (;))x− fxθ2. (6.1)\nNowwemustchoosetheformofourmodel, f(x;θ).Supposethatwechoose\nalinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobe θw b\nf , b (;xw) = xw+ b . (6.2)\nWecanminimize J(θ)inclosedformwithrespecttowand busingthenormal\nequations.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "equations.\nAftersolvingthenormalequations,weobtainw= 0and b=1\n2. Thelinear\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\nthisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhicha\nlinearmodelisabletorepresentthesolution.\nSpeciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithone\nhiddenlayercontainingtwohiddenunits.Seeﬁgureforanillustrationof 6.2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "f , , , b f (;xWcw) = ( 2 )( f( 1 )())x .\nWhatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =Wx\nand f( 2 )(h) =hw.Then f(x) =wWx.Wecouldrepresentthisfunctionas\nf() = xxwwherew= Ww.\nClearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "networksdosousinganaﬃnetransformationcontrolledbylearnedparameters,\nfollowedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethat\nstrategyhere,bydeﬁningh= g(Wx+c) ,whereWprovidestheweightsofa\nlineartransformationandcthebiases.Previously,todescribealinearregression\n1 7 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 1\nx 101x 2O r i g i n a l s p a c e x\n0 1 2\nh 101h 2L e a r n e d s p a c e h\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\n( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\nfunction.When x1= 0,themodel’soutputmustincreaseas x2increases.When x1= 1,\nthemodel’soutputmustdecreaseas x 2increases.Alinearmodelmustapplyaﬁxed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "coeﬃcient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\nthecoeﬃcienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\nmappedbothx= [1 ,0]andx= [0 ,1]toasinglepointinfeaturespace,h= [1 ,0].",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Thelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2.\nInthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\ncapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned\nrepresentationscanalsohelpthemodeltogeneralize.\n1 7 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nyy\nhh\nx xWwyy\nh 1 h 1\nx 1 x 1h 2 h 2\nx 2 x 2\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph.\nThisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "eachentirevectorrepresentingalayer’sactivations. Thisstyleismuchmorecompact.\nSometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\nofdrawing.\nmodel,weusedavectorofweightsandascalarbiasparametertodescribean",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "aﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\nanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbias\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\nthatisappliedelement-wise,with h i= g(xW : , i+ c i).Inmodernneuralnetworks,\nthedefaultrecommendation istousetherectiﬁedlinearunitorReLU(Jarrett\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)deﬁnedbytheactivation 2011a\nfunction depictedinﬁgure. g z , z () = max0{} 6.3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "function depictedinﬁgure. g z , z () = max0{} 6.3\nWecannowspecifyourcompletenetworkas\nf , , , b (;xWcw) = wmax0{ ,Wxc+}+ b . (6.3)\nWecannowspecifyasolutiontotheXORproblem.Let\nW=11\n11\n, (6.4)\nc=\n0\n−1\n, (6.5)\n1 7 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0\nz0g z ( ) = m a x 0{ , z}\nFigure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefault\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\nHowever,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\nfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,they",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "preservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\ncomplicatedsystemsfromminimalcomponents. MuchasaTuringmachine’smemory\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\nfromrectiﬁedlinearfunctions.\n1 7 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nw=1\n−2\n, (6.6)\nand. b= 0\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.\nLetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\nwithoneexampleperrow:\nX=\n00\n01\n10\n11\n. (6.7)\nTheﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrst\nlayer’sweightmatrix:\nXW=\n00\n11\n11\n22\n. (6.8)\nNext,weaddthebiasvector,toobtainc\n\n0 1−\n10\n10\n21\n. (6.9)\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\nAlinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalue\nofforeachexample,weapplytherectiﬁedlineartransformation: h\n\n00\n10\n10\n21\n. (6.10)\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\nlongerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea 6.1\nlinearmodelcansolvetheproblem.\nWeﬁnishbymultiplyingbytheweightvector:w\n\n0\n1\n1\n0\n. (6.11)\n1 7 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\nInthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained\nzeroerror. Inarealsituation,theremightbebillionsofmodelparametersand\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\nhere.Instead,agradient-basedoptimization algorithmcanﬁndparametersthat\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "globalminimumofthelossfunction,sogradientdescentcouldconvergetothis\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\ndescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsonthe\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\nﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\nhere.\n6. 2 Gradi en t - Bas e d L earni n g\nDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtrainingany",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "othermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\nacostfunction,andamodelfamily.\nThelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneural\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "functiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\nconvergesstartingfromanyinitialparameters(intheory—inpracticeitisvery\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "importanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\nparticulardiscussedinsection.Forthemoment,itsuﬃcestounderstandthat 8.4\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\ncostfunctioninonewayoranother. The speciﬁcalgorithmsareimprovements",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "andreﬁnementsontheideasofgradientdescent,introducedinsection,and,4.3\n1 7 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescent\nalgorithm,introducedinsection.5.9\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\nmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightly\nmorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "Sectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm.\nAswithothermachinelearningmodels,toapplygradient-basedlearningwe\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\ntheneuralnetworksscenario.\n6.2.1CostFunctions\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "costfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\nthesameasthoseforotherparametricmodels,suchaslinearmodels.\nInmostcases,ourparametricmodeldeﬁnesadistribution p(yx|;θ)and\nwesimplyuse theprinciple ofmaximumlikelihood.Thismeansweusethe\ncross-entropybetweenthetrainingdataandthemodel’spredictionsasthecost\nfunction.\nSometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "on.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "describedinchapter.7\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\n1 7 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\ncostfunctionisgivenby\nJ() = θ − E x y ,∼ ˆ pdatalog p m o de l( )yx| . (6.12)\nThespeciﬁcformofthecostfunctionchangesfrommodeltomodel,depending\nonthespeciﬁcformoflog p m o de l.Theexpansionoftheaboveequationtypically\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;θ) ,I),\nthenwerecoverthemeansquarederrorcost,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "thenwerecoverthemeansquarederrorcost,\nJ θ() =1\n2E x y ,∼ ˆ pdata||− ||y f(;)xθ2+const , (6.13)\nuptoascalingfactorof1\n2andatermthatdoesnotdependon.Thediscardedθ\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)xθ",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Anadvantageofthisapproachofderivingthecostfunctionfrommaximum\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\nSpecifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).\nOnerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\nforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)undermine\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\nhiddenunitsortheoutputunitssaturate. Thenegativelog-likelihoodhelpsto\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\nsection.6.2.2\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "likelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\n1 7 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\ncross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribed\ninchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso 7\nthatthemodelcannotreapunlimitedrewardinthisway.\n6.2.1.2LearningConditionalStatistics",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "6.2.1.2LearningConditionalStatistics\nInsteadoflearningafullprobabilitydistribution p(yx|;θ)weoftenwanttolearn\njustoneconditionalstatisticofgiven.yx\nForexample,wemayhaveapredictor f(x;θ) thatwewishtopredictthemean\nof.y\nIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneural\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\nratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,we",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "canviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters.\nWecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.\nSolvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "toolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\nusedtoderivethefollowingtworesults.\nOurﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\ntionproblem\nf∗= argmin\nfE x y ,∼ pdata||− ||y f()x2(6.14)\nyields\nf∗() = x E y∼ pdata ( ) y x|[]y , (6.15)\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "couldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\nmeanofforeachvalueof. y x\n1 8 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusing\ncalculusofvariationsisthat\nf∗= argmin\nfE x y ,∼ pdata||− ||y f()x 1 (6.16)\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\nfunctioniscommonlycalled . meanabsoluteerror\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "saturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\nThisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\nentiredistribution. p( )yx|\n6.2.2OutputUnits\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\nmodeldistribution. Thechoiceofhowtorepresenttheoutputthendetermines\ntheformofthecross-entropyfunction.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "theformofthecross-entropyfunction.\nAnykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\nsetofhiddenfeaturesdeﬁnedbyh= f(x;θ).Theroleoftheoutputlayeristhen\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thatthenetworkmustperform.\n6.2.2.1LinearUnitsforGaussianOutputDistributions\nOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformation\nwithnononlinearity.Theseareoftenjustcalledlinearunits.\nGivenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=Wh+b.\nLinearoutputlayersareoftenusedtoproducethemeanofaconditional\nGaussiandistribution:\np( ) = (;yx| NyˆyI ,) . (6.17)\n1 8 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\nerror.\nThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\ndeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinear\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\nBecauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\nalgorithms.\n6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\nManytasksrequirepredictingthevalueofabinaryvariable y.Classiﬁcation\nproblemswithtwoclassescanbecastinthisform.\nThemaximum-likelihoodapproachistodeﬁneaBernoullidistributionover y\nconditionedon.x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "conditionedon.x\nABernoullidistributionisdeﬁnedbyjustasinglenumber.Theneuralnet\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\nmustlieintheinterval[0,1].\nSatisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposewewere\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\nP y(= 1 ) = max |x\n0min ,\n1 ,wh+ b\n.(6.18)\nThiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeable\ntotrainitveryeﬀectivelywithgradientdescent.Anytimethatwh+ bstrayed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "outsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\nparameters.\nInstead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysa\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood.\nAsigmoidoutputunitisdeﬁnedby\nˆ y σ= \nwh+ b\n(6.19)\n1 8 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwhereisthelogisticsigmoidfunctiondescribedinsection. σ 3.10\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\nusesalinearlayertocompute z=wh+ b.Next,itusesthesigmoidactivation\nfunctiontoconvertintoaprobability. z\nWeomitthedependenceonxforthemomenttodiscusshowtodeﬁnea\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\nbyconstructinganunnormalized probabilitydistribution˜ P( y),whichdoesnot",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "sumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\nprobabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution\ncontrolledbyasigmoidaltransformationof: z\nlog˜ P y y z () = (6.20)\n˜ P y y z () = exp() (6.21)\nP y() =exp() y z1\ny= 0exp( yz)(6.22)\nP y σ y z . () = ((2−1)) (6.23)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "P y σ y z . () = ((2−1)) (6.23)\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\nthroughoutthestatisticalmodelingliterature.The zvariabledeﬁningsucha\ndistributionoverbinaryvariablesiscalleda.logit\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\nlikelihoodis−log P( y|x),theloginthecostfunctionundoestheexpofthe\nsigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "based learningfrom makinggoodprogress.Theloss functionfor maximum\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\nJ P y () = logθ − (|x) (6.24)\n= log((2 1)) − σ y− z (6.25)\n= ((12)) ζ − y z . (6.26)\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\n(1−2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\nhastherightanswer—when y= 1and zisverypositive,or y= 0and zisvery",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "negative.When zhasthewrongsign,theargumenttothesoftplusfunction,\n1 8 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n(1−2 y) z,maybesimpliﬁedto|| z.As|| zbecomeslargewhile zhasthewrongsign,\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\ncorrectamistaken. z\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "saturateanytime σ( z)saturates.Thesigmoidactivationfunctionsaturatesto0\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.\nThegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\noutputunits.\nAnalytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,because",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "thesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\nfunctionof z,ratherthanasafunctionofˆ y= σ( z).Ifthesigmoidfunction\nunderﬂowstozero,thentakingthelogarithmofˆ yyieldsnegativeinﬁnity.\n6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "with npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\ndistributionoverabinaryvariable.\nSoftmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresent\ntheprobabilitydistributionover ndiﬀerentclasses.Morerarely,softmaxfunctions\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\nndiﬀerentoptionsforsomeinternalvariable.\nInthecaseofbinaryvariables,wewishedtoproduceasinglenumber",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "ˆ y P y . = (= 1 )|x (6.27)\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\nthelog-likelihood,wechosetoinsteadpredictanumber z=log˜ P( y=1|x).\nExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\nsigmoidfunction.\n1 8 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\ntoproduceavectorˆy,with ˆ y i= P( y= i|x).Werequirenotonlythateach\nelementofˆ y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\nlayerpredictsunnormalized logprobabilities:\nzW= hb+ , (6.28)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "zW= hb+ , (6.28)\nwhere z i=log˜ P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\nnormalizetoobtainthedesired z ˆy.Formally,thesoftmaxfunctionisgivenby\nsoftmax()z i=exp( z i)\njexp( z j). (6.29)\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deﬁningthe\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\ntheofthesoftmax: exp",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "theofthesoftmax: exp\nlogsoftmax()z i= z i−log\njexp( z j) . (6.30)\nTheﬁrsttermofequationshowsthattheinput 6.30 z ialwayshasadirect\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst 6.30\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\npusheddown.Togainsomeintuitionforthesecondterm,log\njexp( z j),observe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "jexp( z j),observe\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\nbasedontheideathatexp( z k) isinsigniﬁcantforany z kthatisnoticeablylessthan\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\nthe− z itermandthelog\njexp( z j)≈max j z j= z itermswillroughlycancel.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "jexp( z j)≈max j z j= z itermswillroughlycancel.\nThisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\ndominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.\nSofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\n1 8 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\nsoftmax((;))zxθ i≈m\nj = 1 1y() j= i , x() j= xm\nj = 1 1x() j = x. (6.31)\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\nmodelisonlyabletoapproximatethesefractions.\nManyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "withthesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogto\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\noutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\nthesoftmaxfunctionitself.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "thesoftmaxfunctionitself.\nLikethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\noutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecome\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.\nToseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\ninputs:\nsoftmax() = softmax(+) zz c . (6.32)\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\nsoftmax() = softmax( max zz−\niz i) . (6.33)\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\nbytheamountthatitsargumentsdeviatefrommax i z i.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "bytheamountthatitsargumentsdeviatefrommax i z i.\nAnoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\n1 8 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedto\ncompensateforit.\nTheargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways.\nThemostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\neveryelementofz,asdescribedaboveusingthelinearlayerz=Wh+b.While\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\nconstraintthatthe noutputsmustsumtomeansthatonly 1 n−1parametersare",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "necessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\nﬁrst n−1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\nofzbeﬁxed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\nwhatthesigmoidunitdoes.Deﬁning P( y= 1|x) = σ( z)isequivalenttodeﬁning\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n−1\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\nsetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "thereisrarelymuchdiﬀerencebetweenusingtheoverparametrized versionorthe\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.\nFromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxas\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "extreme(whenthediﬀerencebetweenthemaximal a iandtheothersislargein\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\nandtheothersarenearly0).\nThename“softmax”canbesomewhatconfusing.Thefunctionismoreclosely\nrelatedtotheargmaxfunctionthanthemaxfunction. Theterm“soft”derives\nfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable. The\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\nordiﬀerentiable. Thesoftmaxfunctionthusprovidesa“softened”versionofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "argmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)z.\nItwouldperhapsbebettertocallthesoftmaxfunction“softargmax,” butthe\ncurrentnameisanentrenchedconvention.\n6.2.2.4OtherOutputTypes\nThelinear, sigmoid, andsoftmaxoutputunitsdescribedabovearethemost\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\n1 8 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nagoodcostfunctionfornearlyanykindofoutputlayer.\nIngeneral,ifwedeﬁneaconditionaldistribution p(yx|;θ),theprincipleof\nmaximumlikelihoodsuggestsweuse asourcostfunction. − | log( pyxθ;)\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;θ).\nTheoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\nf(x;θ) =ωprovidestheparametersforadistributionover y.Ourlossfunction\ncanthenbeinterpretedas . −log(;()) p yωx",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "canthenbeinterpretedas . −log(;()) p yωx\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\ngiven x.Inthesimplecase,wherethevariance σ2isaconstant,thereisaclosed\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\nempiricalmeanofthesquareddiﬀerencebetweenobservations yandtheirexpected\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "distribution p( y|x)thatiscontrolledbyω= f(x;θ).Thenegativelog-likelihood\n−log p(y;ω(x))willthenprovideacostfunctionwiththeappropriateterms\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\ncanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnew\nparametermightbe σitselforcouldbeaparameter vrepresenting σ2oritcould\nbeaparameter βrepresenting1\nσ2,dependingonhowwechoosetoparametrize",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "σ2,dependingonhowwechoosetoparametrize\nthedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvariance\nin yfordiﬀerentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\nheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneof\nthevaluesoutputby f( x;θ).AtypicalwaytodothisistoformulatetheGaussian\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\ndiag (6.34) ()β .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "diag (6.34) ()β .\nThisformulationworkswellwithgradientdescentbecausetheformulaforthe\nlog-likelihoodoftheGaussiandistributionparametrized byβinvolvesonlymul-\ntiplicationby β iandadditionoflogβ i.Thegradientofmultiplication, addition,\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "arbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\ncanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared.\n1 8 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\nensurethatthecovariancematrixoftheGaussianispositivedeﬁnite. Because\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\npositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "Ifwesupposethataistherawactivationofthemodelusedtodeterminethe\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\nvector:β= ζ(a) .Thissamestrategyappliesequallyifusingvarianceorstandard\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\ndiagonalmatrix.\nItisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\ndiagonal. Ifthecovarianceisfullandconditional,thenaparametrization must",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "bechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.\nThiscanbeachievedbywriting Σ() = ()xBxB()x,whereBisanunconstrained\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\nlikelihoodisexpensive,witha d d×matrixrequiring O( d3)computationforthe\ndeterminantandinverseof Σ(x)(orequivalently,andmorecommonlydone,its\neigendecompositionorthatof).Bx()\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "thatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiﬀerent\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\nanaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisdeﬁnedby\ntheconditionalprobabilitydistribution\np( ) =yx|n\ni = 1p i (= c |Nx)(;yµ( ) i()x , Σ( ) i())x .(6.35)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "Theneuralnetworkmusthavethreeoutputs:avectordeﬁning p(c= i|x),a\nmatrixprovidingµ( ) i(x)forall i,andatensorproviding Σ( ) i(x)forall i.These\noutputsmustsatisfydiﬀerentconstraints:\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\noverthe ndiﬀerentcomponentsassociatedwithlatentvariable1c,andcan\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "y , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\nra n d o m v a ria b l e .\n1 8 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\nthattheseoutputsarepositiveandsumto1.\n2.Meansµ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\nan n d×matrixcontainingall nofthese d-dimensionalvectors. Learning\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "learningthemeansofadistributionwithonlyoneoutputmode.Weonly\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\neachexample’scontributiontothelossforeachcomponentbytheprobability\nthatthecomponentproducedtheexample.\n3.Covariances Σ( ) i(x):thesespecifythecovariancematrixforeachcomponent\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "matrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\nspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel.\nIthasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "getsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients).\nOnesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\nthegradientsheuristically( ,). MurrayandLarochelle2014\nGaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsof\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "modesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\ndensitynetworkisshowninﬁgure.6.4\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\nthatformsasentence.Inthese cases,wemaycontinuetousetheprinciple\nofmaximumlikelihoodappliedtoourmodel p(y;ω(x)),butthemodelweuse",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "1 9 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nxy\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\nTheinput xissampledfromauniformdistributionandtheoutput yissampledfrom\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "predictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.\nChapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels 10\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\nprobabilitydistributions.\n6. 3 Hi d d en Un i t s\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "arecommontomostparametricmachinelearningmodelstrainedwithgradient-\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\nmodel.\nThedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\nyethavemanydeﬁnitiveguidingtheoreticalprinciples.\nRectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\ntypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentouse",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "whichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice). We\n1 9 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.\nTheseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\nonavalidationset.\nSomeofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "allinputpoints.Forexample,therectiﬁedlinearfunction g( z) =max{0 , z}isnot\ndiﬀerentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\nforthesemodelstobeusedformachinelearningtasks. Thisisinpartbecause\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\nthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshownin\nﬁgure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "expecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\nfortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.\nHiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiable atonlya\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeﬁned\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\ndeﬁnedbytheslopeofthefunctionimmediately totherightof z.Afunction\nisdiﬀerentiableat zonlyifboththeleftderivativeandtherightderivativeare",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "deﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneural\nnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthe\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\ntheone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedor\nraisinganerror. Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "Whenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 thatwasrounded\nto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but 0\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\ninpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunit\nactivationfunctionsdescribedbelow.\nUnlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "avectorofinputsx,computinganaﬃnetransformationz=Wx+b,and\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\nfunction. g()z\n1 9 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.1RectiﬁedLinearUnitsandTheirGeneralizations\nRectiﬁedlinearunitsusetheactivationfunction . g z , z () = max0{}\nRectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\nunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitis\nthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain. This makesthe\nderivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive.\nThegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "rectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\nthatintroducesecond-ordereﬀects.\nRectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:\nhW= ( gxb+) . (6.36)\nWheninitializingtheparametersoftheaﬃnetransformation,itcanbeagood\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "itverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputs\ninthetrainingsetandallowthederivativestopassthrough.\nSeveralgeneralizations ofrectiﬁedlinearunitsexist.Mostofthesegeneral-\nizationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperform\nbetter.\nOnedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-\nbased methods onexamples for which their activ ation iszero.Avariety of\ngeneralizations ofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-\nwhere.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "where.\nThreegeneralizations ofrectiﬁedlinearunitsarebasedonusinganon-zero\nslope α iwhen z i <0: h i= g(zα ,) i=max(0 , z i)+ α imin(0 , z i).Absolutevalue\nrectiﬁcationﬁxes α i=−1toobtain g( z) =|| z.Itisusedforobjectrecognition\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\nofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "2013)ﬁxes α itoasmallvaluelike0.01whileaparametricReLUorPReLU\ntreats α iasalearnableparameter(,). He e t a l .2015\nMaxoutunits( ,)generalizerectiﬁedlinearunits Goodfellow e t a l .2013a\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\n1 9 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\noneofthesegroups:\ng()z i=max\nj∈ G() iz j (6.37)\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( i−1) k+1 , . . . , i k}.\nThisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\ndirectionsintheinputspace.x\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces.\nMaxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "learntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\ninputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolute\nvaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearnto\nimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcourse\nbeparametrized diﬀerentlyfromanyoftheseotherlayertypes,sothelearning\ndynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "samefunctionofasoneoftheotherlayertypes. x\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\nsomaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.They\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\npiecesperunitiskeptlow(,). Cai e t a l .2013\nMaxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-\ntisticalandcomputational advantagesbyrequiringfewerparameters.Speciﬁcally,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "ifthefeaturescapturedby ndiﬀerentlinearﬁlterscanbesummarizedwithout\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\nlayercangetbywithtimesfewerweights. k\nBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\nthepast( ,). Goodfellow e t a l .2014a\nRectiﬁedlinearunitsandallofthesegeneralizations ofthemarebasedonthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "principlethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\nThissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "1 9 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\narchitectures,theLSTM,propagatesinformationthroughtimeviasummation—a\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\ninsection.10.10\n6.3.2LogisticSigmoidandHyperbolicTangent\nPriortotheintroduction ofrectiﬁedlinearunits,mostneuralnetworksusedthe\nlogisticsigmoidactivationfunction\ng z σ z () = () (6.38)\northehyperbolictangentactivationfunction\ng z z . () = tanh( ) (6.39)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "g z z . () = tanh( ) (6.39)\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z σ z−\nWe havealready seen sigmoid unitsasoutput units, usedto predictthe\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\nunitssaturateacrossmostoftheirdomain—they saturatetoahighvaluewhen\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "sigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\nlayer.\nWhenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while σ(0) =1\n2.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "2.\nBecausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\nnetworkˆ y=wtanh(Utanh(Vx))resemblestrainingalinearmodelˆ y=\nwUVxsolongastheactivationsofthenetworkcanbekeptsmall.This\nmakestrainingthenetworkeasier. tanh\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "linearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\ndrawbacksofsaturation.\n1 9 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.3OtherHiddenUnits\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.\nIngeneral,awidevarietyofdiﬀerentiable functionsperformperfectlywell.\nManyunpublishedactivationfunctionsperformjustaswellasthepopularones.\nToprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "functions.Duringresearchanddevelopmentofnewtechniques,itiscommon\ntotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationson\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcant\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\ntypesaresocommonastobeuninteresting.\nItwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "intheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\nOnepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "aneuralnetworklayerwith ninputsand poutputs,h= g(Wx+b).Wemay\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\nusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehave\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\nfactoredapproachistocomputeh= g(VUx+b).IfUproduces qoutputs,\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "comesatthecostofconstrainingthelineartransformationtobelow-rank,but\ntheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeran\neﬀectivewayofreducingthenumberofparametersinanetwork.\nSoftmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "unitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\nmanipulatememory,describedinsection.10.12\n1 9 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAfewotherreasonablycommonhiddenunittypesinclude:\n•RadialbasisfunctionorRBFunit: h i=exp\n−1\nσ2\ni||W : , i−||x2\n.This\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\nsaturatestoformost,itcanbediﬃculttooptimize. 0x\n•Softplus: g( a) = ζ( a) =log(1+ ea).Thisisasmoothversionoftherectiﬁer,\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "models. ()comparedthesoftplusandrectiﬁerandfound Glorot e t a l .2011a\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\nThesoftplusdemonstratesthattheperformanceofhiddenunittypescan\nbeverycounterintuitive—onemightexpectittohaveanadvantageover\ntherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatingless\ncompletely,butempiricallyitdoesnot.\n•Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlike\nthelatter,itisbounded, g( a)=max(−1 ,min(1 , a)).Itwasintroduced",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "by(). Collobert2004\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\nunittypesremaintobediscovered.\n6. 4 A rc h i t ec t u re D es i gn\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\nThewordarchitecturereferstotheoverallstructureofthenetwork:howmany\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\nMostneuralnetworksareorganizedintogroupsofunitscalledlayers. Most\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "layerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayer\nisgivenby\nh( 1 )= g( 1 )\nW( 1 )xb+( 1 )\n, (6.40)\nthesecondlayerisgivenby\nh( 2 )= g( 2 )\nW( 2 )h( 1 )+b( 2 )\n, (6.41)\nandsoon.\n1 9 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\nanetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deeper\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize. The\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\nmonitoringthevalidationseterror.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "monitoringthevalidationseterror.\n6.4.1UniversalApproximationPropertiesandDepth\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\nbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.\nAtﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\nmationframework.Speciﬁcally,theuniversalapproximationtheorem(Hornik\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\nlayerandatleastonehiddenlayerwithany“squashing”activationfunction(such\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\nfunctionfromoneﬁnite-dimensional spacetoanotherwithanydesirednon-zero\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "derivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\nisbeyondthescopeofthisbook; forourpurposesitsuﬃcestosaythatany\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\nalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespace\ntoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswith",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "activationfunctionsthatsaturatebothforverynegativeandforverypositive\narguments,universalapproximation theoremshavealsobeenprovedforawider\nclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinear\nunit( ,). Leshno e t a l .1993\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "to l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\ncanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining\n1 9 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesired\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\noverﬁtting.Recallfromsectionthatthe“nofreelunch”theoremshowsthat 5.2.1\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "isnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesand\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.\nTheuniversalapproximationtheoremsaysthatthereexistsanetworklarge\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions.\nUnfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "withonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobe\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\nnumberofpossiblebinaryfunctionsonvectorsv∈{0 ,1}nis22nandselecting\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\nfreedom.\nInsummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresent\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "numberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\namountofgeneralization error.\nThereexistfamiliesoffunctionswhichcanbeapproximated eﬃcientlybyan\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n. Suchresults\nwereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\nmodels.Theﬁrstresultswereforcircuitsoflogicgates(,).Later Håstad1986\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\n( ,; ,),andthentonetworkswith HåstadandGoldmann1991Hajnal e t a l .1993\ncontinuous-valuedactivations(,; ,). Manymodern Maass1992Maass e t a l .1994\nneuralnetworksuserectiﬁedlinearunits. ()demonstrated Leshno e t a l .1993",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\nincludingrectiﬁedlinearunits,haveuniversalapproximation properties,butthese\nresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythat\nasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufar e t a l .\n1 9 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\nMoreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\nfromrectiﬁernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\nanetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunction",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "computedontopofsomehiddenunit,withrespecttotheinputofthathidden\nunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreate\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "rectiﬁernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectiﬁcation\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\nofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.A\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "befoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\npermissionfrom (). Montufar e t a l .2014\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\nnumberoflinearregionscarvedoutbyadeeprectiﬁernetworkwith dinputs,\ndepth,andunitsperhiddenlayer,is l n\nOn\ndd l (− 1 )\nnd\n, (6.42)\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersper l k\nunit,thenumberoflinearregionsis\nO",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "unit,thenumberoflinearregionsis\nO\nk( 1 ) + l− d\n. (6.43)\n2 0 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\nWemayalsowanttochooseadeepmodelforstatisticalreasons. Anytime\nwechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsome\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "wanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\nmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput. These",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "intermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . ,;2014dSzegedy ,).Seeﬁgureandﬁgureforexamplesof 2014a 6.6 6.7\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\n6.4.2OtherArchitecturalConsiderations\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\nInpractice,neuralnetworksshowconsiderablymorediversity.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "Manyneuralnetworkarchitectures havebeendevelopedforspeciﬁctasks.\nSpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\nhavetheirownarchitecturalconsiderations.\nIngeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "architecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfrom\noutputlayerstolayersnearertheinput.\n2 0 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n3 4 5 6 7 8 9 1 0 1 1\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\ne t a l .(). Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See 2014d",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "ﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\ndonotyieldthesameeﬀect.\nAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "theoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\nthenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\nthatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃcult\ntogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneural\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "havebeenfoundtoworkwellfordiﬀerentapplicationdomains.\n2 0 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\nN u m b e r o f p a r a m e t e r s × 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\n3,fullyconnected\n11,convolutional\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "nearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\ncontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhaving\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\nthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "functionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\ncornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependent\nsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\nthem).\n2 0 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i ﬀ eren t i at i on A l go-\nri t h m s\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\noutput ˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovide\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\nandﬁnallyproduces ˆy.Thisiscalledforwardpropagation.Duringtraining,\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J(θ).",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "Theback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\nbackprop,allowstheinformationfromthecosttothenﬂowbackwardsthrough\nthenetwork,inordertocomputethegradient.\nComputingananalyticalexpressionforthegradientisstraightforward,but\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\nThetermback-propagation isoften misunders toodasmeaningthewhole",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\nFurthermore,back-propagation isoftenmisunderstoodasbeingspeciﬁctomulti-\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\nfunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "∇ x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\n∇ θ J(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,either\naspartof thelearning process, or to analyzethelearned model. The back-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\nusedcasewherehasasingleoutput. f\n2 0 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.1ComputationalGraphs\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\nTodescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\nmoreprecise language. computationalgraph\nManywaysofformalizingcomputationasgraphsarepossible.\nHere,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype.\nToformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\ntogether.\nWithoutlossofgenerality, wedeﬁneanoperationtoreturnonlyasingle\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "descriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\nconceptualunderstanding.\nIfavariable yiscomputedbyapplyinganoperationtoavariable x,then\nwedrawadirectededgefrom xto y. Wesometimesannotatetheoutputnode\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\noperationisclearfromcontext.\nExamplesofcomputational graphsareshowninﬁgure.6.8\n6.5.2ChainRuleofCalculus\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "usedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\nchainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.\nLet xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\nthechainrulestatesthatd z\nd x=d z\nd yd y\nd x. (6.44)\nWecangeneralizethisbeyondthescalarcase.Supposethatx∈ Rm,y∈ Rn,\n2 0 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxx yy\n( a)×\nx x ww\n( b)u( 1 )u( 1 )\nd o t\nbbu( 2 )u( 2 )\n+ˆ y ˆ y\nσ\n( c )XX WWU( 1 )U( 1 )\nm a t m u l\nb bU( 2 )U( 2 )\n+HH\nr e l u\nx x ww\n( d)ˆ yˆ y\nd o t\nλ λu( 1 )u( 1 )\ns q ru( 2 )u( 2 )\ns u mu( 3 )u( 3 )\n×\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) ×operationto\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b ) ˆ y= σ\nxw+ b\n.\nSomeoftheintermediateexpressionsdonothavenamesinthealgebraicexpression",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "butneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\nmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\nofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit ( d )\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\nweightsareusedtomakeboththepredictionˆ yandtheweightdecaypenalty λ",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "iw2\ni.\n2 0 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\n∂ z\n∂ x i=\nj∂ z\n∂ y j∂ y j\n∂ x i. (6.45)\nInvectornotation,thismaybeequivalentlywrittenas\n∇ x z=∂y\n∂x\n∇ y z , (6.46)\nwhere∂ y\n∂ xistheJacobianmatrixof. n m× g\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\naJacobianmatrix∂ y\n∂ xbyagradient∇ y z.Theback-propagation algorithmconsists\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\nsameasback-propagation withvectors.Theonlydiﬀerenceishowthenumbers\narearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensor\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\nback-propagationisstilljustmultiplyingJacobiansbygradients.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "Todenotethegradientofavalue zwithrespecttoatensor X,wewrite ∇ X z,\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates—for\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\npossibleindextuples i,(∇ X z) igives∂ z\n∂ X i.Thisisexactlythesameashowforall\npossibleintegerindices iintoavector,(∇ x z) igives∂ z\n∂ x i.Usingthisnotation,we",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "∂ x i.Usingthisnotation,we\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\n∇ X z=\nj(∇ X Y j)∂ z\n∂ Y j. (6.47)\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\nintroducessomeextraconsiderations.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "introducessomeextraconsiderations.\nSpeciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithinthe\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\n2 0 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\nﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\nbewasteful. Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible.\nInothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\nreducememoryconsumptionatthecostofhigherruntime.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "reducememoryconsumptionatthecostofhigherruntime.\nWeﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁesthe\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\ncomputations orviewthedescriptionofthealgorithmasasymbolicspeciﬁcation\nofthecomputational graphforcomputingtheback-propagation. However,this",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "formulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\nsymbolicgraphthatperformsthegradientcomputation. Such aformulationis\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\nnodesthatcontainarbitrarytensors.\nFirstconsideracomputational graphdescribinghowtocomputeasinglescalar\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ). In\notherwordswewishtocompute∂ u() n",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "otherwordswewishtocompute∂ u() n\n∂ u() iforall i∈{1 ,2 , . . . , n i}.Intheapplication\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\ncorrespondtotheparametersofthemodel.\nWewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\ngoingupto u( ) n.Asdeﬁnedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "operation f( ) iandiscomputedbyevaluatingthefunction\nu( ) i= ( f A( ) i) (6.48)\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.\nThatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecould\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\nthederivative∂ u() n",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "thederivative∂ u() n\n∂ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\n2 0 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\nu( 1 )to u( n i )toanoutput u( ) n.Thisdeﬁnesacomputational graphwhereeachnode\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a∈ ( u( ) i).The\ninputtothecomputational graphisthevectorx,andissetintotheﬁrst n inodes\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreadoﬀthelast(output)\nnode u( ) n.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "node u( ) n.\nfor i , . . . , n = 1 ido\nu( ) i← x i\nendfor\nfor i n= i+1 , . . . , ndo\nA( ) i←{ u( ) j|∈ j P a u(( ) i)}\nu( ) i← f( ) i( A( ) i)\nendfor\nreturn u( ) n\nusingthechainrulewithrespecttoscalaroutput u( ) n:\n∂ u( ) n\n∂ u( ) j=\ni j P a u :∈ (() i )∂ u( ) n\n∂ u( ) i∂ u( ) i\n∂ u( ) j(6.49)\nasspeciﬁedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\nthecomputationof∂ u() i",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "thecomputationof∂ u() i\n∂ u() j.Inaddition,adotproductisperformedforeachnode,\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\nof u( ) jandthevectorcontainingthepartialderivatives∂ u() i\n∂ u() jforthesamechildren\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "nodewithrespecttooneofitsparents)aswellasperformingonemultiplication\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\neﬃcientimplementations.\nTheback-propagationalgorithmisdesignedtoreducethenumberofcommon\nsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorder\nofoneJacobianproductpernodeinthegraph. Thiscanbeseenfromthefact\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "thegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂ u() i\n∂ u() j.\n2 0 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.2Simpliﬁedversionoftheback-propagation algorithmforcomputing\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\nintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariables\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ).\nThissimpliﬁedversioncomputesthederivativesofallnodesinthegraph. The\ncomputational costofthisalgorithmisproportional tothenumberofedgesin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "thegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\ntheforwardpropagation. Each∂ u() i\n∂ u() jisafunctionoftheparents u( ) jof u( ) i,thus\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\ngraph.\nRunforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\ntionsofthenetwork\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "beencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\n∂ u() n\n∂ u() i.\ng r a d t a b l e_ [ u( ) n] 1←\nfor do j n= −1downto1\nThenextlinecomputes∂ u() n\n∂ u() j=\ni j P a u :∈ (() i )∂ u() n\n∂ u() i∂ u() i\n∂ u() jusingstoredvalues:\ng r a d t a b l e_ [ u( ) j] ←\ni j P a u :∈ (() i ) g r a d t a b l e_ [ u( ) i]∂ u() i\n∂ u() j\nendfor\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\nsimpliﬁcationsonthecomputational graph,ormaybeabletoconservememoryby\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\nafterdescribingtheback-propagation algorithmitself.\n6.5.4Back-PropagationComputationinFully-ConnectedMLP\nToclarifytheabovedeﬁnitionoftheback-propagation computation,letusconsider\nthespeciﬁcgraphassociatedwithafully-connected multi-layerMLP.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "Algorithmﬁrstshowstheforwardpropagation, whichmapsparametersto 6.3\nthesupervisedloss L(ˆyy ,)associatedwithasingle(input,target) trainingexample\n( )xy ,,with ˆytheoutputoftheneuralnetworkwhenisprovidedininput. x\nAlgorithm  then shows thecorresponding computation to be donefor 6.4\n2 1 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfff\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\nthegradient.Let w∈ Rbetheinputtothegraph.Weusethesamefunction f: R R→\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).\nTocompute∂ z\n∂ w,weapplyequationandobtain: 6.44\n∂ z\n∂ w(6.50)\n=∂ z\n∂ y∂ y\n∂ x∂ x\n∂ w(6.51)\n= f() y f() x f() w (6.52)\n= f((())) f f w f(()) f w f() w (6.53)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "= f((())) f f w f(()) f w f() w (6.53)\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "runtime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\nusefulwhenmemoryislimited.\n2 1 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napplyingtheback-propagation algorithmtothisgraph.\nAlgorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\nstraightforwardtounderstand.However, theyarespecializedtoonespeciﬁc\nproblem.\nModernsoftwareimplementations arebasedonthegeneralizedformofback-\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\ncomputation.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "computation.\nAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\nthecomputationofthecostfunction.Theloss L(ˆyy ,)dependsontheoutput\nˆyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer Ω( θ),where θ\ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "useaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\nRequire:Networkdepth, l\nRequire:W( ) i, i , . . . , l , ∈{1 }theweightmatricesofthemodel\nRequire:b( ) i, i , . . . , l , ∈{1 }thebiasparametersofthemodel\nRequire:x,theinputtoprocess\nRequire:y,thetargetoutput\nh( 0 )= x\nfordo k , . . . , l = 1\na( ) k= b( ) k+W( ) kh( 1 ) k−\nh( ) k= ( fa( ) k)\nendfor\nˆyh= ( ) l\nJ L= (ˆyy ,)+Ω() λ θ\n6.5.5Symbol-to-SymbolDerivatives\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "variables thatdo not havespeciﬁc values.Thesealgebraic and graph-based\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\ntrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.We\nreplaceasymbolicinputtothenetworkxwithaspeciﬁcnumericvalue,suchas\n[123765 18] . , . ,− ..\n2 1 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\noutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,\nwhichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchange\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "gradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\ncomputed)orusedwithothergradient-basedoptimization methods.\nAftertheforwardcomputation,computethegradientontheoutputlayer:\ng←∇ ˆ y J= ∇ ˆ y L(ˆyy ,)\nfor do k l , l , . . . , = −1 1\nConvert thegradienton thelayer’s output into a gradient into thepre-\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\ng←∇a() k J f = g(a( ) k)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "g←∇a() k J f = g(a( ) k)\nComputegradientsonweightsandbiases(includingtheregularizationterm,\nwhereneeded):\n∇b() k J λ = +g ∇b() kΩ() θ\n∇W() k J= gh( 1 ) k−+ λ∇W() kΩ() θ\nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:\ng←∇h(1) k − J= W( ) kg\nendfor\n2 1 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfffz z\nxxyy\nw wfff\nd z\nd yd z\nd yf\nd y\nd xd y\nd xf\nd z\nd xd z\nd x×\nd x\nd wd x\nd wf\nd z\nd wd z\nd w×\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\nspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "derivativesforanyspeciﬁcnumericvalues. ( L e f t )Inthisexample,webeginwithagraph\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\nittoconstructthegraphfortheexpressioncorrespondingtod z\nd w.Inthisexample,wedo\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\nderivative.\nSomeapproachestoback-propagationtakeacomputational graphandaset",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "ofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-\nto-number”diﬀerentiation. ThisistheapproachusedbylibrariessuchasTorch\n( ,)andCaﬀe(,). Collobert e t a l .2011b Jia2013\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "andTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\nisillustratedinﬁgure.Theprimaryadvantageofthisapproachisthat 6.10\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.\nBecausethederivativesarejustanothercomputational graph,itispossibletorun\nback-propagationagain,diﬀerentiating thederivativesinordertoobtainhigher\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "2 1 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\ngraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.This\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\nInstead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\nparents’valuesareavailable.\nThedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "performingexactlythesamecomputations asaredoneinthegraphbuiltbythe\nsymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number\napproachdoesnotexposethegraph.\n6.5.6GeneralBack-Propagation\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\nthatthegradientwithrespectto zisgivenbyd z\nd z=1.Wecanthencompute\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "currentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\ntwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsat\nthatnode.\nMoreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\nmaximumgenerality,wedescribethisvariableasbeingatensor V. Tensorcan\ningeneralhaveanynumberofdimensions. Theysubsumescalars,vectors,and\nmatrices.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "matrices.\nWeassumethateachvariableisassociatedwiththefollowingsubroutines: V\n• g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\noperation,andtheget_operationfunction.Supposewehaveavariablethat\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\nreturnsapointertoaninstanceofthecorrespondingC++class.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "• g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\nVinthecomputational graph.G\n• G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\ninthecomputational graph.G\n2 1 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\nEachoperationisresponsibleforknowinghowtoback-propagate throughthe\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "ofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\nisresponsiblefordeﬁningtwoback-propagation rules,oneforeachofitsinput\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\nisgivenbyGB.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "b p r o pmethodandspecifyingthatthedesiredgradientisgivenbyAG.The\nback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiation rules.It\nonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,\no p b p r o p i n p u t s . ( , , X G)mustreturn\n\ni(∇ X o p f i n p u t s .( ) i) G i , (6.54)\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "mathematical functionthattheoperationimplements, Xistheinputwhosegradient\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\nderivativeon. x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "derivativeon. x\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\nanynewoperationsmanually.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "anynewoperationsmanually.\nTheback-propagationalgorithmisformallydescribedinalgorithm .6.5\n2 1 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\ninthe subroutineofalgorithm build_grad 6.6.\nRequire: T,thetargetsetofvariableswhosegradientsmustbecomputed.\nRequire:G,thecomputational graph\nRequire: z,thevariabletobediﬀerentiated\nLetGbeGprunedtocontainonlynodesthatareancestorsof zanddescendents\nofnodesin. T",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "ofnodesin. T\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\ng r a d t a b l e_ [] 1 z←\nfordo Vin T\nb u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )\nendfor\nReturn restrictedto grad_table T\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\nNowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstandits",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "computational cost.Ifweassumethateachoperationevaluationhasroughlythe\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "O( n2) operations.Herewearecountingoperationsinthecomputational graph,not\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\n2 1 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )of\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁned\ninalgorithm .6.5\nRequire: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table\nRequire:G,thegraphtomodify.\nRequire:G,therestrictionoftonodesthatparticipateinthegradient. G\nRequire:grad_table,adatastructuremappingnodestotheirgradients\nif then Visingrad_table\nReturn_ g r a d t a b l e[] V\nendif\ni←1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "Return_ g r a d t a b l e[] V\nendif\ni←1\nfor C V in_ g e t c o n s u m e r s( ,G)do\no p g e t o p e r a t i o n ←_ () C\nD C ← b u i l d g r a d_ ( , ,GG, g r a d t a b l e_ )\nG( ) i← G o p b p r o p g e t i n p u t s . (_ ( C ,) ) , , V D\ni i←+1\nendfor\nG←\ni G( ) i\ng r a d t a b l e_ [] = V G\nInsertandtheoperationscreatingitinto G G\nReturn G\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "nodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\ntherecursivechainrule(equation)non-recursively: 6.49\n∂ u( ) n\n∂ u( ) j=\npa t h ( u( π1), u( π2), . . . , u( π t)) ,\nf r o m π1 = t o j π t = nt\nk = 2∂ u( π k )\n∂ u( π k −1 ). (6.55)\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "graph.Thislargecostwouldbeincurredbecausethesamecomputationfor\n∂ u() i\n∂ u() jwouldberedonemanytimes. Toavoidsuchrecomputation, wecanthink\nofback-propagation asatable-ﬁllingalgorithmthattakesadvantageofstoring\nintermediateresults∂ u() n\n∂ u() i.Eachnodeinthegraphhasacorrespondingslotina\ntabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,\n2 1 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁlling\nstrategyissometimescalled . dynamicprogramming\n6.5.7Example:Back-PropagationforMLPTraining\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\ntrainamultilayerperceptron.\nHerewedevelopaverysimplemultilayerperceptionwithasinglehidden\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\nTheback-propagationalgorithmisusedtocomputethegradientofthecostona",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "singleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetraining\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.\nThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "operationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\ndistributiondeﬁnedbytheseunnormalized logprobabilities. Theresultingcross-\nentropydeﬁnesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\nlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,\nwealsoincludearegularizationterm.Thetotalcost\nJ J= M LE+ λ\n\ni , j\nW( 1 )\ni , j2\n+\ni , j\nW( 2 )\ni , j2\n (6.56)\nconsistsofthecross-entropyandaweightdecaytermwithcoeﬃcient λ.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "computational graphisillustratedinﬁgure.6.11\nThecomputational graphforthegradientofthisexampleislargeenoughthat\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁts\noftheback-propagation algorithm,whichisthatitcanautomatically generate\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\nderivemanually.\nWecanroughlytraceoutthebehavioroftheback-propagation algorithm\nbylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish 6.11",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "tocomputeboth∇W(1) Jand ∇W(2) J.Therearetwodiﬀerentpathsleading\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\nalwayscontribute 2 λW( ) itothegradientonW( ) i.\n2 1 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nXXW( 1 )W( 1 )U( 1 )U( 1 )\nm a t m u lHH\nr e l u\nU( 3 )U( 3 )\ns q ru( 4 )u( 4 )\ns u mλ λ u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\nm a t m u ly yJ M L E J M L E\nc r o s s _ e n t r o p y\nU( 5 )U( 5 )\ns q ru( 6 )u( 6 )\ns u mu( 8 )u( 8 )J J\n+\n×\n+\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\nofasingle-layerMLPusingthecross-entropylossandweightdecay.\nTheotherpaththroughthecross-entropycostisslightlymorecomplicated.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "LetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\nexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsHGtothe\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\ncomputes ∇ H J=GW( 2 )usingtheback-propagationrulefortheﬁrstargument",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "tothematrixmultiplication operation.Next,thereluoperationusesitsback-\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G.Thelaststepofthe\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\nargumentoftheoperationtoadd matmul XGtothegradientonW( 1 ).\nAfterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "updatetheparameters.\nFortheMLP,thecomputational costisdominatedbythecostofmatrix\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\n2 2 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\nThisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "numberofexamplesintheminibatchand n histhenumberofhiddenunits.\n6.5.8Complications\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\nmentationsactuallyusedinpractice.\nAsnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobea\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "besttocomputebothinasinglepassthroughmemory,soitismosteﬃcientto\nimplementthisprocedureasasingleoperationwithtwooutputs.\nWe havenot described how tocontrolthememoryconsumption ofback-\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether.\nInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerand\naddingeachvaluetothatbuﬀerasitiscomputed.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "addingeachvaluetothatbuﬀerasitiscomputed.\nReal-worldimplementationsofback-propagation alsoneedtohandlevarious\ndatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.\nThepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\nSomeoperationshaveundeﬁnedgradients,anditisimportanttotrackthese\ncasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.\nVariousothertechnicalitiesmakereal-worlddiﬀerentiation morecomplicated.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\nthatmanymoresubtletiesexist.\n6.5.9DiﬀerentiationoutsidetheDeepLearningCommunity\nThe deeplearning comm unityhas beensomewhat isolatedfrom the broader\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\n2 2 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nconcerninghowtoperformdiﬀerentiation. Moregenerally,theﬁeldofautomatic\ndiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically .\nTheback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\ndiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\nruleindiﬀerentorders.Ingeneral, determining theorderofevaluationthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "resultsinthelowestcomputational costisadiﬃcultproblem.Findingtheoptimal\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\nexpensiveform.\nForexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\nwedeﬁne\nq i=exp( z i)\niexp( z i), (6.57)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "wedeﬁne\nq i=exp( z i)\niexp( z i), (6.57)\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\noperations, and construct a cross-entropyloss J=−\ni p ilog q i.Ahuman\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\nsimpleform: q i− p i.Theback-propagation algorithmisnotcapableofsimplifying\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "librariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\nbythepureback-propagation algorithm.\nWhentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\n∂ u() i\n∂ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\nthesameorderasthenumberofcomputations fortheforwardcomputation: this",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "canbeseeninalgorithm becauseeachlocalpartialderivative 6.2∂ u() i\n∂ u() jneedsto\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\ntask. ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "matchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplify\nthegraph.Wedeﬁnedback-propagation onlyforthecomputationofagradientofa\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\nof kdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontaining k\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\n2 2 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\nanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.\nForwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\noﬀcomputational eﬃciencyformemory.Therelationshipbetweenforwardmode\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\nright-multiplyingasequenceofmatrices,suchas\nABCD , (6.58)\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "andgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\nthemultiplications left-to-right,correspondingtotheforwardmode.\nInmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\nplementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramming",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "languagecode,suchasPythonorCcode,andautomatically generatesprograms\nthatdiﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\nrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperation\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.\nHowever,thespecializedapproachalsohasthebeneﬁtofallowingcustomized",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "back-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\nwouldpresumablybeunabletoreplicate.\nBack-propagationisthereforenottheonlywayortheoptimalwayofcomputing\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\nlearningcommunityverywell.Inthefuture,diﬀerentiation technologyfordeep\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\ninthebroaderﬁeldofautomaticdiﬀerentiation.\n2 2 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.10Higher-OrderDerivatives\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\nTheselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\nderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.This\nmeansthatthesymbolicdiﬀerentiation machinerycanbeappliedtoderivatives.\nInthecontextofdeeplearning,itisraretocomputeasinglesecondderivative",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "ofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\nmatrix.Ifwehaveafunction f: Rn→ R,thentheHessianmatrixisofsize n n×.\nIntypicaldeeplearningapplications, nwillbethenumberofparametersinthe\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\nthusinfeasibletoevenrepresent.\nInsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "performingvariousoperationslikeapproximately invertingamatrixorﬁnding\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\notherthanmatrix-vector products.\nInordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\nHv= ∇ x\n(∇ x f x())v\n. (6.59)\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\ntakesthegradientofafunctionoftheinnergradientexpression.\nIfvisitselfavectorproducedbyacomputational graph,itisimportantto\nspecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethrough\nthegraphthatproduced.v\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\ne( ) iistheone-hotvectorwith e( ) i",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "e( ) iistheone-hotvectorwith e( ) i\ni= 1andallotherentriesequalto0.\n6. 6 Hi s t or i c a l Not es\nFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximators\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\n2 2 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\ncenturiesofprogressonthegeneralfunctionapproximationtask.\nThechainrulethatunderliestheback-propagation algorithmwasinvented\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L’Hôpital1696\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\noptimization problemsuntilthe19thcentury(Cauchy1847,).",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "Beginninginthe1940s,thesefunctionapproximation techniqueswereusedto\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\nseveraloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearnthe\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\nLearningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\nceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 273,
      "type": "default"
    }
  },
  {
    "content": "applicationsofthechainrulebasedondynamicprogramming begantoappear\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\ntechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydeveloped\ninpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 274,
      "type": "default"
    }
  },
  {
    "content": "cessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswith\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\nresearchinmulti-layerneuralnetworks. However,theideasputforwardbythe\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\nback-propagation. Theyincludecrucialideasaboutthepossiblecomputational\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 275,
      "type": "default"
    }
  },
  {
    "content": "underthenameof“connectionism” becauseoftheimportancethisschoolofthought\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.\nInparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\ne t a l .,).1986\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\nbeganin2006.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 276,
      "type": "default"
    }
  },
  {
    "content": "beganin2006.\nThecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\nstantiallysincethe1980s. Thesameback-propagationalgorithmandthesame\n2 2 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 277,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\nnetworksnoticeably.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 278,
      "type": "default"
    }
  },
  {
    "content": "networksnoticeably.\nOneofthesealgorithmicchangeswasthereplacementofmeansquarederror\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\nhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemean",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 279,
      "type": "default"
    }
  },
  {
    "content": "squarederrorloss.\nTheothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\nlinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0 , z}\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\nmodelsdid notuserectiﬁed linearunits, but insteadappliedrectiﬁcation to",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 280,
      "type": "default"
    }
  },
  {
    "content": "nonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwas\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\nwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunits\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\nnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009.\nJarrett2009 e t a l .()observedthat“usingarectifyingnonlinearityisthesinglemost",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 281,
      "type": "default"
    }
  },
  {
    "content": "importantfactorinimprovingtheperformanceofarecognitionsystem”among\nseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.\nForsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\nRandomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁed\nlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerent\nfeaturevectorstoclassidentities.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 282,
      "type": "default"
    }
  },
  {
    "content": "featurevectorstoclassidentities.\nWhenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\nshowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeep\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\n2 2 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 283,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\nneurosciencehascontinuedtohave aninﬂuenceonthe developmentofdeep\nlearningalgorithms. ()motivaterectiﬁedlinearunitsfrom Glorot e t a l .2011a\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\ncompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportional",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 284,
      "type": "default"
    }
  },
  {
    "content": "toitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\ntheyareinactive(i.e.,theyshouldhavesparseactivations).\nWhenthemodernresurgenceofdeeplearningbeganin2006,feedforward\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 285,
      "type": "default"
    }
  },
  {
    "content": "Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 286,
      "type": "default"
    }
  },
  {
    "content": "ismorecommontousesupervisedlearningtosupportunsupervisedlearning.\nFeedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,we\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\nsubsequentchapters,weturntohowtousethesemodels—howtoregularizeand\ntrainthem.\n2 2 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 287,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 0\nS e qu e n ce Mo d e l i n g: Recurren t\nan d Recursiv e N e t s\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\nprocessingasequenceofvaluesx( 1 ), . . . ,x( ) τ.Justasconvolutionalnetworks",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "canreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\nlength.\nTogofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\nthe1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharing",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "makesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms\n(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\nseenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengths\nandacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhen\naspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.\nForexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "IwenttoNepal.”Ifweaskamachinelearningmodeltoreadeachsentenceand\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\n373",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\nnetworkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnected\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\nacrossseveraltimesteps.\nArelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "convolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\nstep.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberofthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "outputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\nThisrecurrentformulationresultsinthesharingofparametersthroughavery\ndeepcomputational graph.\nForthesimplicityofexposition,werefertoRNNsasoperatingonasequence\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 τ.In\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\nwithadiﬀerentsequencelength τforeachmemberoftheminibatch.Wehave",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "omittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\nentiresequenceisobservedbeforeitisprovidedtothenetwork.\nThischapterextendstheideaofacomputational graphtoincludecycles.These",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "cyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalue\natafuturetimestep.Suchcomputational graphsallowustodeﬁnerecurrent\nneuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,and\nuserecurrentneuralnetworks.\nFormoreinformationonrecurrentneuralnetworksthanisavailableinthis\nchapter,wereferthereadertothetextbookofGraves2012().\n3 7 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.1UnfoldingComputationalGraphs\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\nPleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\nstructure.\nForexample,considertheclassicalformofadynamicalsystem:\ns( ) t= ( fs( 1 ) t −;)θ , (10.1)\nwheres( ) tiscalledthestateofthesystem.\nEquationisrecurrentbecausethedeﬁnitionof 10.1 sattime trefersbackto\nthesamedeﬁnitionattime. t−1\nForaﬁnitenumberoftimesteps τ,thegraphcanbeunfoldedbyapplying\nthedeﬁnition τ−1times.Forexample,ifweunfoldequationfor10.1 τ= 3time\nsteps,weobtain\ns( 3 )=( fs( 2 );)θ (10.2)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "steps,weobtain\ns( 3 )=( fs( 2 );)θ (10.2)\n=(( f fs( 1 ););)θθ (10.3)\nUnfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhas\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\nnowberepresentedbyatraditionaldirectedacycliccomputational graph. The\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\nﬁgure.10.1\ns( t − 1 )s( t − 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . .\nf f f f f f",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "f f f f f f\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\nunfoldedcomputationalgraph. Eachnoderepresentsthestateatsometime tandthe\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\nofusedtoparametrize)areusedforalltimesteps. θ f\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\nsignalx( ) t,\ns( ) t= ( fs( 1 ) t −,x( ) t;)θ , (10.4)\n3 7 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\nRecurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchas\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\nManyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\ndeﬁnethevaluesoftheirhiddenunits. Toindicatethatthestateisthehidden",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "unitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\nthestate:\nh( ) t= ( fh( 1 ) t −,x( ) t;)θ , (10.5)\nillustratedinﬁgure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "summaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\n(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))toaﬁxedlengthvectorh( ) t.Dependingonthe\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "uptotime t,butratheronlyenoughinformationtopredicttherestofthesentence.\nThemostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\n(chapter).14\nf fhh\nx xh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . .\nf f\nU nf ol df f f f f\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "informationfromtheinputxbyincorporatingitintothestatehthatispassedforward\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\nnodeisnowassociatedwithoneparticulartimeinstance.\nEquationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN 10.5\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\n3 7 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\nview,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalparts\nwhosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "unfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\ndiﬀerentvariables,withonevariablepertimestep,representingthestateofthe\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\nseparatenodeofthecomputational graph,asintherightofﬁgure.Whatwe10.2\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideoftheﬁgure\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\ngraphnowhasasizethatdependsonthesequencelength.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "graphnowhasasizethatdependsonthesequencelength.\nWecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\nh( ) t= g( ) t(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 )) (10.6)\n=( fh( 1 ) t −,x( ) t;)θ (10.7)\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "processthusintroducestwomajoradvantages:\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\ninputsize,becauseitisspeciﬁedintermsoftransitionfromonestateto\nanotherstate,ratherthanspeciﬁedintermsofavariable-length historyof\nstates.\n2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\nateverytimestep.\nThesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "model g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\nrequiredwithoutparametersharing.\nBoththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\n3 7 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninformationﬂowforwardintime(computingoutputsandlosses)andbackward\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\ninformationﬂows.\n10.2RecurrentNeuralNetworks\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\ncandesignawidevarietyofrecurrentneuralnetworks.\nUUV V\nWWo( t − 1 )o( t − 1 )\nhhooy y\nLL",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "UUV V\nWWo( t − 1 )o( t − 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\nUU UU UUU nf ol d\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "thatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.\nAloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\ncomputesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "aweightmatrixV.Equationdeﬁnesforwardpropagationinthismodel. 10.8 ( L e f t )The\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\ntimeinstance.\nSomeexamplesofimportantdesignpatternsforrecurrentneuralnetworks\nincludethefollowing:\n3 7 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsbetweenhiddenunits,illustratedinﬁgure.10.3\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\nunitsatthenexttimestep,illustratedinﬁgure10.4\n•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\nﬁgure.10.5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "ﬁgure.10.5\nﬁgureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\nmostofthechapter.\nTherecurrentneuralnetworkofﬁgureandequationisuniversalinthe 10.3 10.8\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\narecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafter\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .\nTheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\nfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(Siegelmannand\nSontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcation",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "ofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\nmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproof\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\nrationalnumbersofunboundedprecision.\nWenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\nﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe 10.3\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "theﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\nHereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\nobtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagation\nbeginswithaspeciﬁcationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\n3 7 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nUV\nWo( t − 1 )o( t − 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . .\nh( ) . . .h( ) . . .V V V\nU U UU nf ol d\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "tothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram.\n( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\nsmallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.TheRNN 10.3\ninﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\nrepresentationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "putaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosend\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\nUnlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\nfromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasierto\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\nparallelizationduringtraining,asdescribedinsection.10.2.1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "3 8 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nt t τ = 1to= ,weapplythefollowingupdateequations:\na( ) t= +bWh( 1 ) t −+Ux( ) t(10.8)\nh( ) t=tanh(a( ) t) (10.9)\no( ) t= +cVh( ) t(10.10)\nˆy( ) t=softmax(o( ) t) (10.11)\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "givensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\nL\n{x( 1 ), . . . ,x( ) τ}{ ,y( 1 ), . . . ,y( ) τ}\n(10.12)\n=\ntL( ) t(10.13)\n=−\ntlog p m o de l\ny( ) t|{x( 1 ), . . . ,x( ) t}\n, (10.14)\nwhere p m o de l\ny( ) t|{x( 1 ), . . . ,x( ) t}\nisgivenbyreadingtheentryfor y( ) tfromthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "isgivenbyreadingtheentryfor y( ) tfromthe\nmodel’soutputvectorˆy( ) t.Computingthegradientofthislossfunctionwithrespect\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\noftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass 10.3\nmovingrighttoleftthroughthegraph.Theruntimeis O( τ) andcannotbereduced\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "eachtimestepmayonlybecomputedafterthepreviousone. Statescomputed\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\npass,sothememorycostisalso O( τ).Theback-propagation algorithmapplied\ntotheunrolledgraphwith O( τ)costiscalledback-propagationthroughtime\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\nalternative?\n10.2.1TeacherForcingandNetworkswithOutputRecurrence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "Thenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\nthehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful 10.4\n3 8 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "knowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\nneedtocomputetheoutputfortheprevioustimestepﬁrst,becausethetraining\nsetprovidestheidealvalueofthatoutput.\nh( t − 1 )h( t − 1 )\nWh( ) th( ) t . . . . . .",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "h( t − 1 )h( t − 1 )\nWh( ) th( ) t . . . . . .\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\nU U Uh( ) τh( ) τ\nx( ) τx( ) τW\nUo( ) τo( ) τy( ) τy( ) τL( ) τL( ) τ\nV\n. . . . . .\nFigure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\nﬁxed-sizerepresentationusedasinputforfurtherprocessing. Theremightbeatarget\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "back-propagatingfromfurtherdownstreammodules.\nModelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1. Wecansee\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\n3 8 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t − 1 )o( t − 1 )o( ) to( ) t\nh( t − 1 )h( t − 1 )h( ) th( ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\nV V\nU Uo( t − 1 )o( t − 1 )o( ) to( ) tL( t − 1 )L( t − 1 )L( ) tL( ) ty( t − 1 )y( t − 1 )y( ) ty( ) t\nh( t − 1 )h( t − 1 )h( ) th( ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\nV V\nU U\nT r ai n  t i m e T e s t   t i m e\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "applicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel’soutput\no( ) t,andfeedtheoutputbackintothemodel.\n3 8 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nlikelihoodcriterionis\nlog p\ny( 1 ),y( 2 )|x( 1 ),x( 2 )\n(10.15)\n=log p\ny( 2 )|y( 1 ),x( 1 ),x( 2 )\n+log p\ny( 1 )|x( 1 ),x( 2 )\n(10.16)\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\nvaluefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,\nratherthanfeedingthemodel’sownoutputbackintoitself,theseconnections",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "shouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\nThisisillustratedinﬁgure.10.6\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "timesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\nwithbothteacherforcingandBPTT.\nThedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\ntheoutputdistribution)fedbackasinput. Inthiscase,thekindofinputsthat\nthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs\nthatitwillseeattesttime. Onewaytomitigatethisproblemistotrainwith",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "bothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "inputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\nusemoreofthegeneratedvaluesasinput.\n10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.\nOnesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\n3 8 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary.\nGradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\ngradient-basedtechniquestotrainanRNN.\nTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "tforx( ) t,h( ) t,o( ) tand L( ) t. Foreachnode Nweneedtocomputethegradient\n∇ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss\n∂ L\n∂ L( ) t= 1 . (10.17)\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\nsoftmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealso\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "inputsofar.Thegradient∇o( ) t Lontheoutputsattimestep t,forall i , t,isas\nfollows:\n(∇o( ) t L)i=∂ L\n∂ o( ) t\ni=∂ L\n∂ L( ) t∂ L( ) t\n∂ o( ) t\ni=ˆ y( ) t\ni− 1i , y( ) t .(10.18)\nWeworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnal\ntimestep, τh( ) τonlyhaso( ) τasadescendent,soitsgradientissimple:\n∇h( ) τ L= V∇o( ) τ L. (10.19)\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\nfrom t= τ−1downto t= 1,notingthath( ) t(for t < τ)hasasdescendentsboth",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "o( ) tandh( + 1 ) t.Itsgradientisthusgivenby\n∇h( ) t L=\n∂h( + 1 ) t\n∂h( ) t\n(∇h( +1) t L)+\n∂o( ) t\n∂h( ) t\n(∇o( ) t L) (10.20)\n= W(∇h( +1) t L)diag\n1−\nh( + 1 ) t2\n+V(∇o( ) t L)(10.21)\nwhere diag\n1−\nh( + 1 ) t2\nindicatesthediagonalmatrixcontainingtheelements\n1−( h( + 1 ) t\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\nhiddenunitattime. i t+1\n3 8 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nOncethegradientsonthe internalnodesofthe computational graphare\nobtained, wecanobtainthegradientsontheparameternodes.Becausethe\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\nofasingleedgeinthecomputational graphtothegradient.However,the∇ W f",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "operatorusedincalculustakesintoaccountthecontributionofWtothevalue\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\nintroducedummyvariablesW( ) tthataredeﬁnedtobecopiesofWbutwitheach\nW( ) tusedonlyattimestep t.Wemaythenuse∇W( ) ttodenotethecontribution\noftheweightsattimesteptothegradient. t\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\n∇ c L=\nt\n∂o( ) t\n∂c\n∇o( ) t L=\nt∇o( ) t L (10.22)\n∇ b L=\nt\n∂h( ) t\n∂b( ) t\n∇h( ) t L=\ntdiag\n1−\nh( ) t2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "∂b( ) t\n∇h( ) t L=\ntdiag\n1−\nh( ) t2\n∇h( ) t L(10.23)\n∇ V L=\nt\ni\n∂ L\n∂ o( ) t\ni\n∇ V o( ) t\ni=\nt(∇o( ) t L)h( ) t(10.24)\n∇ W L=\nt\ni\n∂ L\n∂ h( ) t\ni\n∇W( ) t h( ) t\ni (10.25)\n=\ntdiag\n1−\nh( ) t2\n(∇h( ) t L)h( 1 ) t −(10.26)\n∇ U L=\nt\ni\n∂ L\n∂ h( ) t\ni\n∇U( ) t h( ) t\ni (10.27)\n=\ntdiag\n1−\nh( ) t2\n(∇h( ) t L)x( ) t(10.28)\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\nitdoesnothaveanyparametersasancestorsinthecomputational graphdeﬁning\ntheloss.\n3 8 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.2.3RecurrentNetworksasDirectedGraphicalModels\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\nThelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "weusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss.\nMeansquarederroristhecross-entropylossassociatedwithanoutputdistribution\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\nWhen we use apredictivelog-likelihood trainingobjective,such asequa-\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\nthelog-likelihood\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "log( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\ntimestep,\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t −) . (10.30)\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "fromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\nvaluesinthepasttothecurrent y( ) tvalue.\nAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) τ},withnoadditionalinputs",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "x.Theinputattimestep tissimplytheoutputattimestep t−1.TheRNNthen\ndeﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\nprobabilities:\nP P () = Y ( y( 1 ), . . . , y( ) τ) =τ\nt = 1P( y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 ))(10.31)\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) τ}accordingtosuchamodel\n3 8 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\nFigure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\npastobservation y( ) imayinﬂuencetheconditionaldistributionofsome y( ) t(for t > i),\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\ngraph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof 10.6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "inputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\nconnectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8\nis\nL=\ntL( ) t(10.32)\nwhere\nL( ) t= log( − Py( ) t= y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 )) .(10.33)\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .\nFigure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\neﬃcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\nsharethesameparameterswiththeotherstages.\nTheedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\neﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\n3 8 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\nshouldonlycontainedgesfrom{y( ) t k −, . . . ,y( 1 ) t −}toy( ) t,ratherthancontaining\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\ninputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsare\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\nfromthedistantpastinawaythatisnotcapturedbytheeﬀectofy( ) iony( 1 ) t −.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\ndeﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\nvalueswiththecompletegraphstructureisshowninﬁgure.Thecomplete10.7\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\nmarginalizing themoutofthemodel.\nItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "hiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcient\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\nrepresentation—anarraycontainingaseparateentryforeachpossibleassignment\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\noccurring. If ycantakeon kdiﬀerentvalues,thetabularrepresentationwould\nhave O( kτ)parameters.Bycomparison,duetoparametersharing,thenumberof",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "parametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\nlong-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplications\nofthesamefunction fandsameparametersθateachtimestep.Figure10.8\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "quantitybetweenthem.Avariable y( ) iinthedistantpastmayinﬂuenceavariable\ny( ) tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbe\neﬃcientlyparametrized byusingthesameconditionalprobabilitydistributionsat\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\njointassignmentofallvariablescanbeevaluatedeﬃciently.\nEvenwiththeeﬃcientparametrization ofthegraphicalmodel,someoperations\nremaincomputationally challenging.Forexample,itisdiﬃculttopredictmissing",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\nh i d d e n u n i t s .\n3 8 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nvaluesinthemiddleofthesequence.\nThepricerecurrentnetworkspayfortheirreducednumberofparametersis\nthat theparametersmaybediﬃcult. o p t i m i z i ng\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\nthatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,the\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "betweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\ndiﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerent\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\nextrapolatewhenfacedwithnewvaluesof. t\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "todrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\nsimplytosamplefromtheconditionaldistributionateachtimestep. However,\nthereisoneadditionalcomplication. The RNNmusthavesomemechanismfor\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.\nInthecasewhentheoutputisasymboltakenfromavocabulary,onecan\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).\nWhenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "weinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) τ\nineachtrainingexample.\nAnotheroptionistointroduceanextraBernoullioutputtothemodelthat\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "sigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\nsequenceendsorcontinuesateachtimestep.\nAnotherwaytodeterminethesequencelength τistoaddanextraoutputto\nthemodelthatpredictstheinteger τitself.Themodelcansampleavalueof τ\nandthensample τstepsworthofdata.Thisapproachrequiresaddinganextra\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "awareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\ncaneitherconsistofthevalueof τorcanconsistof τ t−,thenumberofremaining\n3 9 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat\nendabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis\nbasedonthedecomposition\nP(x( 1 ), . . . ,x( ) τ) = ()( P τ Px( 1 ), . . . ,x( ) τ| τ .)(10.34)\nThestrategyofpredicting τdirectlyisusedforexamplebyGoodfellow e t a l .\n().2014d\n10.2.4ModelingSequencesConditionedonContextwithRNNs\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "graphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of\ncourse,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8\ninputsx( 1 ),x( 2 ), . . . ,x( ) τ.Ingeneral,RNNsallowtheextensionofthegraphical\nmodelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut\nalsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof\nfeedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;θ)\ncanbereinterpretedasamodelrepresentingaconditionaldistribution P(yω|)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "withω=θ.Wecanextendsuchamodeltorepresentadistribution P(yx|)by\nusingthesame P(yω|)asbefore,butmakingωafunctionofx.Inthecaseof\nanRNN,thiscanbeachievedindiﬀerentways.Wereviewherethemostcommon\nandobviouschoices.\nPreviously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor\nt=1 , . . . , τasinput. Anotheroptionistotakeonlyasinglevectorxasinput.\nWhenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN\nthatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto\nanRNNare:",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "anRNNare:\n1. asanextrainputateachtimestep,or\n2. astheinitialstateh( 0 ),or\n3. both.\nTheﬁrstandmostcommonapproachisillustratedinﬁgure.Theinteraction10.9\nbetweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly\nintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequence\nof yvalues. ThesameproductxRisaddedasadditionalinputtothehidden\nunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue\n3 9 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nofxRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits.\nTheweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\ntheparametersθofthenon-conditional modelandturningthemintoω,where\nthebiasparameterswithinarenowafunctionoftheinput. ω\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "h( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\nx xy( ) . . .y( ) . . .\nR R R R R\nFigure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequences\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\nEachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\ntimestep)and,duringtraining,astarget(fortheprevioustimestep).",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) τ|x( 1 ), . . . ,x( ) τ)thatmakesa\nconditionalindependence assumptionthatthisdistributionfactorizesas\n\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\ntheoutputattime ttothehiddenunitattime t+1,asshowninﬁgure.The10.10",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.\nThiskindofmodelrepresentingadistributionoverasequencegivenanother\n3 9 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V\nU U U\nx( t − 1 )x( t − 1 )R\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\nFigure10.10: Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "ofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\nﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\ngivensequencesofxofthesamelength.TheRNNofﬁgureisonlyabletorepresent 10.3\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\nthevalues.x\n3 9 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t − 1 )g( t − 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "Figure10.11: Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.\nThehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\npoint t,theoutputunitso( ) tcanbeneﬁtfromarelevantsummaryofthepastinitsh( ) t\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput.\n10.3BidirectionalRNNs",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "10.3BidirectionalRNNs\nAlloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\nx( 1 ), . . . ,x( 1 ) t −,andthepresentinputx( ) t.Someofthemodelswehavediscussed\nalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhenthey\nvaluesareavailable.\nHowever,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\n3 9 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "alsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\ntasks,describedinthenextsection.\nBidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\ne t a l .,).1999",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).1999\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\nsub-RNNthatmovesbackwardthroughtime. Thisallowstheoutputunitso( ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "tocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\nﬁxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\naconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer).\nThisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\nhavingRNNs,eachonegoinginoneofthefourdirections: up, down,left, f o u r\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "representationthatwouldcapturemostlylocalinformationbutcouldalsodepend\non long-range inputs,ifthe RNN isable tolearn tocarry that information.\nComparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "totherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\ninteractions.\n3 9 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\ntures\nWehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size 10.5\nvector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa 10.9\nsequence. Wehaveseeninﬁgures,,andhowanRNNcan 10.310.410.1010.11\nmapaninputsequencetoanoutputsequenceofthesamelength.\nE nc ode r\n…\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\nD e c ode r\n…",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "D e c ode r\n…\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\nFigure10.12: Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\ngivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputea",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "generallyﬁxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\nsequenceandisgivenasinputtothedecoderRNN.\nHerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\noutputsequencewhichisnotnecessarilyofthesamelength. This comesupin\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\n3 9 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\nnotofthesamelength(althoughtheirlengthsmightberelated).\nWeoftencalltheinputtotheRNNthe“context.”Wewanttoproducea\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )).\nThesimplestRNNarchitectureformappingavariable-length sequenceto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "anothervariable-length sequencewasﬁrstproposedby ()and Cho e t a l .2014a\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\ntectureandweretheﬁrsttoobtainstate-of-the-art translationusingthisapproach.\nTheformersystemisbasedonscoringproposalsgeneratedbyanothermachine\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\nthetranslations. Theseauthorsrespectivelycalledthisarchitecture, illustrated",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "inﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\nﬁnalhiddenstate. (2)adecoderorwriteroroutputRNNisconditionedon\nthatﬁxed-lengthvector(justlikeinﬁgure)togeneratetheoutputsequence 10.9\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "varyfromeachother,whilepreviousarchitectures constrained n x= n y= τ.Ina\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\ndecoderRNN.\nIfthecontext Cisavector,thenthedecoderRNNissimplyavector-to-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "sequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\nateachtimestep.Thesetwowayscanalsobecombined.\nThereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\nasthedecoder.\nOneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "sequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\nthanaﬁxed-sizevector.Additionally,theyintroducedanattentionmechanism\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\n3 9 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequence.Seesectionformoredetails. 12.4.5.1\n10.5DeepRecurrentNetworks\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\nandassociatedtransformations:\n1. fromtheinputtothehiddenstate,\n2. fromtheprevioushiddenstatetothenexthiddenstate,and\n3. fromthehiddenstatetotheoutput.\nWiththeRNNarchitectureofﬁgure,eachofthesethreeblocksisassociated 10.3\nwithasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "ofthesecorrespondstoashallowtransformation. Byashallowtransformation,\nwemeanatransformationthatwouldberepresentedbyasinglelayerwithin\nadeepMLP.Typicallythisisatransformationrepresentedbyalearnedaﬃne\ntransformationfollowedbyaﬁxednonlinearity.\nWoulditbeadvantageoustointroducedepthineachoftheseoperations?\nExperimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests\nso.Theexperimentalevidenceisinagreementwiththeideathatweneedenough",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "depthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),\nElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs.\nGraves2013 e t a l .()weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing\nthestateofanRNNintomultiplelayersasinﬁgure(left).Wecanthink 10.13\nofthelowerlayersinthehierarchydepictedinﬁgureaasplayingarole 10.13\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\nthehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "andproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\nenumeratedabove,asillustratedinﬁgureb.Considerationsofrepresentational 10.13\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\nsobyaddingdepthmayhurtlearningbymakingoptimization diﬃcult.Ingeneral,\nitiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof\nﬁgurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable\nintimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "layerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\nshortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththe\nordinaryRNNofﬁgure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this\n3 9 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhy\nxz\n( a) ( b) ( c )xhy\nxhy\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\nhidden,hidden-to-hiddenandhidden-to-outputparts. Thismaylengthentheshortest\npathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby ( c )\nintroducingskipconnections.\n3 9 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\nillustratedinﬁgurec.10.13\n10.6RecursiveNeuralNetworks\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\nx( 4 )x( 4 )Voo\nU W U WUW\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\nbemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhich\nsometargetisprovidedwhichisassociatedwiththewholesequence. y\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\nnetworks,withadiﬀerentkindofcomputational graph,whichisstructuredasa\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\ngraphforarecursivenetworkisillustratedinﬁgure.Recursiveneural 10.14",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "2W e s u g g e s t t o n o t a b b re v i a t e “ re c u rs i v e n e u ra l n e t w o rk ” a s “ R NN” t o a v o i d c o n f u s i o n with\n“ re c u rre n t n e u ra l n e t w o rk . ”\n4 0 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\ncomputervision( ,). Socher e t a l .2011b\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "ofthesamelength τ,thedepth(measuredasthenumberofcompositionsof\nnonlinearoperations)canbedrasticallyreducedfrom τto O(log τ),whichmight\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\nlanguagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "thestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\nparser( ,,). Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\nsuggestedby(). Bottou2011\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\nandassociatethe inputsandtargetswith individualnodesofthe tree.The",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "computationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcial\nneuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotone\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\nrepresentedbycontinuousvectors(embeddings).\n10.7TheChallengeofLong-TermDependencies",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "10.7TheChallengeofLong-TermDependencies\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\n(rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\nwithgradientsnotexploding),thediﬃcultywithlong-termdependenciesarises",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "fromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\n4 0 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n− − − 6 0 4 0 2 0 0 2 0 4 0 6 0\nI nput c o o r di na t e− 4− 3− 2− 101234P r o j e c t i o n o f o utput0\n1\n2\n3\n4\n5\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "downtoasingledimension,plottedonthe y-axis. The x-axisisthecoordinateofthe\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\nhasbeencomposed.\n1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\nbehavior,asillustratedinﬁgure.10.15\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\nsomewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation\nh( ) t= Wh( 1 ) t −(10.36)\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\nandlackinginputsx.As described insection , thisrecurrencerelation 8.2.5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "essentiallydescribesthepowermethod.Itmaybesimpliﬁedto\nh( ) t=\nWth( 0 ), (10.37)\nandifadmitsaneigendecompositionoftheform W\nWQQ = Λ, (10.38)\n4 0 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwithorthogonal ,therecurrencemaybesimpliﬁedfurtherto Q\nh( ) t= QΛtQh( 0 ). (10.39)\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\nwilleventuallybediscarded.\nThisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "multiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\nnetworkthathasadiﬀerentweight w( ) tateachtimestep,thesituationisdiﬀerent.\nIftheinitialstateisgivenby,thenthestateattime 1 tisgivenby\nt w( ) t.Suppose\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\ndesiredvariance v∗wemaychoosetheindividualweightswithvariance v=n√",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "v∗.\nVerydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\nThevanishingandexplodinggradientproblemforRNNswasindependently\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "mustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\n1994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\nthegradientofashortterminteraction. It doesnotmeanthatitisimpossible\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\nﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "in ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydiﬃcult,\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\nreaching0forsequencesofonlylength10or20.\nForadeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "approachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\n4 0 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\noneofthemainchallengesindeeplearning.\n10.8EchoStateNetworks\nTherecurrentweightsmappingfromh( 1 ) t −toh( ) tandtheinputweightsmapping\nfromx( ) ttoh( ) taresomeofthemostdiﬃcultparameterstolearninarecurrent\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\nJaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweights",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "suchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "reservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthat\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerent\naspectsofthehistoryofinputs.\nOnewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\nhistoryofinputsuptotime t)intoaﬁxed-lengthvector(therecurrentstateh( ) t),\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "theproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\nsimplelearningalgorithms(,). Jaeger2003\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "networkstate? Theanswerproposedinthereservoircomputingliteratureisto\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\nweightssuchthatthedynamicalsystemisneartheedgeofstability.\nTheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\nJ( ) t=∂ s( ) t\n∂ s( 1 ) t −.OfparticularimportanceisthespectralradiusofJ( ) t,deﬁnedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "bethemaximumoftheabsolutevaluesofitseigenvalues.\n4 0 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTounderstandtheeﬀectofthespectralradius,considerthesimplecaseof\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\naneigenvectorvwithcorrespondingeigenvalue λ.Considerwhathappensaswe\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "stepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\naperturbedversionofg.Ifwebeginwithg+ δv,thenafteronestep,wewill\nhaveJ(g+ δv).After nsteps,wewillhaveJn(g+ δv).Fromthiswecansee\nthatback-propagationstartingfromgandback-propagationstartingfromg+ δv\ndivergeby δJnvafter nstepsofback-propagation.Ifvischosentobeaunit\neigenvectorofJwitheigenvalue λ,thenmultiplicationbytheJacobiansimply\nscalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationare",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "separatedbyadistanceof δ λ||n.Whenvcorrespondstothelargestvalueof|| λ,\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\nofsize. δ\nWhen || λ >1,thedeviationsize δ λ||ngrowsexponentiallylarge.When || λ <1,\nthedeviationsizebecomesexponentiallysmall.\nOfcourse,thisexampleassumedthattheJacobianwasthesameatevery\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "manytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\nradius. Indeed,themostrecentworkonechostatenetworksadvocatesusinga\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\nwherethestateh( + 1 ) t= h( ) t W.\nWhenalinearmapWalwaysshrinkshasmeasuredbythe L2norm,then",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "wesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\nthepastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostore\nthestatevector.\nTheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "duringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\n4 0 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\ncoeﬃcients, whenwemultiplythematrixbythevector.Aneigenvaluewith\nmagnitudegreaterthanonecorrespondstomagniﬁcation (exponentialgrowth,if",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "appliediteratively)orshrinking(exponentialdecay,ifappliediteratively).\nWithanonlinearmap, theJacobianisfreetochangeateachstep.The\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\ndiﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\nbounded.Notethat itispossible forback-propagation to retainunbounded",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "dynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\nThestrategyofechostatenetworksissimplytoﬁxtheweightstohavesome\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\ndoesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearities liketanh.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\nwiththesparseinitialization schemedescribedinsection.8.4\n10.9LeakyUnitsandOtherStrategiesforMultiple\nTimeScales",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "TimeScales\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\natmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grained\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\nscalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.\nVariousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.These\nincludetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegrate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "signalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections\n4 0 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nusedtomodelﬁne-grainedtimescales.\n10.9.1AddingSkipConnectionsthroughTime\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "Itispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\ndiminishexponentiallyasafunctionofτ\ndratherthan τ.Sincethereareboth\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin τ.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\nlong-termdependencies mayberepresentedwellinthisway.\n10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.\nWhenweaccumulatearunningaverage µ( ) tofsomevalue v( ) tbyapplyingthe\nupdate µ( ) t← α µ( 1 ) t −+(1− α) v( ) tthe αparameterisanexampleofalinearself-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "connectionfrom µ( 1 ) t −to µ( ) t.When αisnearone,therunningaverageremembers\ninformationaboutthepastforalongtime,andwhen αisnearzero,information\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\nunits.\nSkipconnectionsthrough dtimestepsareawayofensuringthataunitcan\nalwayslearntobeinﬂuencedbyavaluefrom dtimestepsearlier.Theuseofa\nlinearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthatthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "unitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\nthiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valued\nαratherthanbyadjustingtheinteger-valuedskiplength.\nTheseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\n(,). Jaeger e t a l .2007\n4 0 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\nunits. Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,for\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time.\nAnotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\nHavingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).\n10.9.3RemovingConnections",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "10.9.3RemovingConnections\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\ninformationﬂowingmoreeasilythroughlongdistancesattheslowertimescales.\nThisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlier\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\nwithlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateona",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "longtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\nfocusontheirothershort-termconnections.\nTherearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto\noperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,\nbuttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.\nThiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "e t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\natdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisis\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\nwellonanumberofbenchmarkdatasets.\n10.10TheLongShort-TermMemoryandOtherGated\nRNNs\nAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\nnetworksbasedonthe . gatedrecurrentunit",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "networksbasedonthe . gatedrecurrentunit\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\ntimethathavederivativesthatneithervanishnorexplode.Leakyunits did\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\n4 0 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nateachtimestep.\n×\ni nput i nput gate f or ge t   gate output gateoutput\ns t at es e l f - l oop×\n+ ×\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnected\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\nAninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbe\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "linearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\nbeshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\ninputunitcanhaveanysquashingnonlinearity. Thestateunitcanalsobeusedasan\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\nLeakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\nforaparticularfeatureorcategory)overalongduration.However,oncethat",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "informationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\n4 0 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\niswhatgatedRNNsdo.\n10.10.1LSTM\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\ncanﬂowforlongdurationsisacorecontributionoftheinitiallongshort-term\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\nﬁxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\nInthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescale\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\ninmanyapplications, suchasunconstrainedhandwriting recognition(Graves\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\nparsing(Vinyals2014a e t a l .,).\nTheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding 10.16\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "wisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTM\nrecurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\nsystemofgatingunitsthatcontrolstheﬂowofinformation. Themostimportant\ncomponentisthestateunit s( ) t\nithathasalinearself-loopsimilartotheleaky\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "associatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\ni(fortimestep t\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\nf( ) t\ni= σ\n bf\ni+\njUf\ni , j x( ) t\nj+\njWf\ni , j h( 1 ) t −\nj\n ,(10.40)\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\n4 1 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\nf( ) t\ni:\ns( ) t\ni= f( ) t\ni s( 1 ) t −\ni + g( ) t\ni σ\n b i+\njU i , j x( ) t\nj+\njW i , j h( 1 ) t −\nj\n ,(10.41)\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\niiscomputed\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\n0and1),butwithitsownparameters:\ng( ) t\ni= σ\n bg\ni+",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "g( ) t\ni= σ\n bg\ni+\njUg\ni , j x( ) t\nj+\njWg\ni , j h( 1 ) t −\nj\n .(10.42)\nTheoutput h( ) t\nioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgate q( ) t\ni,\nwhichalsousesasigmoidunitforgating:\nh( ) t\ni= tanh\ns( ) t\ni\nq( ) t\ni (10.43)\nq( ) t\ni= σ\n bo\ni+\njUo\ni , j x( ) t\nj+\njWo\ni , j h( 1 ) t −\nj\n (10.44)\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\ni",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "i\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\ninﬁgure.Thiswouldrequirethreeadditionalparameters. 10.16\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\nthanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfor\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "Graves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\nhavebeenstudiedandusedandarediscussednext.\n10.10.2OtherGatedRNNs\nWhichpieces ofthe LSTMarchitecture are actually necessary?Whatother\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\ncontrolthetimescaleandforgettingbehaviorofdiﬀerentunits?\n4 1 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\ndiﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\narethefollowing:\nh( ) t\ni= u( 1 ) t −\ni h( 1 ) t −\ni+(1− u( 1 ) t −\ni) σ\n b i+",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "i h( 1 ) t −\ni+(1− u( 1 ) t −\ni) σ\n b i+\njU i , j x( 1 ) t −\nj +\njW i , j r( 1 ) t −\nj h( 1 ) t −\nj\n ,\n(10.45)\nwhereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedas\nusual:\nu( ) t\ni= σ\n bu\ni+\njUu\ni , j x( ) t\nj+\njWu\ni , j h( ) t\nj\n (10.46)\nand\nr( ) t\ni= σ\n br\ni+\njUr\ni , j x( ) t\nj+\njWr\ni , j h( ) t\nj\n .(10.47)\nTheresetandupdatesgatescanindividually“ignore”partsofthestatevector.\nTheupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "dimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\nignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\nadditionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate.\nManymorevariantsaroundthisthemecanbedesigned.Forexamplethe\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\nacrossawiderangeoftasks(,; Greﬀ e t a l .2015Jozefowicz2015Greﬀ e t a l .,).\ne t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "advocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\nexploredarchitecturalvariants.\n4 1 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.11OptimizationforLong-TermDependencies\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.\nAninterestingideaproposedbyMartensandSutskever2011()isthatsecond\nderivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-order\noptimization algorithmsmayroughlybeunderstoodasdividingtheﬁrstderivative",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "bythesecondderivative(inhigherdimension,multiplyingthegradientbythe\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrst\nderivative,thentheratioofﬁrstandsecondderivativesmayremainrelatively\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "methodssuchasNesterovmomentumwithcarefulinitialization couldachieve\nsimilarresults.SeeSutskever2012()formoredetail. Bothoftheseapproaches\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\npowerfuloptimization algorithm.\n10.11.1ClippingGradients\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "byarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\neitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3\nﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\nparameters)hasa“landscape” inwhichoneﬁnds“cliﬀs”:wideandratherﬂat\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\nformingakindofcliﬀ.\nThediﬃcultythatarisesisthatwhentheparametergradientisverylarge,a\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "regionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\ncorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthe\ncurrentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmay\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\n4 1 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\nlandscapeonthenextstep.\n\n \n               \n\n \n            \nFigure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwith",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "twoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\nreasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccur\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\nThecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "fromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\nreactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothat\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\npermissionfromPascanu2013 e t a l .().\nAsimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\nclippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "Pascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\nupdate:\nif||||g > v (10.48)\ng←g v\n||||g(10.49)\n4 1 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\ngradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchas\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "normclipping,theparameterupdatevectornormisnowbounded.Thisbounded\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\ngradientisnumerically InforNan(consideredinﬁniteornot-a-number),then\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\nnumericallyunstableconﬁguration. Clippingthegradientnormper-minibatchwill",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "notchangethedirectionofthegradientforanindividualminibatch.However,\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\ntotheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "minibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "conjecturethatallthesemethodsbehavesimilarly.\n10.11.2RegularizingtoEncourageInformationFlow\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "loopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\ntoregularizeorconstraintheparameterssoastoencourage“informationﬂow.”\nInparticular,wewouldlikethegradientvector∇h( ) t Lbeingback-propagatedto\n4 1 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\nendofthesequence.Formally,wewant\n(∇h( ) t L)∂h( ) t\n∂h( 1 ) t −(10.50)\ntobeaslargeas\n∇h( ) t L. (10.51)\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\nΩ =\nt\n|∇(h( ) t L)∂ h( ) t\n∂ h( 1 ) t −|\n||∇h( ) t L||−1\n2\n. (10.52)\nComputingthegradientofthisregularizermayappeardiﬃcult,butPascanu",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\nvectors∇h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\nthedependenciesthatanRNNcanlearn. BecauseitkeepstheRNNdynamics\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\nAkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor\ntaskswheredataisabundant,suchaslanguagemodeling.\n10.12ExplicitMemory\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\ntherearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-\nconscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooks",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\nstraightforwardtoputintowords—everydaycommonsense knowledge,like“acat\nisakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplish\nyourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom\n141.”\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\nmemorizefacts. Stochasticgradientdescentrequiresmanypresentationsofthe\n4 1 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nT ask   ne t w or k ,\nc ontrol l i ng th e   m e m o r yMe m or y   c e l l s\nW r i t i ng\nm e c hani s mR e adi ng\nm e c hani s m\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\ndistinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrent\nnetinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "storefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfrom\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).\n4 1 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\ninformationthat arerelevantto achieving some goal.Suchexplicit memory\ncomponentswouldallowoursystemsnotonlytorapidlyand“intentionally”store",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "andretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneed\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\nresponsestotheinput(,). Hinton1990\nToresolvethisdiﬃculty,Weston2014 e t a l .()introducedmemorynetworks\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "nism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "relatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).\nEachmemorycellcanbethoughtofasanextensionofthememorycellsin\nLSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstate\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\ndigitalcomputerreadfromorwritetoaspeciﬁcaddress.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "digitalcomputerreadfromorwritetoaspeciﬁcaddress.\nItisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.To\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\nmodifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperations\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "thefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\ndescent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshould\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\nmemoryaddressesreceivingalargecoeﬃcient.\nThesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\n4 1 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\ncostofaccessingamemorycell. Wepaythecomputational costofproducinga\ncoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmall\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\noﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "theyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\nreadinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘We\nallliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "maketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredina\nseparatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.\nWecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsof\nthesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensible\nmechanismevenwhenthememorycellsaresmall.\nIfthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "theinformationitcontainscanbepropagatedforwardintimeandthegradients\npropagatedbackwardintimewithouteithervanishingorexploding.\nTheexplicitmemoryapproachisillustratedinﬁgure,whereweseethat 10.18\na“taskneuralnetwork” iscoupledwithamemory.Althoughthattaskneural\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\nThetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses.\nExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "RNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\nforverylongdurations.\nAsanalternativetoback-propagationthroughweightedaveragesofmemory\ncells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilities and\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "insection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\ndecisions.\nWhetheritissoft(allowingback-propagation) orstochasticandhard,the\n4 1 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmechanism forchoosing anaddress isin itsform identical totheattention\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\ntranslation( ,)anddiscussedinsection. Theidea Bahdanau e t a l .2015 12.4.5.1\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "thecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\nattentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep.\nRecurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\ntasks.\n4 2 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 5\nMac h i n e L e ar n i n g B asics\nDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstand\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\non traditional machinelearning techniques thathavestrongly inﬂuenced the\ndevelopmentofdeeplearningalgorithms.\nWebeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentan\nexample:thelinearregressionalgorithm. W ethenproceedtodescribehowthe\nchallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatterns",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "thatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\nonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthe\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\nexamplesofsimplelearningalgorithmsfromeachcategory. Mostdeeplearning\nalgorithmsare basedonan optimization algorithmcalled stochasticgradient\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\n98",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\nThesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\novercometheseobstacles.\n5.1LearningAlgorithms\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\nwhatdowemeanbylearning?Mitchell1997()providesthedeﬁnition“Acomputer",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\nimproveswithexperienceE.”Onecanimagineaverywidevarietyofexperiences\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\nbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities.\nInstead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\ndiﬀerentkindsoftasks,performance measuresandexperiencesthatcanbeused",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "toconstructmachinelearningalgorithms.\n5.1.1TheTask, T\nMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewith\nﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcand\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\nprinciplesthatunderlieintelligence.\nInthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearning\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "task.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\nWecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\naprogramthatspeciﬁeshowtowalkmanually.\nMachinelearningtasksareusuallydescribedintermsofhowthemachine\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "vectorx∈ Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\n9 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\ncommonmachinelearningtasksincludethefollowing:\n•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecify\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\nalgorithmisusuallyaskedtoproduceafunctionf: Rn→{1,...,k}.When\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\nidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "task,forexample,wherefoutputsaprobabilitydistributionoverclasses.\nAnexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinput\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\ndiﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "deeplearning( ,; ,).Object Krizhevskyetal.2012IoﬀeandSzegedy2015\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\ntheirusers.\n•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechal-\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "task,thelearningalgorithmonlyhastodeﬁneafunctionmapping single\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\nbemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearning\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\nfyingxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituation\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\nareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargeset",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "offunctionsistolearnaprobabilitydistributionoveralloftherelevant\nvariables,thensolvetheclassiﬁcationtaskbymarginalizing outthemissing\nvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁ-\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.\nSeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "sectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcation\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando.\n1 0 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\nisaskedtooutputafunctionf: Rn→ R.Thistypeoftaskissimilarto\nclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleof\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\nalgorithmictrading.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "algorithmictrading.\n•Transcription:Inthistypeoftask,themachinelearningsystemisasked\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\nrecognition,thecomputerprogramisshownaphotographcontainingan\nimageoftextandisaskedtoreturnthistextintheformofasequence\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal.\n2012b).\n•Machinetranslation:Inamachinetranslationtask,theinputalready\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "mustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\n•Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\nimportantrelationshipsbetweenthediﬀerentelements.Thisisabroad",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "category,andsubsumesthetranscriptionandtranslationtasksdescribed\nabove,butalsomanyothertasks.Oneexampleisparsing—mappinga\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.\nSee ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\ntask.Anotherexampleispixel-wisesegmentationofimages, wherethe\ncomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.For\n1 0 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "calledstructuredoutputtasksbecausetheprogrammustoutputseveral\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\nanimagecaptioningprogrammustformavalidsentence.\n•Anomalydetection:Inthistypeoftask,thecomputerprogramsifts\nthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusual\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "information,thethief’spurchaseswilloftencomefromadiﬀerentprobability\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\nsurveyofanomalydetectionmethods.\n•Synthesisandsampling:Inthistypeoftask,themachinelearningal-\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\ntrainingdata. Synthesisandsamplingviamachinelearningcanbeuseful",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "formediaapplicationswhereitcanbeexpensiveorboringforanartistto\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespeciﬁc\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "containingaspokenversionofthatsentence. Thisisakindofstructured\noutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrect\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.\n•Imputationofmissingvalues:Inthistypeoftask,themachinelearning\nalgorithmisgivenanewexamplex∈ Rn,butwithsomeentriesx iofx\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\nentries.\n1 0 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\ninputacorruptedexample˜x∈ Rnobtainedbyanunknowncorruptionprocess\nfromacleanexamplex∈ Rn.Thelearnermustpredictthecleanexample\nxfromitscorruptedversion˜x,ormoregenerallypredicttheconditional\nprobabilitydistributionp(x|˜x).\n•Densityestimationorprobabilitymassfunctionestimation:In\nthedensityestimationproblem,themachinelearningalgorithmisasked\ntolearnafunctionpmodel: Rn→ R,wherepmodel(x)canbeinterpreted",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "asaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.\nTodosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistribution.Densityestimationallowsustoexplicitlycapture\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\navaluex iismissingandalloftheothervalues,denotedx − i,aregiven,\nthenweknowthedistributionoveritisgivenbyp(x i|x − i).Inpractice,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\nintractable.\nOfcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\ndo,nottodeﬁnearigidtaxonomyoftasks.\n5.1.2ThePerformanceMeasure, P\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "speciﬁctothetaskbeingcarriedoutbythesystem. T\nFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\n1 0 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\nifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\nloss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodel",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "acontinuous-valuedscoreforeachexample.Themostcommonapproachisto\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.\nUsuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\nlearningsystem.\nThechoiceofperformancemeasuremayseemstraightforwardandobjective,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "butitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswellto\nthedesiredbehaviorofthesystem.\nInsomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.\nForexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\nofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grained\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "systemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\nInothercases,weknowwhatquantitywewouldideallyliketomeasure,but\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\naspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "mustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\nordesignagoodapproximationtothedesiredcriterion.\n5.1.3TheExperience, E\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\nlearningprocess.\nMostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\n1 0 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndeﬁnedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\ndiﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.\nThefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "alsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesare\nrepresentedinthedataset.\nUnsupervisedlearningalgorithmsexperienceadatasetcontainingmany\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "performotherroles,likeclustering,whichconsistsofdividingthedatasetinto\nclustersofsimilarexamples.\nSupervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\ndiﬀerentspeciesbasedontheirmeasurements.\nRoughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "ofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "teacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\nUnsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.\nThelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\nthatforavector x∈ Rn,thejointdistributioncanbedecomposedas\np() = xn\ni=1p(x i|x1,...,x i −1). (5.1)\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "modelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\n1 0 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\nunsupervised learningtechnologiesto learn thejointdistributionp( x,y)and\ninferring\npy(| x) =p,y( x)\nyp,y( x). (5.2)\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "andstructuredoutputproblemsassupervisedlearning.Densityestimationin\nsupportofothertasksisusuallyconsideredunsupervisedlearning.\nOthervariantsofthelearningparadigmarepossible.Forexample,insemi-\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "withdeepmodels,seeKotzias 2015etal.().\nSomemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences. Such\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\nwhichareinturncollectionsoffeatures.\nOnecommonwayofdescribingadatasetiswitha .Adesign designmatrix\nmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthe\nmatrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "thedatasetwithadesignmatrixX∈ R1504 ×,whereX i ,1isthesepallengthof\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\nOfcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\nThisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\nwithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerent",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "numbersofpixels,sonotallofthephotographs maybedescribedwiththesame\nlengthofvector.Sectionandchapterdescribehowtohandlediﬀerent 9.7 10\n1 0 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\nx() iandx() jhavethesamesize.\nInthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "appearsineachofthephotos.Wemightdothiswithanumericcode,with0\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Justasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\n5.1.4Example:LinearRegression\nOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapable\nofimprovingacomputerprogram’sperformanceatsometaskviaexperienceis\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "simplemachinelearningalgorithm:linearregression.Wewillreturntothis\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\nunderstanditsbehavior.\nAsthenameimplies,linearregressionsolvesaregressionproblem. Inother\nwords,thegoalistobuildasystemthatcantakeavectorx∈ Rnasinputand\npredictthevalueofascalary∈ Rasitsoutput.Inthecaseoflinearregression,\ntheoutputisalinearfunctionoftheinput.Letˆybethevaluethatourmodel\npredictsshouldtakeon.Wedeﬁnetheoutputtobe y\nˆy= wx (5.3)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "ˆy= wx (5.3)\nwherew∈ Rnisavectorof .parameters\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\nthecoeﬃcientthatwemultiplybyfeaturex ibeforesummingupthecontributions\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\neachfeatureaﬀectstheprediction. If afeaturex ireceivesapositiveweightw i,\n1 0 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction ˆy.\nIfafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\ndecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,\nthenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasno\neﬀectontheprediction.\nWethushaveadeﬁnitionofourtaskT: topredictyfromxbyoutputting\nˆy= wx.Nextweneedadeﬁnitionofourperformancemeasure,.P",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "Supposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\ntargetsasy()test.\nOnewayofmeasuringtheperformanceofthemodelistocomputethemean\nsquarederrorofthemodelonthetestset.Ifˆy()testgivesthepredictionsofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "modelonthetestset,thenthemeansquarederrorisgivenby\nMSEtest=1\nm\ni(ˆy()test−y()test)2\ni. (5.4)\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when ˆy()test=y()test.\nWecanalsoseethat\nMSEtest=1\nm||ˆy()test−y()test||2\n2, (5.5)\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\nandthetargetsincreases.\nTomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "isallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\nminimizethemeansquarederroronthetrainingset,MSEtrain.\nTominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\n∇ wMSEtrain= 0 (5.6)\n⇒∇ w1\nm||ˆy()train−y()train||2\n2= 0 (5.7)\n⇒1\nm∇ w||X()trainwy−()train||2\n2= 0 (5.8)\n1 0 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n− − 1 0 . 0 5 0 0 0 5 1 0 . . . .\nx1− 3− 2− 10123yL i n ea r r eg r es s i o n ex a m p l e\n0 5 1 0 1 5 . . .\nw10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "tosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\n⇒∇ w\nX()trainwy−()train\nX()trainwy−()train\n= 0(5.9)\n⇒∇ w\nwX()train X()trainww−2X()train y()train+y()train y()train\n= 0\n(5.10)\n⇒2X()train X()trainwX−2()train y()train= 0(5.11)\n⇒w=\nX()train X()train−1\nX()train y()train(5.12)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "X()train X()train−1\nX()train y()train(5.12)\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\nseeﬁgure.5.1\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\naslightlymoresophisticatedmodelwithoneadditionalparameter—an intercept\nterm.Inthismodelb\nˆy= wx+b (5.13)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "term.Inthismodelb\nˆy= wx+b (5.13)\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\nmappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensionto\naﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikea\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\n1 0 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\nplaystheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”when\nreferringtoaﬃnefunctionsthroughoutthisbook.\nTheintercepttermbisoftencalledthebiasparameteroftheaﬃnetransfor-\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "isdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimation\nalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.\nLinearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\nlearningalgorithms.\n5.2Capacity,OverﬁttingandUnderﬁtting",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "5.2Capacity,OverﬁttingandUnderﬁtting\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\npreviouslyunseeninputs—notjustthoseonwhichourmodelwastrained. The\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.\nTypically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "anoptimization problem.Whatseparatesmachinelearningfromoptimization is\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\nwell. Thegeneralization errorisdeﬁnedastheexpectedvalueoftheerrorona\nnewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawn\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice.\nWetypicallyestimatethegeneralization errorofamachinelearningmodelby\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "fromthetrainingset.\nInourlinearregressionexample,wetrainedthemodelbyminimizingthe\ntrainingerror,\n1\nm()train||X()trainwy−()train||2\n2, (5.14)\nbutweactuallycareaboutthetesterror,1\nm()test||X()testwy−()test||2\n2.\nHowcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythe\n1 1 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntrainingset?Theﬁeldofstatisticallearningtheoryprovidessomeanswers.If\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\ndo.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\nsetarecollected,thenwecanmakesomeprogress.\nThetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\ncalledthedatageneratingprocess.Wetypicallymakeasetofassumptions\nknowncollectivelyasthei.i.d. assumptions. Theseassumptionsarethatthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "examplesineachdatasetareindependentfromeachother,andthatthetrain\nsetandtestsetareidenticallydistributed,drawnfromthesameprobability\ndistributionaseachother. Thisassumptionallowsustodescribethedatagen-\neratingprocesswithaprobabilitydistributionoverasingleexample.Thesame\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample.\nWecallthatsharedunderlyingdistributionthedatageneratingdistribution,\ndenotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "mathematically studytherelationshipbetweentrainingerrorandtesterror.\nOneimmediateconnectionwecanobservebetweenthetrainingandtesterror\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\nexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution\np(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\nset.Forsomeﬁxedvaluew,theexpectedtrainingseterrorisexactlythesameas\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "datasetsamplingprocess.Theonlydiﬀerencebetweenthetwoconditionsisthe\nnameweassigntothedatasetwesample.\nOfcourse, when weuseamachinelearning algorithm, w edonotﬁxthe\nparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\ntestset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\ntheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine\nlearningalgorithmwillperformareitsabilityto:",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmwillperformareitsabilityto:\n1. Makethetrainingerrorsmall.\n2. Makethegapbetweentrainingandtesterrorsmall.\nThesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\nunderﬁttingandoverﬁtting.Underﬁttingoccurswhenthemodelisnotableto\nobtainasuﬃcientlylowerrorvalueonthetrainingset.Overﬁttingoccurswhen\nthegapbetweenthetrainingerrorandtesterroristoolarge.\nWecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyaltering",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "itscapacity.Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof\n1 1 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Models\nwithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdo\nnotservethemwellonthetestset.\nOnewaytocontrolthecapacityofalearningalgorithmisbychoosingits\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "linearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\nhypothesisspace.Doingsoincreasesthemodel’scapacity.\nApolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\narealreadyfamiliar,withprediction\nˆybwx. = + (5.15)\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\ncanlearnamodelthatisquadraticasafunctionof:x\nˆybw = +1xw+2x2. (5.16)\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "stillalinearfunctionoftheparameters,sowecanstillusethenormalequations\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\nˆyb= +9\ni=1w ixi. (5.17)\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\namountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacity\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "tasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\nmayoverﬁt.\nFigureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\nanddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlying\nfunctionisquadratic. Thelinearfunctionisunabletocapturethecurvaturein\nthetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableof\nrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitely\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\n1 1 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\nasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.In\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\nthetasksoitgeneralizeswelltonewdata.\n          \n                  \n          \nFigure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "generatedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\nbyevaluatingaquadraticfunction.  ( L e f t )Alinearfunctionﬁttothedatasuﬀersfrom\nunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\nquadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfrom\nasigniﬁcantamountofoverﬁttingorunderﬁtting.Apolynomialofdegree9ﬁtto ( R i g h t )\nthedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolve",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "theunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\nItnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\nfunctiondecreasesinthisarea.\nSofarwehavedescribedonlyonewayofchangingamodel’scapacity:by\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "parametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\namodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\nmodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\ntherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebest\nfunctionwithinthisfamilyisaverydiﬃcultoptimization problem.Inpractice,\nthelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyone",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "thatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchas\n1 1 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm’s\neﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodel\nfamily.\nOurmodernideasaboutimprovingthegeneralization ofmachinelearning\nmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearly\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\nmostwidelyknownasOccam’srazor(c.1287-1347).Thisprinciplestatesthat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "amongcompetinghypothesesthatexplainknownobservationsequallywell,one\nshouldchoosethe“simplest”one.Thisideawasformalizedandmademoreprecise\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,).\nStatisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\nAmongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.The",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "VCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthere\nexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.\nQuantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\nintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyare\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\ndiﬃculttodeterminethecapacityofdeeplearningalgorithms. Theproblemof\ndeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausethe\neﬀectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "wehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\nproblemsinvolvedindeeplearning.\nWemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\nsuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,training\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "generalization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\nillustratedinﬁgure.5.3\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\n1 1 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n0 O pti m a l C a pa c i t y\nC a pa c i t yE r r o rU nde r ﬁtti ng z o ne O v e r ﬁtti ng z o ne\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\nG e n e r a l i z a t i o n e r r o r\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\nbehavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\narebothhigh.Thisistheunderﬁttingregime.Asweincreasecapacity,trainingerror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "decreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁtting\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.\ntheconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\nbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.\nNon-parametric modelshavenosuchlimitation.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "Non-parametric modelshavenosuchlimitation.\nSometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\nwhichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodel",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "simplystorestheXandyfromthetrainingset. Whenaskedtoclassifyatest\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\nassociatedregressiontarget.Inotherwords,ˆy=y iwherei=argmin||X i ,:−||x2\n2.\nThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "mightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerent\noutputs)onanyregressiondataset.\nFinally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\n1 1 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\npolynomialexpansionoftheinput.\nTheidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\nthatgeneratesthedata. Evensuchamodelwillstillincursomeerroronmany\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "ofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\ndistributioniscalledthe p,y(x)Bayeserror.\nTrainingandgeneralization errorvaryasthesizeofthetrainingsetvaries.\nExpectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "thebestpossibleerrorisachieved.Anyﬁxedparametricmodelwithlessthan\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\nﬁgureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.\nInthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\nexamples.\n5.2.1TheNoFreeLunchTheorem\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "aﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\nisnotlogicallyvalid. Tologicallyinferaruledescribingeverymemberofaset,\nonemusthaveinformationabouteverymemberofthatset.\nInpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machine\nlearningpromisestoﬁndrulesthatareprobably most correctaboutmembersof\nthesettheyconcern.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "thesettheyconcern.\nUnfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\nallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthe\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\n1 1 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n      \n                                                    \n                \n              \n                     \n                       \n      \n                                                     ",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "Figure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\nandthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40\ndiﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals.\n( T o p )TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "andamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.For\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\nThisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "trainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\ntomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,\nthetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoat\nleasttheBayeserror. Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimal\ncapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.\n1 1 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\ntothesameclass.\nFortunately,theseresultsholdonlywhenweaverageoverpossibledata all\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\nalgorithmsthatperformwellonthesedistributions.\nThismeansthatthegoalofmachinelearningresearchisnottoseekauniversal",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\nunderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAI\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.\n5.2.2Regularization\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\nalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetof\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "thelearningproblemsweaskthealgorithmtosolve,itperformsbetter.\nSofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\nconcretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyadding\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\nisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthe\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\nfarisoversimpliﬁed.\nThebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "makethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentity\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "controltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\nfunctions.\nWecanalsogivealearningalgorithmapreferenceforonesolutioninits\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\nispreferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetraining\n1 1 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndatasigniﬁcantlybetterthanthepreferredsolution.\nForexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,\nJ() = wMSEtrain+λww, (5.18)\nwhereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "forsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcesthe\nweightstobecomesmaller. MinimizingJ(w)resultsinachoiceofweightsthat\nmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesus\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\nexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweight\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvalues\nof.Seeﬁgurefortheresults. λ 5.5\n           \n         ",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "           \n         \n                       \n         \n          \n    \nFigure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingset\nfromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.\n( L e f t )Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "all.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. λ\nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\ncoeﬃcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "degree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2\n1 1 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)by\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\ndecay,theregularizerisΩ(w) =ww.Inchapter,wewillseethatmanyother 7\nregularizersarepossible.\nExpressingpreferencesforonefunctionoveranotherisamoregeneralway\nofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthe\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "expressinganinﬁnitelystrongpreferenceagainstthatfunction.\nInourweightdecayexample,weexpressedourpreferenceforlinearfunctions\ndeﬁnedwithsmallerweightsexplicitly, viaanextraterminthecriterionwe\nminimize.Thereare many otherwaysof expressing preferencesfor diﬀerent\nsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproaches\nareknownasregularization. Regularizationisanymodiﬁcationwemaketoa\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "trainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachine\nlearning,rivaledinitsimportanceonlybyoptimization.\nThenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "thatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeforms\nofregularization.\n5.3HyperparametersandValidationSets\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\nitself(thoughwecan designa nestedlearning procedure where one learning\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "Inthepolynomialregressionexamplewesawinﬁgure,thereisasingle 5.2\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\nparameter.Theλvalueusedtocontrolthestrengthofweightdecayisanother\nexampleofahyperparameter.\nSometimesasettingischosentobeahyperparameter thatthelearningal-\ngorithmdoesnotlearnbecauseitisdiﬃculttooptimize.Morefrequently,the\n1 2 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\nalwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refer\ntoﬁgure).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher 5.3\ndegreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalower\ndegreepolynomialandapositiveweightdecaysetting.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "degreepolynomialandapositiveweightdecaysetting.\nTosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\nalgorithmdoesnotobserve.\nEarlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\nitshyperparameters . Forthisreason,noexamplefromthetestsetcanbeused",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "inthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\ntrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.One\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "Thesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\n20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters ,\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\niscomplete,thegeneralization errormaybeestimatedusingthetestset.\nInpractice, when thesametestsethasbeenusedrepeatedlytoevaluate",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "performanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsider\nalltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthe\ntrueﬁeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\n1 2 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.3.1Cross-Validation\nDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematic\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\naroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithm\nAworksbetterthanalgorithmonthegiventask. B\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "tousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\nthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplits\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "dataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\nused.\n5.4Estimators,BiasandVariance\nTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.\nFoundationalconceptssuchasparameterestimation,biasandvarianceareuseful",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "toformallycharacterizenotionsofgeneralization, underﬁttingandoverﬁtting.\n5.4.1PointEstimation\nPointestimationistheattempttoprovidethesingle“best”predictionofsome\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\nconventionwillbetodenoteapointestimateofaparameterbyθ ˆθ.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Let{x(1),...,x() m}beasetofmindependentandidenticallydistributed\n1 2 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "inthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\nofunsupervisedlearning. The algorithmreturnsthevectoroferrorseforeach\nexamplein D,whosemeanistheestimatedgeneralization error. Theerrorson\nindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean\n(equation). Whiletheseconﬁdenceintervalsarenotwell-justiﬁedafterthe 5.47\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\nalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "ofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm\nB.\nDeﬁneKFoldXV(): D,A,L,k\nRequire: D,thegivendataset,withelementsz() i\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\ninputandoutputsalearnedfunction\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\nanexamplez() i∈ ∈ Dtoascalar R\nRequire:k,thenumberoffolds\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis. D\nfordoikfromto1\nf i= (A D D\\ i)\nforz() jin D ido\ne j= (Lf i,z() j)\nendfor",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "forz() jin D ido\ne j= (Lf i,z() j)\nendfor\nendfor\nReturne\n1 2 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\nˆθ m= (gx(1),...,x() m). (5.19)\nThedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\nθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ.\nThisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofan\nestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthat\ngeneratedthetrainingdata.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "generatedthetrainingdata.\nFornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\nthatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimate\nˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\nfunctionofthedataisrandom.Therefore ˆθisarandomvariable.\nPointestimationcanalsorefertotheestimationoftherelationshipbetween\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\nestimators.\nFunctionEstimationAswementionedabove,sometimesweareinterestedin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "performingfunctionestimation(orfunctionapproximation).Herewearetryingto\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\nwemayassumethaty=f(x)+,wherestandsforthepartofythatisnot\npredictablefromx. Infunctionestimation,weareinterestedinapproximating\nfwithamodelorestimate ˆf.Functionestimationisreallyjustthesameas\nestimatingaparameterθ;thefunctionestimator ˆfissimplyapointestimatorin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "functionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\nafunction ˆf y mappingfromtox.\nWenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\ndiscusswhattheytellusabouttheseestimators.\n5.4.2Bias\nThebiasofanestimatorisdeﬁnedas:\nbias(ˆθ m) = ( Eˆθ m)−θ (5.20)\n1 2 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\nandθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistri-\nbution.Anestimator ˆθ missaidtobeunbiasedifbias(ˆθ m) = 0,whichimplies\nthat E(ˆθ m)=θ.Anestimator ˆθ missaidtobeasymptoticallyunbiasedif\nlim m → ∞bias(ˆθ m) = 0,whichimpliesthatlim m → ∞ E(ˆθ m) = θ.\nExample:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\nbutionwithmean:θ\nPx(() i;) = θθx() i(1 )−θ(1 − x() i). (5.21)\nAcommonestimatorfortheθparameterofthisdistributionisthemeanofthe\ntrainingsamples:\nˆθ m=1\nmm\ni=1x() i. (5.22)\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\nintoequation:5.20\nbias(ˆθ m) = [ Eˆθ m]−θ (5.23)\n= E\n1\nmm\ni=1x() i\n−θ (5.24)\n=1\nmm\ni=1E\nx() i\n−θ (5.25)\n=1\nmm\ni=11\nx() i=0\nx() iθx() i(1 )−θ(1 − x() i)\n−θ(5.26)\n=1\nmm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "x() iθx() i(1 )−θ(1 − x() i)\n−θ(5.26)\n=1\nmm\ni=1()θ−θ (5.27)\n= = 0θθ− (5.28)\nSince bias(ˆθ) = 0,wesaythatourestimator ˆθisunbiased.\nExample:GaussianDistributionEstimatoroftheMeanNow,consider\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\naccordingtoaGaussiandistributionp(x() i) =N(x() i;µ,σ2),wherei∈{1,...,m}.\n1 2 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\npx(() i;µ,σ2) =1√\n2πσ2exp\n−1\n2(x() i−µ)2\nσ2\n.(5.29)\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\nmean:\nˆµ m=1\nmm\ni=1x() i(5.30)\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\nitsexpectation:\nbias(ˆµ m) = [ˆ Eµ m]−µ (5.31)\n= E\n1\nmm\ni=1x() i\n−µ (5.32)\n=\n1\nmm\ni=1E\nx() i\n−µ (5.33)\n=\n1\nmm\ni=1µ\n−µ (5.34)\n= = 0µµ− (5.35)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "=\n1\nmm\ni=1µ\n−µ (5.34)\n= = 0µµ− (5.35)\nThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmean\nparameter.\nExample:EstimatorsoftheVarianceofaGaussianDistributionAsan\nexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofa\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\nTheﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:\nˆσ2\nm=1\nmm\ni=1\nx() i−ˆµ m2\n, (5.36)\nwhere ˆµ misthesamplemean,deﬁnedabove.Moreformally,weareinterestedin\ncomputing\nbias(ˆσ2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "computing\nbias(ˆσ2\nm) = [ˆ Eσ2\nm]−σ2(5.37)\n1 2 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nWebeginbyevaluatingtheterm E[ˆσ2\nm]:\nE[ˆσ2\nm] = E\n1\nmm\ni=1\nx() i−ˆµ m2\n(5.38)\n=m−1\nmσ2(5.39)\nReturningtoequation,weconcludethatthebiasof 5.37 ˆσ2\nmis−σ2/m.Therefore,\nthesamplevarianceisabiasedestimator.\nTheunbiasedsamplevarianceestimator\n˜σ2\nm=1\nm−1m\ni=1\nx() i−ˆµ m2\n(5.40)\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\nThatis,weﬁndthat E[˜σ2\nm] = σ2:\nE[˜σ2\nm] = E\n1\nm−1m\ni=1\nx() i−ˆµ m2\n(5.41)\n=m\nm−1E[ˆσ2\nm] (5.42)\n=m\nm−1m−1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "(5.41)\n=m\nm−1E[ˆσ2\nm] (5.42)\n=m\nm−1m−1\nmσ2\n(5.43)\n= σ2. (5.44)\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\nestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswe\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\n5.4.3VarianceandStandardError\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "Thevarianceofanestimatorissimplythevariance\nVar(ˆθ) (5.45)\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\nvarianceiscalledthe ,denotedstandarderror SE(ˆθ).\n1 2 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\nlowvariance.\nWhenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimate\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "obtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\nbeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceof\nerrorthatwewanttoquantify.\nThestandarderrorofthemeanisgivenby\nSE(ˆµ m) =Var\n1\nmm\ni=1x() i\n=σ√m, (5.46)\nwhereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften\nestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootof\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "provideanunbiasedestimateofthestandarddeviation.Bothapproachestend\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\nForlarge,theapproximation isquitereasonable. m\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\nWeoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "accuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\nfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredon\nthemean ˆµ mis\n(ˆµ m−196SE( ˆ.µ m)ˆ,µ m+196SE( ˆ.µ m)), (5.47)\nunderthenormaldistributionwithmean ˆµ mandvariance SE(ˆµ m)2.Inmachine\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "Biftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAis\nlessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithm\nB.\n1 2 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nExample: BernoulliDistributionWeonceagainconsiderasetofsamples\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\n(recallP(x() i;θ) =θx() i(1−θ)(1 − x() i)).Thistimeweareinterestedincomputing\nthevarianceoftheestimator ˆθ m=1\nmm\ni=1x() i.\nVar\nˆθ m\n= Var\n1\nmm\ni=1x() i\n(5.48)\n=1\nm2m\ni=1Var\nx() i\n(5.49)\n=1\nm2m\ni=1θθ (1−) (5.50)\n=1\nm2mθθ (1−) (5.51)\n=1\nmθθ (1−) (5.52)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "=1\nm2mθθ (1−) (5.51)\n=1\nmθθ (1−) (5.52)\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\nreturntowhenwediscussconsistency(seesection).5.4.5\n5.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquared\nError\nBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Bias\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\nVarianceontheotherhand,providesameasureofthedeviationfromtheexpected",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "estimatorvaluethatanyparticularsamplingofthedataislikelytocause.\nWhathappenswhenwearegivenachoicebetweentwoestimators,onewith\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\nﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand 5.2\nonethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?\nThemostcommonwaytonegotiatethistrade-oﬀistousecross-validation.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\nMSE = [( Eˆθ m−θ)2] (5.53)\n= Bias(ˆθ m)2+Var(ˆθ m) (5.54)\n1 2 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nTheMSEmeasurestheoverallexpecteddeviation—in asquarederrorsense—\nbetweentheestimatorandthetruevalueoftheparameterθ.Asisclearfrom\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\nmanagetokeepboththeirbiasandvariancesomewhatincheck.\nC apac i t yB i as Ge ne r al i z at i on\ne r r orV ar i anc e\nO pt i m al\nc apac i t yO v e r ﬁt t i ng z o n e U nde r ﬁt t i ng z o n e",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁtting\nwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationship\nissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedin\nsectionandﬁgure. 5.2 5.3\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "learningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\nanddecreasebias.Thisisillustratedinﬁgure,whereweseeagaintheU-shaped 5.6\ncurveofgeneralization errorasafunctionofcapacity.\n5.4.5Consistency\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\nﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "amountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\n1 3 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\nplimm → ∞ˆθ m= θ. (5.55)\nThesymbolplimindicatesconvergenceinprobability,meaningthatforany>0,\nP(|ˆθ m−|θ>)→0asm→∞.Theconditiondescribedbyequationis5.55\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\nstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almost\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\noccurswhenp(lim m → ∞ x() m= ) = 1x.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "occurswhenp(lim m → ∞ x() m= ) = 1x.\nConsistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\nnumberofdataexamplesgrows.However,thereverseisnottrue—asymptotic\nunbiasednessdoesnotimplyconsistency. Forexample,considerestimatingthe\nmeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsisting\nofmsamples:{x(1),...,x() m}.Wecouldusetheﬁrstsamplex(1)ofthedataset\nasanunbiasedestimator:ˆθ=x(1).Inthatcase, E(ˆθ m)=θsotheestimator",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "isunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\nestimatorasitisthecasethat not ˆθ m→ →∞θmas.\n5.5MaximumLikelihoodEstimation\nPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzed\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁc",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "functionsthataregoodestimatorsfordiﬀerentmodels.\nThemostcommonsuchprincipleisthemaximumlikelihoodprinciple.\nConsiderasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\nthetruebutunknowndatageneratingdistributionpdata() x.\nLetpmodel( x;θ)beaparametricfamilyofprobabilitydistributionsoverthe\nsamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationx\ntoarealnumberestimatingthetrueprobabilitypdata()x.\nThemaximumlikelihoodestimatorforisthendeﬁnedas θ\nθML= argmax\nθpmodel(;) Xθ (5.56)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "θML= argmax\nθpmodel(;) Xθ (5.56)\n= argmax\nθm\ni=1pmodel(x() i;)θ (5.57)\n1 3 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.\nForexample,itispronetonumericalunderﬂow.Toobtainamoreconvenient\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\nintoasum:\nθML= argmax\nθm\ni=1logpmodel(x() i;)θ. (5.58)\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "withrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:\nθML= argmax\nθE x ∼ˆ pdatalogpmodel(;)xθ. (5.59)\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\nthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetraining\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\nDKL(ˆpdatapmodel) = E x ∼ˆ pdata[log ˆpdata()logx−pmodel()]x.(5.60)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\nneedonlyminimize\n− E x ∼ˆ pdata[logpmodel()]x (5.61)\nwhichisofcoursethesameasthemaximization inequation.5.59\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\nentropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”to\nidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\nentropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthe\nprobabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhe\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.\nWecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\ntributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatch\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\ndistribution.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "distribution.\nWhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthe\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\n1 3 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\narediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.\nMaximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "5.5.1ConditionalLog-LikelihoodandMeanSquaredError\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\nourgoalistoestimateaconditionalprobabilityP( y x|;θ)inordertopredict y\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\ntargets,thentheconditionalmaximumlikelihoodestimatoris\nθML= argmax\nθP. ( ;)YX|θ (5.62)\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\nθML= argmax\nθm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "θML= argmax\nθm\ni=1log(Py() i|x() i;)θ. (5.63)\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\nintroducedearlierinsection,maybejustiﬁedasamaximumlikelihood 5.1.4\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\ntotakeaninputxandproduceanoutputvalue ˆy.Themappingfromxtoˆyis\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "likelihoodestimation.Insteadofproducingasingleprediction ˆy,wenowthink\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\nthatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexamples\nwiththesameinputvaluexbutdiﬀerentvaluesofy. Thegoalofthelearning\nalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvalues\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\nweobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunction ˆy(x;w)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\nthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthis\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\n1 3 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\ngivenby\nm\ni=1log(py() i|x() i;)θ (5.64)\n= log −mσ−m\n2log(2)π−m\ni=1ˆy() i−y() i2\n2σ2,(5.65)\nwhere ˆy() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\nsquarederror,\nMSEtrain=1\nmm\ni=1||ˆy() i−y() i||2, (5.66)\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "thesameestimateoftheparameterswasdoesminimizingthemeansquarederror.\nThetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.This\njustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\n5.5.2PropertiesofMaximumLikelihood\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\nbethebestestimatorasymptotically,asthenumberofexamplesm→∞,interms\nofitsrateofconvergenceasincreases.m",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "ofitsrateofconvergenceasincreases.m\nUnderappropriate conditions, the maximumlikelihood estimatorhas the\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\noftrainingexamplesapproachesinﬁnity,themaximumlikelihoodestimateofa\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\n•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ).\nOtherwise,noestimatorcanrecoverpdata.\n•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. θ\nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\ntor,manyofwhichsharethepropertyofbeingconsistentestimators. However,\n1 3 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nconsistentestimatorscandiﬀerintheirstatisticeﬃciency,meaningthatone\nconsistentestimatormayobtainlowergeneralization errorforaﬁxednumberof\nsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelof\ngeneralization error.\nStatisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinear\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "measurehowclosewearetothetrueparameterisbytheexpectedmeansquared\nerror,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\nformlarge,theCramér-Raolowerbound(,;,)showsthatno Rao1945Cramér1946\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\nestimator.\nForthesereasons(consistencyandeﬃciency),maximumlikelihoodisoften",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "consideredthepreferredestimatortouseformachinelearning.Whenthenumber\nofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategies\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\nthathaslessvariancewhentrainingdataislimited.\n5.6BayesianStatistics\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\ningasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatone\nestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakinga",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "prediction.ThelatteristhedomainofBayesianstatistics.\nAsdiscussed insection , the frequen tist perspective isthat thetrue 5.4.1\nparametervalueθisﬁxedbutunknown,whilethepointestimate ˆθisarandom\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\nTheBayesianperspectiveonstatisticsisquitediﬀerent. The Bayesianuses\nprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetis\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθ",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "isunknownoruncertainandthusisrepresentedasarandomvariable.\nBeforeobservingthedata,werepresentourknowledgeofθusingtheprior\nprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).\nGenerally,themachinelearningpractitionerselectsapriordistributionthatis\nquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthe\n1 3 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nvalueofθbeforeobservinganydata.Forexample,onemightassume that apriori\nθliesinsomeﬁniterangeorvolume,withauniformdistribution. Manypriors\ninsteadreﬂectapreferencefor“simpler” solutions(suchassmallermagnitude\ncoeﬃcients,orafunctionthatisclosertobeingconstant).\nNowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\nrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihood\npx((1),...,x() m|θ)withthepriorviaBayes’rule:",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "px((1),...,x() m|θ)withthepriorviaBayes’rule:\npx(θ|(1),...,x() m) =px((1),...,x() m|θθ)(p)\npx((1),...,x() m)(5.67)\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\nfewhighlylikelyvaluesoftheparameters.\nRelativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwo",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "importantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakes\npredictionsusingapointestimateofθ,theBayesianapproachistomakepredictions\nusingafulldistributionoverθ.Forexample,afterobservingmexamples,the\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\npx((+1) m|x(1),...,x() m) =\npx((+1) m| |θθ)(px(1),...,x() m)d.θ(5.68)\nHereeachvalueofθwithpositiveprobabilitydensitycontributestotheprediction\nofthenextexample,withthecontributionweightedbytheposteriordensityitself.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\nvalueofθ,thenthisuncertaintyisincorporated directlyintoanypredictionswe\nmightmake.\nInsection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\ntaintyinagivenpointestimateofθbyevaluatingitsvariance.Thevarianceof\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "protectwellagainstoverﬁtting. Thisintegralisofcoursejustanapplicationof\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\nestimate.\nThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimation\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\n1 3 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\npriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensity\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\nCriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\njudgmentimpactingthepredictions.\nBayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\nisavailable,buttypicallysuﬀerfromhighcomputational costwhenthenumberof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "trainingexamplesislarge.\nExample:BayesianLinearRegressionHereweconsidertheBayesianesti-\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\nwelearnalinearmappingfromaninputvectorx∈ Rntopredictthevalueofa\nscalar.Thepredictionisparametrized bythevector y∈ R w∈ Rn:\nˆy= wx. (5.69)\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\nofovertheentiretrainingsetas: y\nˆy()train= X()trainw. (5.70)\nExpressedasaGaussianconditionaldistributionony()train,wehave",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "p(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\n∝exp\n−1\n2(y()train−X()trainw)(y()train−X()trainw)\n,\n(5.72)\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\n(X()train,y()train) ( ) assimplyXy,.\nTodeterminetheposteriordistributionoverthemodelparametervectorw,we\nﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebelief\naboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnatural",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "toexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\naboutθ. Forreal-valuedparametersitiscommontouseaGaussianasaprior\ndistribution:\np() = (;w Nwµ0, Λ0) exp∝\n−1\n2(wµ−0)Λ−1\n0(wµ−0)\n,(5.73)\n1 3 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nwhereµ0and Λ0arethepriordistributionmeanvectorandcovariancematrix\nrespectively.1\nWiththepriorthusspeciﬁed,wecannowproceedindeterminingtheposterior\ndistributionoverthemodelparameters.\np,p,p (wX|y) ∝(yX|w)()w (5.74)\n∝exp\n−1\n2( )yXw−( )yXw−\nexp\n−1\n2(wµ−0)Λ−1\n0(wµ−0)\n(5.75)\n∝exp\n−1\n2\n−2yXww+XXww+Λ−1\n0wµ−2\n0 Λ−1\n0w\n.\n(5.76)\nWenowdeﬁne Λ m=\nXX+ Λ−1\n0 −1andµ m= Λ m\nXy+ Λ−1\n0µ0\n.Using\nthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussian",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "distribution:\np, (wX|y) exp∝\n−1\n2(wµ− m)Λ−1\nm(wµ− m)+1\n2µ\nm Λ−1\nmµ m\n(5.77)\n∝exp\n−1\n2(wµ− m)Λ−1\nm(wµ− m)\n. (5.78)\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\nEquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\neﬀectofBayesianinference.Inmostsituations,wesetµ0to 0.Ifweset Λ0=1\nαI,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "αI,\nthenµ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\nweightdecaypenaltyofαww.OnediﬀerenceisthattheBayesianestimateis\nundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearning\nprocesswithaninﬁnitelywideprioronw.Themoreimportantdiﬀerenceisthat\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\ndiﬀerentvaluesofare,ratherthanprovidingonlytheestimate w µ m.\n5.6.1Maximum (MAP)Estimation A P o s t e ri o ri",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\nposteriordistributionovertheparameterθ,itisstilloftendesirabletohavea\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\nd i a g o n a l c o v a ria n c e m a t rix Λ0= diag( λ0) .\n1 3 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nsinglepointestimate. Onecommonreasonfordesiringapointestimateisthat\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\nintractable,andapointestimateoﬀersatractableapproximation.Ratherthan\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\nthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoice\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "maximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\ncaseofcontinuous):θ\nθMAP= argmax\nθp( ) = argmaxθx|\nθlog( )+log() pxθ|pθ.(5.79)\nWerecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-\nlikelihoodterm,and,correspondingtothepriordistribution. log()pθ\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\nλI2),thenthelog-priortermin\nequationisproportional tothefamiliar 5.79 λwwweightdecaypenalty,plusa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "termthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAP\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\ndecay.\nAswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\nthepriceofincreasedbias.\nManyregularizedestimationstrategies,suchasmaximumlikelihoodlearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "regularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\naddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).Not\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\nOtherregularizationtermsdependonthedata,whichofcourseapriorprobability\ndistributionisnotallowedtodo.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "distributionisnotallowedtodo.\nMAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\ndistribution,astheprior(NowlanandHinton1992,).\n1 3 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.7SupervisedLearningAlgorithms\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\ntrainingsetofexamplesofinputsxandoutputsy. Inmanycasestheoutputs\nymaybediﬃculttocollectautomatically andmustbeprovidedbyahuman\n“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswere\ncollectedautomatically .\n5.7.1ProbabilisticSupervisedLearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "5.7.1ProbabilisticSupervisedLearning\nMost supervised learning algorithms inthis book are based on estimating a\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\nlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamily\nofdistributions .py(|xθ;)\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\npyy (| Nxθ;) = (;θxI,). (5.80)\nWecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁninga\ndiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "class1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\nmustaddupto1.\nThenormaldistributionoverreal-valuednumbersthatweusedforlinear\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "interval(0,1)andinterpretthatvalueasaprobability:\npy σ (= 1 ;) = |xθ (θx). (5.81)\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\nweusethemodelforclassiﬁcationratherthanregression).\nInthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsby\nsolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.There\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "log-likelihood(NLL)usinggradientdescent.\n1 4 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\ntherightkindofinputandoutputvariables.\n5.7.2SupportVectorMachines\nOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvector\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\nlogisticregressioninthatitisdrivenbyalinearfunctionwx+b.Unlikelogistic",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "regression,thesupportvectormachinedoesnotprovideprobabilities, butonly\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\nwx+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\nwx+bisnegative.\nOnekeyinnovationassociatedwithsupportvectormachinesisthekernel\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "bere-writtenas\nwx+= +bbm\ni=1α ixx() i(5.82)\nwherex() iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthe\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\nfunctionφ(x) andthedotproductwithafunctionk(xx,() i) =φ(x)·φ(x() i) called\nakernel.The ·operatorrepresentsaninnerproductanalogoustoφ(x)φ(x() i).\nForsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\nsomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "example,innerproductsbasedonintegrationratherthansummation.Acomplete\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\nAfterreplacingdotproductswithkernelevaluations,wecanmakepredictions\nusingthefunction\nfb () = x +\niα ik,(xx() i). (5.83)\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)\nandf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.The\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "φ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\nThekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\n1 4 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedand\noptimizeonlyα,i.e.,theoptimization algorithmcanviewthedecisionfunction\nasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmits\nanimplementationthatissigniﬁcantlymorecomputational eﬃcientthannaively\nconstructingtwovectorsandexplicitlytakingtheirdotproduct. φ()x\nInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "aninﬁnitecomputational costforthenaive,explicitapproach.Inmanycases,\nk(xx,)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As\nanexampleofaninﬁnite-dimens ionalfeaturespacewithatractablekernel,we\nconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethat\nthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros.\nWecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\ntothecorrespondinginﬁnite-dimens ionaldotproduct.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "tothecorrespondinginﬁnite-dimens ionaldotproduct.\nThemostcommonlyusedkernelistheGaussiankernel\nk, ,σ (uvuv ) = (N −;02I) (5.84)\nwhere N(x;µ, Σ)isthestandardnormaldensity.Thiskernelisalsoknownas\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\nproductinaninﬁnite-dimens ionalspace,butthederivationofthisspaceisless\nstraightforwardthaninourexampleofthekernelovertheintegers. min",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "WecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\nforclassy.WhenatestpointxisnearxaccordingtoEuclideandistance,the\nGaussiankernelhasalargeresponse,indicatingthatxisverysimilartothex\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.\nOverall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\nsimilarityofthecorrespondingtrainingexamples.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "similarityofthecorrespondingtrainingexamples.\nSupportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\norkernelmethods( ,; WilliamsandRasmussen1996Schölkopf1999etal.,).\nAmajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "contributesatermα ik(xx,() i)tothedecisionfunction.Supportvectormachines\nareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.\nClassifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\nthetrainingexamplesthathavenon-zeroα i.Thesetrainingexamplesareknown\n1 4 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nassupportvectors.\nKernelmachinesalsosuﬀerfromahighcomputational costoftrainingwhen\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\n()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\nontheMNISTbenchmark.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "ontheMNISTbenchmark.\n5.7.3OtherSimpleSupervisedLearningAlgorithms\nWehavealreadybrieﬂyencounteredanothernon-probabilis ticsupervisedlearning\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\nafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asa\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxed\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "trainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\nInstead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\nweﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\nanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.In\nthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithc y= 1\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "one-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\nsupposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\nnumberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayes\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\ndistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsx",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "willhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\nofthem,theprocedureconvergestotheBayeserrorrate. Thehighcapacityof\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.\nHowever,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\ngivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatit\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "imaginewehavearegressiontaskwithx∈ R100drawnfromanisotropicGaussian\n1 4 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\nThenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\nfeaturesx2throughx100,notbythelonefeaturex1. Thustheoutputonsmall\ntrainingsetswillessentiallyberandom.\n1 4 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n0\n101\n1110 1\n011\n1111 1110110\n10010\n001110 11111101001 00\n010 01111\n111\n11\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\ndisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtained\nbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom).",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitis\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\nnumberoftrainingexamples.\n1 4 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal.\n1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision 5.7\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\ncut). Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "correspondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "struggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\nwithaxis-alignedsteps.\nAswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "resourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\nlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetween\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\nSee (), (),  ()orothermachine Murphy2012Bishop2006Hastieetal.2001\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\n5.8UnsupervisedLearningAlgorithms\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "only“features”butnotasupervisionsignal.Thedistinctionbetweensupervised\nandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisno\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "ﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\n1 4 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nrelatedexamples.\nAclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthe\ndata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelooking\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\nmoreaccessiblethanitself.x\nTherearemultiplewaysofdeﬁningarepresentation.Threeofthe simpler \nmostcommonincludelowerdimensionalrepresentations,sparserepresentations",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "andindependentrepresentations.Low-dimensionalrepresentationsattemptto\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation.\nSparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "overallstructureoftherepresentationthattendstodistributedataalongtheaxes\noftherepresentationspace.Independentrepresentationsattempttodisentangle\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\noftherepresentationarestatisticallyindependent.\nOf coursethese three criteriaare certainly notmutuallyexclusive.Low-\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "reducethesizeofarepresentationistoﬁndandremoveredundancies.Identifying\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\nachievemorecompressionwhilediscardinglessinformation.\nThenotionofrepresentationisoneofthecentralthemesofdeeplearningand\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "remainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\ndevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria.\n5.8.1PrincipalComponentsAnalysis\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\n1 4 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n− − 2 0 1 0 0 1 0 2 0\nx 1− 2 0− 1 001 02 0x 2\n− − 2 0 1 0 0 1 0 2 0\nz 1− 2 0− 1 001 02 0z 2\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\nwiththeaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.  ( R i g h t )The\ntransformeddataz=xWnowvariesmostalongtheaxisz 1.Thedirectionofsecond\nmostvarianceisnowalongz 2.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "mostvarianceisnowalongz 2.\nrepresentationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\nisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsare\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\nPCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "inputxtoarepresentationzasshowninﬁgure.Insection,wesawthat 5.8 2.12\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\ncorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCA\nasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuch\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "decorrelatestheoriginaldatarepresentation.X\nLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethat\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.\nTheunbiasedsamplecovariancematrixassociatedwithisgivenby:X\nVar[] =x1\nm−1XX. (5.85)\n1 4 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nPCAﬁndsarepresentation(throughlineartransformation)z=xWwhere\nVar[]zisdiagonal.\nInsection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\naregivenbytheeigenvectorsofXX.Fromthisview,\nXXWW = Λ. (5.86)\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.\nSpeciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "rightsingularvectorsinthedecompositionX=UW Σ. Wethenrecoverthe\noriginaleigenvectorequationwithastheeigenvectorbasis: W\nXX=\nUW Σ\nUW Σ= W Σ2W.(5.87)\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\nSVDof,wecanexpressthevarianceofas: X X\nVar[] =x1\nm−1XX (5.88)\n=1\nm−1(UW Σ)UW Σ(5.89)\n=1\nm−1W ΣUUW Σ(5.90)\n=1\nm−1W Σ2W, (5.91)\nwhereweusethefactthatUU=IbecausetheUmatrixofthesingularvalue\ndecompositionisdeﬁnedtobeorthogonal.Thisshowsthatifwetakez=xW,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "wecanensurethatthecovarianceofisdiagonalasrequired: z\nVar[] =z1\nm−1ZZ (5.92)\n=1\nm−1WXXW (5.93)\n=1\nm−1WW Σ2WW (5.94)\n=1\nm−1Σ2, (5.95)\nwherethistimeweusethefactthatWW=I,againfromthedeﬁnitionofthe\nSVD.\n1 4 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\n(asgivenby Σ2)whichimmediatelyimpliesthattheindividualelementsofzare\nmutuallyuncorrelated.\nThisabilityofPCAtotransformdataintoarepresentationwheretheelements\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "variationunderlyingthedata. InthecaseofPCA,thisdisentanglingtakesthe\nformofﬁndingarotationoftheinputspace(describedbyW)thatalignsthe\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\nwith.z\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\ncanbedonewithasimplelineartransformation.\n5.8.2-meansClustering k",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "5.8.2-meansClustering k\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.\nThek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclusters\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\nzero.\nTheone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "representation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\nwewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\nareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁts\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\nsimilartoeachother)anditconfersthecomputational advantagethattheentire",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "representationmaybecapturedbyasingleinteger.\nThek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ() k}\ntodiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.\nInonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\nthenearestcentroidµ() i.Intheotherstep,eachcentroidµ() iisupdatedtothe\nmeanofalltrainingexamplesx() jassignedtocluster.i\n1 5 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherently\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "clusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\nmaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyof\ntherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebut\nobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.For\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\ncars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmay",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "ﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterof\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\nlostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgray\ncars,justastheyareinadiﬀerentclusterfromgraytrucks. Theoutputofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "clusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\nthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisall\nweknow.\nTheseissuesillustratesomeofthereasonsthatwemaypreferadistributed\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\ntwoattributesforeachvehicle—onerepresentingitscolorandonerepresenting\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "twoattributesweareinterestedinarecolorandcar-versus-truckratherthan\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\nsimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributes\ninsteadofjusttestingwhetheroneattributematches.\n5.9StochasticGradientDescent\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\n1 5 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ngradientdescentalgorithmintroducedinsection.4.3\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\nexpensive.\nThecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\nJ() = θ E x ,y ∼ˆ pdataL,y,(xθ) =1\nmm\ni=1L(x() i,y() i,θ)(5.96)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "mm\ni=1L(x() i,y() i,θ)(5.96)\nwhereistheper-exampleloss L L,y,py. (xθ) = log− (|xθ;)\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\n∇ θJ() =θ1\nmm\ni=1∇ θL(x() i,y() i,.θ) (5.97)\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\nlong.\nTheinsightofstochasticgradientdescentisthatthegradientisanexpectation.\nTheexpectationmaybeapproximately estimatedusingasmallsetofsamples.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "Speciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamples\nB={x(1),...,x( m)}drawnuniformlyfromthetrainingset.Theminibatchsize\nmistypicallychosentobearelativelysmallnumberofexamples,rangingfrom\n1toafewhundred.Crucially,misusuallyheldﬁxedasthetrainingsetsizem\ngrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputed\nononlyahundredexamples.\nTheestimateofthegradientisformedas\ng=1\nm∇ θm\ni=1L(x() i,y() i,.θ) (5.98)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "g=1\nm∇ θm\ni=1L(x() i,y() i,.θ) (5.98)\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\nthenfollowstheestimatedgradientdownhill:\nθθg ← −, (5.99)\nwhereisthelearningrate. \n1 5 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\nwasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\nlocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue\nofthecostfunctionquicklyenoughtobeuseful.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "ofthecostfunctionquicklyenoughtobeuseful.\nStochasticgradientdescenthasmanyimportantusesoutsidethecontextof\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\ndatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\nconvergenceusuallyincreaseswithtrainingsetsize. However,asmapproaches",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "inﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\nextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\namodelwithSGDisasafunctionof. O(1) m\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "algorithmsrequireconstructinganmm×matrixG i , j=k(x() i,x() j).Constructing\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\nwith billions of examples. In academia, starting in2006,deep learning was\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "industry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\ndatasets.\nStochasticgradientdescentandmanyenhancements toitaredescribedfurther\ninchapter.8\n5.10BuildingaMachineLearningAlgorithm\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\nafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,an\noptimization procedureandamodel.\nForexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n1 5 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nXyand,thecostfunction\nJ,b(w) = − E x ,y ∼ˆ pdatalogpmodel( )y|x, (5.100)\nthemodelspeciﬁcationpmodel(y|x) =N(y;xw+b,1),and,inmostcases,the\noptimization algorithmdeﬁnedbysolvingforwherethegradientofthecostiszero\nusingthenormalequations.\nByrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\nfromtheothers,wecanobtainaverywidevarietyofalgorithms.\nThecostfunctiontypicallyincludesatleastonetermthatcausesthelearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "processtoperformstatisticalestimation.Themostcommoncostfunctionisthe\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\nlikelihoodestimation.\nThecostfunctionmayalsoincludeadditionalterms,suchasregularization\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\ntoobtain\nJ,bλ (w) = ||||w2\n2− E x ,y ∼ˆ pdatalogpmodel( )y|x.(5.101)\nThisstillallowsclosed-formoptimization.\nIfwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "beoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\noptimization procedure,suchasgradientdescent.\nTherecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\nlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandproviding\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrst",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "PCAvectorbyspecifyingthatourlossfunctionis\nJ() = w E x ∼ˆ pdata||− ||xr(;)xw2\n2 (5.102)\nwhileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunction\nr() = xwxw.\nInsomecases,thecostfunctionmaybeafunctionthatwecannotactually\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\napproximatingitsgradients.\nMostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "immediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\n1 5 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\ntheircostfunctionshaveﬂatregionsthatmaketheminappropriate forminimization\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\ncanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofa\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "thanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.\n5.11ChallengesMotivatingDeepLearning\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\nThedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\ntraditionalalgorithmstogeneralizewellonsuchAItasks.\nThissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "exponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\nareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\novercometheseandotherobstacles.\n5.11.1TheCurseofDimensionality\nManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumber\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "dimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\nconﬁgurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\nincreases.\n1 5 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\nright),thenumberofconﬁgurationsofinterestmaygrowexponentially. ( L e f t )Inthis\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\nAstraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "eachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\ndimensionsitismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable. Weneed\ntokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplesto\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\naxis,weseemtoneedO(vd)regionsandexamples. Thisisaninstanceofthecurseof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "dimensionality.FiguregraciouslyprovidedbyNicolasChapados.\nThecurseofdimensionalityarisesinmanyplacesincomputerscience,and\nespeciallysoinmachinelearning.\nOnechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\nAsillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof 5.9\npossibleconﬁgurations ofxismuchlargerthanthenumberoftrainingexamples.\nTounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\ngrid,likeintheﬁgure.Wecandescribelow-dimensional spacewithalownumber",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "ofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.\nIfwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\nexamplesinthesamecell. Ifwearedoingregressionwecanaveragethetarget",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "valuesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\nconﬁgurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\n1 5 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nmeaningfulaboutthesenewconﬁgurations? Manytraditionalmachinelearning\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\nthesameastheoutputatthenearesttrainingpoint.\n5.11.2LocalConstancyandSmoothnessRegularization\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 273,
      "type": "default"
    }
  },
  {
    "content": "overparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\ndirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparameters function\nviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\ndegreeofbeliefinvariousfunctions.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 274,
      "type": "default"
    }
  },
  {
    "content": "degreeofbeliefinvariousfunctions.\nAmongthemostwidelyusedoftheseimplicit“priors” isthesmoothness\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\nshouldnotchangeverymuchwithinasmallregion.\nManysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\nadditional(explicit andimplicit)priorsinorder toreducethegeneralization",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 275,
      "type": "default"
    }
  },
  {
    "content": "erroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\ninsuﬃcientforthesetasks.\nTherearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbelief\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerent\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗that\nsatisﬁesthecondition\nf∗() x≈f∗(+)x (5.103)\nformostconﬁgurationsxandsmallchange.Inotherwords,ifweknowagood\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 276,
      "type": "default"
    }
  },
  {
    "content": "answerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\npossible.\nAnextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\n1 5 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 277,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\nthanthenumberoftrainingexamples.\nWhilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 278,
      "type": "default"
    }
  },
  {
    "content": "apartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\nresembleseachtrainingexamplex() i. Muchofthemodernmotivationfordeep\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n( ,). Bengioetal.2006b\nDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-based\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 279,
      "type": "default"
    }
  },
  {
    "content": "leavesanduseaseparateparameter(orsometimesmanyparametersforextensions\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\nrequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatistical\nconﬁdenceinthepredictedoutput.\nIngeneral,todistinguishO(k)regionsininputspace,allofthesemethods\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 280,
      "type": "default"
    }
  },
  {
    "content": "associatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\nwhereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustrated\ninﬁgure.5.10\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.\nFor example, imagine that thetargetfunctionis akind ofcheckerboard.A\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 281,
      "type": "default"
    }
  },
  {
    "content": "Imaginewhathappenswhenthenumberoftrainingexamplesissubstantially\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 282,
      "type": "default"
    }
  },
  {
    "content": "exampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\n1 5 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 283,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintoregions. Anexample(representedherebyacircle)withineachregiondeﬁnesthe\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\ndeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. The\nregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoi\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 284,
      "type": "default"
    }
  },
  {
    "content": "oftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighbor\nalgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\nsurroundingthatexample.\n1 5 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 285,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.\nThesmoothnessassumptionandtheassociatednon-parametric learningalgo-\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\nInhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 286,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerently\nindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\ngeneralizewell?\nTheanswertobothofthesequestions—whetheritispossibletorepresent\nacomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimated\nfunctiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthatavery",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 287,
      "type": "default"
    }
  },
  {
    "content": "largenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solong\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\ndiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\nOtherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 288,
      "type": "default"
    }
  },
  {
    "content": "sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\nstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralize\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\ncomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions.\nThecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 289,
      "type": "default"
    }
  },
  {
    "content": "thecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.\nManyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\ngorithms. Theseapparentlymildassumptionsallowanexponentialgaininthe\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 290,
      "type": "default"
    }
  },
  {
    "content": "ofdimensionality.\n1 6 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 291,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.11.3ManifoldLearning\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\nmanifold.\nAmanifoldisaconnected region. Mathematically , it isasetofpoints,\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\n3-Dspace.\nThedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistence",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 292,
      "type": "default"
    }
  },
  {
    "content": "oftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\ntoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecan\nwalknorth,south,east,orwest.\nAlthoughthereisaformalmathematical meaningtotheterm“manifold,”in\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\ndimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan5.11",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 293,
      "type": "default"
    }
  },
  {
    "content": "exampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\nofthemanifoldtovaryfromonepointtoanother. This oftenhappenswhena\nmanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingle\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\n0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .− 1 0 .− 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 294,
      "type": "default"
    }
  },
  {
    "content": "Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\ntheunderlyingmanifoldthatthelearnershouldinfer.\n1 6 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 295,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nManymachinelearningproblemsseemhopelessifweexpectthemachine\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\nManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\nof Rnconsistsofinvalidinputs, andthatinterestinginputsoccuronlyalong\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 296,
      "type": "default"
    }
  },
  {
    "content": "thatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\nhighlyconcentrated.\nTheassumptionthatthedataliesalongalow-dimensional manifoldmaynot\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 297,
      "type": "default"
    }
  },
  {
    "content": "thosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\noftwocategoriesofobservations.\nTheﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\nfromthesedomains. Figureshowshow,instead,uniformlysampledpoints 5.12",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 298,
      "type": "default"
    }
  },
  {
    "content": "looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\n1 6 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 299,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\nvolumeofimagespace.\nOfcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshow",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 300,
      "type": "default"
    }
  },
  {
    "content": "thatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\n1 6 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 301,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 302,
      "type": "default"
    }
  },
  {
    "content": "moveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\nconnectedtothemanifoldofimagesofcatfaces.\nThesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\ntuitivereasonssupportingit.Morerigorousexperiments (Cayton2005Narayanan,;\nandMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 303,
      "type": "default"
    }
  },
  {
    "content": "2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\ninterestinAI.\nWhenthedataliesonalow-dimensional manifold,itcanbemostnatural\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\nspeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notinterms",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 304,
      "type": "default"
    }
  },
  {
    "content": "ofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\nmethodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee 20.6\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\nThisconcludespart,whichhasprovidedthebasicconceptsinmathematics I",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 305,
      "type": "default"
    }
  },
  {
    "content": "andmachinelearningwhichareemployedthroughouttheremainingpartsofthe\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\n1 6 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 306,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\nfeat.\n1 6 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 307,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 3\nL i n e ar F act or Mo d e l s\nManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel\noftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto\npredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many\nofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|.\nTheselatentvariablesprovideanothermeansofrepresentingthedata.Distributed",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "representationsbased onlatent variablescanobtain alloftheadvantagesof\nrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrent\nnetworks.\nInthischapter,wedescribesomeofthesimplestprobabilisticmodelswith\nlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding\nblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,;\nRoweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "alsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat\nthemoreadvanceddeepmodelswillextendfurther.\nAlinearfactormodelisdeﬁnedbytheuseofastochastic,lineardecoder\nfunctionthatgeneratesbyaddingnoisetoalineartransformationof. x h\nThesemodelsareinterestingbecausetheyallowustodiscoverexplanatory\nfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder\nmadethesemodelssomeoftheﬁrstlatentvariablemodelstobeextensivelystudied.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Alinearfactormodeldescribesthedatagenerationprocessasfollows.First,\nwesampletheexplanatoryfactorsfromadistribution h\nh∼ p ,() h (13.1)\nwhere p( h)isafactorialdistribution,with p( h) =\ni p( h i),sothatitiseasyto\n489",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:\nx W h b = ++noise (13.2)\nwherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).\nThisisillustratedinﬁgure.13.1\nh 1 h 1 h 2 h 2 h 3 h 3\nx 1 x 1 x 2 x 2 x 3 x 3\nx h n  o i s  e x h n  o i s  e = W + + b = W + + b\nFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in\nwhichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "independentlatentfactors h,plussomenoise.Diﬀerentmodels,suchasprobabilistic\nPCA,factoranalysisorICA,makediﬀerentchoicesabouttheformofthenoiseandof\ntheprior. p() h\n13.1ProbabilisticPCAandFactorAnalysis\nProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear\nfactormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2\ndiﬀerinthechoicesmadeforthenoisedistributionandthemodel’spriorover\nlatentvariablesbeforeobserving. h x",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "latentvariablesbeforeobserving. h x\nIn f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994\npriorisjusttheunitvarianceGaussian\nh 0 ∼N(; h , I) (13.3)\nwhiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent,\ngiven h.Speciﬁcally, the noiseisassumed tobedrawnfroma diagonalco-\nvariance Gaussian distribution,with covariancematrix ψ=diag( σ2),with\nσ2= [ σ2\n1 , σ2\n2 , . . . , σ2\nn]avectorofper-variablevariances.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "2 , . . . , σ2\nn]avectorofper-variablevariances.\nTheroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween\nthediﬀerentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta\nmultivariatenormalrandomvariable,with\nx∼N(; x b W W ,+) ψ . (13.4)\n490",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nInordertocastPCAinaprobabilisticframework, wecanmakeaslight\nmodiﬁcationtothefactoranalysismodel,makingtheconditionalvariances σ2\ni\nequaltoeachother.Inthatcasethecovarianceof xisjust W W+ σ2I,where\nσ2isnowascalar.Thisyieldstheconditionaldistribution\nx∼N(; x b W W ,+ σ2I) (13.5)\norequivalently\nx h z = W ++ b σ (13.6)\nwhere z∼N( z; 0 , I)isGaussiannoise. ()thenshowan TippingandBishop1999\niterativeEMalgorithmforestimatingtheparameters and W σ2.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "This pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost\nvariationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall\nresidual r e c o nst r u c t i o n e r r o r σ2.Asshownby (), TippingandBishop1999\nprobabilisticPCAbecomesPCAas σ→0.Inthatcase,theconditionalexpected\nvalueof hgiven xbecomesanorthogonalprojectionof x b−ontothespace\nspannedbythecolumnsof,likeinPCA. d W\nAs σ→0,thedensitymodeldeﬁnedbyprobabilisticPCAbecomesverysharp",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "aroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe\nmodelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster\nnearahyperplane.\n13.2IndependentComponentAnalysis(ICA)\nIndependentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning\nalgorithms( ,; ,; ,; Herault andAns1984Jutten andHerault1991Comon1994\nHyvärinen1999Hyvärinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,).\nItisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "signalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform\ntheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan\nmerelydecorrelatedfromeachother.1\nManydiﬀerentspeciﬁcmethodologiesarereferredtoasICA.Thevariant\nthatismostsimilartotheothergenerativemodelswehavedescribedhereisa\nvariant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992\npriordistributionovertheunderlyingfactors, p( h),mustbeﬁxedaheadoftimeby",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "theuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma\n1Seesectionforadiscussionofthediﬀerencebetweenuncorrelatedvariablesandindepen- 3.8\ndentvariables.\n491",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nnonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning\nthemodelthenproceedsasusual,usingmaximumlikelihood.\nThemotivationforthisapproachisthatbychoosing p( h)tobeindependent,\nwecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.\nThisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto\nrecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each\ntrainingexampleisonemomentintime,each x iisonesensor’sobservationof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "themixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For\nexample,wemighthave npeoplespeakingsimultaneously.Ifwehave ndiﬀerent\nmicrophonesplacedindiﬀerentlocations,ICAcandetectthechangesinthevolume\nbetweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso\nthateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused\ninneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical\nsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject’s",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "headareusedtomeasuremanyelectricalsignalscomingfromthebody.The\nexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom\nthesubject’sheartandeyesarestrongenoughtoconfoundmeasurementstaken\natthesubject’sscalp.Thesignalsarriveattheelectrodesmixedtogether,so\nICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals\noriginatinginthebrain,andtoseparatesignalsindiﬀerentbrainregionsfrom\neachother.\nAsmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "inthegenerationof xratherthanusingadeterministicdecoder.Mostdonot\nusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof\nh= W− 1xindependentfromeachother.Manycriteriathataccomplishthisgoal\narepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe\nanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis\nproblematicoperationbyconstrainingtobeorthogonal. W\nAllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "isanindependentpriorwithGaussiancomponents,then Wisnotidentiﬁable.\nWecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery\ndiﬀerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,\nthatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe\nmodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe\nuserexplicitlyspeciﬁesthedistribution,atypicalchoiceistouse p( h i) =d\nd h iσ( h i).",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "d h iσ( h i).\nTypicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than\ndoestheGaussiandistribution,sowecanalsoseemostimplementations ofICA\naslearningsparsefeatures.\n492",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe\nphrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples\nfromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but\ndonothaveanywayofrepresenting p( h),andthusdonotimposeadistribution\nover p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof\nh= W− 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "accomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore\noftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating\ndataorestimatingitsdensity.\nJustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin\nchapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14\nweuseanonlinearfunction ftogeneratetheobserveddata.SeeHyvärinenand\nPajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "ensemblelearningby ()and (). RobertsandEverson2001Lappalainen e t a l .2000\nAnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent\nc o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014\nofinvertibletransformations(encoderstages)thathavethepropertythatthe\ndeterminantoftheJacobianofeachtransformationcanbecomputedeﬃciently.\nThismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "totransformthedataintoaspacewhereithasafactorizedmarginaldistribution,\nbutismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder\nisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto\ngeneratesamplesfromthemodel(byﬁrstsamplingfrom p( h)andthenapplying\nthedecoder).\nAnothergeneralization ofICAistolearngroupsoffeatures,withstatistical\ndependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyvärinenand",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "Hoyer1999Hyvärinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen\ntobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso\npossibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping\ngroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar\nfeatures.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns\nGaborﬁlters,suchthatneighboringfeatureshavesimilarorientation,locationor",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "frequency.ManydiﬀerentphaseoﬀsetsofsimilarGaborfunctionsoccurwithin\neachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.\n13.3SlowFeatureAnalysis\nSl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom\n493",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\ntimesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002\nSlowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness\nprinciple.Theideaisthattheimportantcharacteristicsofsceneschangevery\nslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa\nscene.Forexample,incomputervision,individualpixelvaluescanchangevery\nrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "willrapidlychangefromblacktowhiteandbackagainasthezebra’sstripespass\noverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe\nimagewillnotchangeatall,andthefeaturedescribingthezebra’spositionwill\nchangeslowly. Wethereforemaywishtoregularizeourmodeltolearnfeatures\nthatchangeslowlyovertime.\nTheslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied\ntoawidevarietyofmodels(,;,; ,; Hinton1989Földiák1989Mobahi e t a l .2009",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany\ndiﬀerentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe\nintroducedbyaddingatermtothecostfunctionoftheform\nλ\ntL f(( x( + 1 ) t)( , f x( ) t)) (13.7)\nwhere λisahyperparameter determiningthestrengthoftheslownessregularization\nterm, tistheindexintoatimesequenceofexamples, fisthefeatureextractor\ntoberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "and f( x( + 1 ) t).Acommonchoiceforisthemeansquareddiﬀerence. L\nSlowfeatureanalysisisaparticularlyeﬃcientapplicationoftheslowness\nprinciple.Itiseﬃcientbecauseitisappliedtoalinearfeatureextractor,andcan\nthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea\ngenerativemodelperse,inthesensethatitdeﬁnesalinearmapbetweeninput\nspaceandfeaturespacebutdoesnotdeﬁneaprioroverfeaturespaceandthus\ndoesnotimposeadistributiononinputspace. p() x",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "doesnotimposeadistributiononinputspace. p() x\nTheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeﬁning f( x; θ)\ntobealineartransformation,andsolvingtheoptimization problem\nmin\nθE t(( f x( + 1 ) t) i− f( x( ) t) i)2(13.8)\nsubjecttotheconstraints\nE t f( x( ) t) i= 0 (13.9)\nand\nE t[( f x( ) t)2\ni] = 1 . (13.10)\n494",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe\nproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature\nvaluesandobtainadiﬀerentsolutionwithequalvalueoftheslownessobjective.\nTheconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe\npathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0\nareordered,withtheﬁrstfeaturebeingtheslowest.Tolearnmultiplefeatures,we\nmustalsoaddtheconstraint",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "mustalsoaddtheconstraint\n∀ i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 . (13.11)\nThisspeciﬁesthatthelearnedfeaturesmustbelinearlydecorrelated fromeach\nother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe\noneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing\nreconstructionerror, to forcethe featurestodiversify, but thisdecorrelation\nmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA\nproblemmaybesolvedinclosedformbyalinearalgebrapackage.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis\nexpansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe\nquadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear\nSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors\nbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis\nexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron\ntopofthatexpansion.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "topofthatexpansion.\nWhentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith\nquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith\nthoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained\nonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep\nSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented\nbyneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA\nthusseemstobeareasonablybiologicallyplausiblemodel.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich\nfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical\npredictions,onemustknowaboutthedynamicsoftheenvironmentintermsof\nconﬁguration space (e.g., inthe caseofrandom motion inthe 3-Drendered\nenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability\ndistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow\ntheunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "optimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA\nappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.\n495",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nThisisincomparisontootherlearningalgorithmswherethecostfunctiondepends\nhighlyonspeciﬁcpixelvalues,makingitmuchmorediﬃculttodeterminewhat\nfeaturesthemodelwilllearn.\nDeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose\nestimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome\nthebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited\nitsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "that,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,\nitwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom\nonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof\nwhethertheobject’svelocityishighorlow,buttheslownessprincipleencourages\nthemodeltoignorethepositionofobjectsthathavehighvelocity.\n13.4SparseCoding\nSpar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996\nbeenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "mechanism. Strictlyspeaking,theterm“sparsecoding”referstotheprocessof\ninferringthevalueof hinthismodel,while“sparsemodeling”referstotheprocess\nofdesigningandlearningthemodel,buttheterm“sparsecoding”isoftenusedto\nrefertoboth.\nLikemostotherlinearfactormodels,itusesalineardecoderplusnoiseto\nobtainreconstructionsof x,asspeciﬁedinequation.Morespeciﬁcally,sparse 13.2\ncodingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith\nisotropicprecision: β\np , ( ) = (; + x h| N x W h b1\nβI) . (13.12)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "p , ( ) = (; + x h| N x W h b1\nβI) . (13.12)\nThedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen\nandField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized\nStudent- tdistributions.Forexample,theLaplacepriorparametrized intermsof\nthesparsitypenaltycoeﬃcientisgivenby λ\np h( i) = Laplace( h i;0 ,2\nλ) =λ\n4e−1\n2λ h| i|(13.13)\nandtheStudent-priorby t\np h( i) ∝1\n(1+h2\ni\nν)ν +1\n2. (13.14)\n496",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the\ntrainingalternatesbetweenencodingthedataandtrainingthedecodertobetter\nreconstructthedatagiventheencoding.Thisapproachwillbejustiﬁedfurtheras\naprincipledapproximation tomaximumlikelihoodlater,insection.19.3\nFormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction\nthatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "thatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder\nisanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek\nthesinglemostlikelycodevalue:\nh∗= () = argmax f x\nhp . ( ) h x| (13.15)\nWhencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12\noptimization problem:\nargmax\nhp( ) h x| (13.16)\n=argmax\nhlog( ) p h x| (13.17)\n=argmin\nhλ|||| h 1+ β||− || x W h2\n2 , (13.18)\nwherewehavedroppedtermsnotdependingon handdividedbypositivescaling",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "factorstosimplifytheequation.\nDuetotheimpositionofan L1normon h,thisprocedurewillyieldasparse\nh∗(Seesection).7.1.2\nTotrainthemodelratherthanjustperforminference,wealternatebetween\nminimization withrespectto handminimization withrespectto W.Inthis\npresentation,wetreat βasahyperparameter.Typicallyitissetto1becauseits\nroleinthisoptimization problemissharedwith λandthereisnoneedforboth\nhyperparameters.Inprinciple,wecouldalsotreat βasaparameterofthemodel",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "andlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend\non hbutdodependon β.Tolearn β,thesetermsmustbeincluded,or βwill\ncollapseto.0\nNotallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|).\nOftenwearejustinterestedinlearningadictionaryoffeatureswithactivation\nvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.\nIfwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor\nanelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "sparse,onlythefeatureextractoris. ()describeapproximate Goodfellow e t a l .2013d\n497",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\ninferenceinadiﬀerentmodelfamily,thespikeandslabsparsecodingmodel,for\nwhichsamplesfromthepriorusuallycontaintruezeros.\nThesparsecodingapproachcombinedwiththeuseofthenon-parametric\nencodercaninprincipleminimizethecombinationofreconstructionerrorand\nlog-priorbetterthananyspeciﬁcparametricencoder.Anotheradvantageisthat\nthereisnogeneralization errortotheencoder.Aparametricencodermustlearn\nhowtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "thetrainingdata,alearned,parametricencodermayfailtoﬁndan hthatresults\ninaccuratereconstructionorasparsecode.Forthevastmajorityofformulations\nofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization\nprocedurewillalwaysﬁndtheoptimalcode(unlessdegeneratecasessuchas\nreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts\ncanstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe\ndecoderweights,ratherthangeneralization errorintheencoder.Thelackof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "generalization errorinsparsecoding’soptimization-based encodingprocessmay\nresultinbettergeneralization whensparsecodingisusedasafeatureextractorfor\naclassiﬁerthanwhenaparametricfunctionisusedtopredictthecode.Coates\nandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor\nobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric\nencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l .",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d\nextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer\nlabelsperclass).\nTheprimarydisadvantageofthenon-parametric encoderisthatitrequires\ngreatertimetocompute hgiven xbecausethenon-parametric approachrequires\nrunninganiterativealgorithm.Theparametricautoencoderapproach,developed\nin chapter ,usesonly a ﬁxed n umber of layers, often only one.Another 14",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe\nnon-parametric encoder,whichmakesitdiﬃculttopretrainasparsecodingmodel\nwithanunsupervisedcriterionandthenﬁne-tuneitusingasupervisedcriterion.\nModiﬁedversionsofsparsecodingthatpermitapproximate derivativesdoexist\nbutarenotwidelyused( ,). BagnellandBradley2009\nSparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as\nshowninﬁgure.Thishappensevenwhenthemodelisabletoreconstruct 13.2",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thedatawellandprovideusefulfeaturesforaclassiﬁer.Thereasonisthateach\nindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode\nresultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated\nsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-\n498",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nFigure13.2: Example samplesandweightsfromaspikeandslabsparsecodingmodel\ntrainedontheMNISTdataset. ( L e f t )Thesamplesfromthemodeldonotresemblethe\ntrainingexamples.Atﬁrstglance,onemightassumethemodelispoorlyﬁt.The ( R i g h t )\nweightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete\ndigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior\noverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "areappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof\ngenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure\nreproducedwithpermissionfromGoodfellow2013d e t a l .().\nfactorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore\nsophisticatedshallowmodels.\n13.5ManifoldInterpretationofPCA\nLinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas\nlearningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "deﬁningathinpancake-shapedregionofhighprobability—aGaussiandistribution\nthatisverynarrowalongsomeaxes,justasapancakeisveryﬂatalongitsvertical\naxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal\naxes. Thisisillustratedinﬁgure. PCAcanbeinterpretedasaligningthis 13.3\npancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation\nappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns\nmatrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "xaspossible,\nLettheencoderbe\nh x W = ( f) = ( ) x µ− . (13.19)\n499",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTheencodercomputesalow-dimensional representationof h.Withtheautoencoder\nview,wehaveadecodercomputingthereconstruction\nˆ x h b V h = ( g) = + . (13.20)\nFigure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional\nmanifold.Theﬁgureshowstheupperhalfofthe“pancake”abovethe“manifoldplane”\nwhichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis\nverysmall(arrowpointingoutofplane)andcanbeconsideredlike“noise,”whiletheother",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "variancesarelarge(arrowsintheplane)andcorrespondto“signal,”andacoordinate\nsystemforthereduced-dimensiondata.\nThechoicesoflinearencoderanddecoderthatminimizereconstructionerror\nE[||− xˆ x||2] (13.21)\ncorrespondto V= W, µ= b= E[ x]andthecolumnsof Wformanorthonormal\nbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance\nmatrix\nC x µ x µ = [( E −)(−)] . (13.22)\nInthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "magnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).\nOnecanalsoshowthateigenvalue λ iof Ccorrespondstothevarianceof x\ninthedirectionofeigenvector v( ) i.If x∈ RDand h∈ Rdwith d < D,thenthe\n500",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\noptimalreconstructionerror(choosing,,andasabove)is µ b V W\nmin[ E||− xˆ x||2] =D \ni d = + 1λ i . (13.23)\nHence,ifthecovariancehasrank d,theeigenvalues λ d + 1to λ Dare0andrecon-\nstructionerroris0.\nFurthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby\nmaximizingthevariancesoftheelementsof h,underorthogonal W,insteadof\nminimizingreconstructionerror.\nLinearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "simplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiﬁersand\nlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear\nfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic\nmodelsthatperformthesametasksbutwithamuchmorepowerfulandﬂexible\nmodelfamily.\n501",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "Deep L ea r ni n g\nI a n G o o d f e l l o w\nY o s h u a B e n g i o\nA a r o n C o u r v i l l e",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "C on t e n t s\nWebsite vii\nAcknowledgments viii\nNotation xi\n1Introduction 1\n1.1WhoShouldReadThisBook?. . . . . .. . . . . . . . . . . . . .8\n1.2HistoricalTrendsinDeepLearning. . . . . . . . . . . . . . . . . 11\nIAppliedMathandMachineLearningBasics 29\n2LinearAlgebra 31\n2.1Scalars,Vectors,MatricesandTensors. . . . . . . . . . . . . . .31\n2.2MultiplyingMatricesandVectors. . . . . . . . . . . . . . . . . .34\n2.3IdentityandInverseMatrices. . . . . . . . . . . . . . . . . . . .36",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "2.4LinearDependenceandSpan. . . . . . . . . . . . . . . . . . . .37\n2.5Norms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.6SpecialKindsofMatricesandVectors. . . . . . . . . . . . . . . 40\n2.7Eigendecomposition. . . . . . . . . . . . . . . . . . . . . . . . . . 42\n2.8SingularValueDecomposition. . . . . . . . . . . . . . . . . . . .44\n2.9TheMoore-PenrosePseudoinverse. . . . . . . . . . . . . . . . . .45",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "2.10TheTraceOperator. . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.11TheDeterminant. .. . . . . . . . . . . . . . . . . . . . . . . . . 47\n2.12Example:PrincipalComponentsAnalysis. . . . . . . . . . . . .48\n3ProbabilityandInformationTheory 53\n3.1WhyProbability?. . . . .. . . . . . . . . . . . . . . . . . . . . .54\ni",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n3.2RandomVariables. . . . .. . . . . . . . . . . . . . . . . . . . .56\n3.3ProbabilityDistributions. . . . . . . . . . . . . . . . . . . . . . .56\n3.4MarginalProbability. . . . . . . . . . . . . . . . . . . . . . . . . 58\n3.5ConditionalProbability. .. . . . . . . . . . . . . . . . . . . . .59\n3.6TheChainRuleofConditionalProbabilities. . . . . . . . . . . .59\n3.7IndependenceandConditionalIndependence. . . . . . . . . . . .60",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "3.8Expectation,VarianceandCovariance. . . . . . . . . . . . . . .60\n3.9CommonProbabilityDistributions. . . . . . . . . . . . . . . . .62\n3.10UsefulPropertiesofCommonFunctions. . .. . . . . . . . . . .67\n3.11Bayes’Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\n3.12TechnicalDetailsofContinuousVariables. . . . . .. . . . . . . 71\n3.13InformationTheory. . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.14StructuredProbabilisticModels. . . .. . . . . . . . . . . . . . . 75",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "4NumericalComputation 80\n4.1OverﬂowandUnderﬂow. . . . . . . . . . . . . . . . . . . . . . .80\n4.2PoorConditioning . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.3Gradient-BasedOptimization . . . . . . .. . . . . . . . . . . . .82\n4.4ConstrainedOptimization . . . . . . . . . . . . . . . . . . . . . .93\n4.5Example:LinearLeastSquares. . . . . . .. . . . . . . . . . . .96\n5MachineLearningBasics 98\n5.1LearningAlgorithms. . . . . . . . . . . . . . . . . . . . . . . . .99",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "5.2Capacity,OverﬁttingandUnderﬁtting. .. . . . . . . . . . . . .110\n5.3HyperparametersandValidationSets.. . . . . . . . . . . . . . .120\n5.4Estimators,BiasandVariance. . . . . .. . . . . . . . . . . . . .122\n5.5MaximumLikelihoodEstimation. . . . . .. . . . . . . . . . . .131\n5.6BayesianStatistics. . . . . . . . . . . . . . . . . . . . . . . . . .135\n5.7SupervisedLearningAlgorithms. . .. . . . . . . . . . . . . . . . 140\n5.8UnsupervisedLearningAlgorithms. . . . . . . . . . . . . . . . .146",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "5.9StochasticGradientDescent. . . .. . . . . . . . . . . . . . . . . 151\n5.10BuildingaMachineLearningAlgorithm. . . . . . . . . . . . . .153\n5.11ChallengesMotivatingDeepLearning. . . . .. . . . . . . . . . .155\nIIDeepNetworks:ModernPractices 166\n6DeepFeedforwardNetworks 168\n6.1Example:LearningXOR.. . . . . . . . . . . . . . . . . . . . . .171\n6.2Gradient-BasedLearning.. . . . . . . . . . . . . . . . . . . . . .177\ni i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n6.3HiddenUnits. . . . . .. . . . . . . . . . . . . . . . . . . . . . .191\n6.4ArchitectureDesign. . . . . . . . . . . . . . . . . . . . . . . . . .197\n6.5Back-PropagationandOtherDiﬀerentiationAlgorithms. . . . .204\n6.6HistoricalNotes. . . . . . .. . . . . . . . . . . . . . . . . . . . .224\n7RegularizationforDeepLearning 228\n7.1ParameterNormPenalties. . . . .. . . . . . . . . . . . . . . . . 230\n7.2NormPenaltiesasConstrainedOptimization . . . . . . . . . . . .237",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "7.3RegularizationandUnder-ConstrainedProblems. .. . . . . . .239\n7.4DatasetAugmentation. . . . . . . . . . . . . . . . . . . . . . . .240\n7.5NoiseRobustness. . . . . . . . . . . . . . . . . . . . . . . . . . .242\n7.6Semi-SupervisedLearning. . . . . . . . . . . . . . . . . . . . . .243\n7.7Multi-TaskLearning. . . . . . . . . . . . . . . . . . . . . . . . .244\n7.8EarlyStopping. . . . . . . . . . . . . . . . . . . . . . . . . . . .246",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "7.9ParameterTyingandParameterSharing . . . . . . . . . . . . . . 253\n7.10SparseRepresentations. . . . . . . . . . . . . . . . . . . . . . . .254\n7.11BaggingandOtherEnsembleMethods.. . . . . . . . . . . . . .256\n7.12Dropout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .258\n7.13AdversarialTraining. . . . . . . . . . . . . . . . . . . . . . . . . 268\n7.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer270\n8OptimizationforTrainingDeepModels 274",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "8OptimizationforTrainingDeepModels 274\n8.1HowLearningDiﬀersfromPureOptimization . . . . . . . . . . . 275\n8.2ChallengesinNeuralNetworkOptimization . . . . .. . . . . . .282\n8.3BasicAlgorithms. . . . . . . . . . . . . . . . . . . . . . . . . . .294\n8.4ParameterInitialization Strategies.. . . . . . . . . . . . . . . .301\n8.5AlgorithmswithAdaptiveLearningRates. . . . . . .. . . . . .306\n8.6ApproximateSecond-Order Methods. . . .. . . . . . . . . . . .310",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "8.7Optimization StrategiesandMeta-Algorithms. . . . .. . . . . .317\n9ConvolutionalNetworks 330\n9.1TheConvolutionOperation. . . . . . . . . . . . . . . . . . . . .331\n9.2Motivation. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .335\n9.3Pooling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .339\n9.4ConvolutionandPoolingasanInﬁnitelyStrongPrior. .. . . . .345\n9.5VariantsoftheBasicConvolutionFunction. . . . . . . . . . . . 347",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "9.6StructuredOutputs.. . . . . . . . . . . . . . . . . . . . . . . . . 358\n9.7DataTypes. . . . . .. . . . . . . . . . . . . . . . . . . . . . . . 360\n9.8EﬃcientConvolutionAlgorithms. . . . . . . . . . . . . . . . . .362\n9.9RandomorUnsupervisedFeatures. . . . . . . . . . . . . . . . .363\ni i i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks. . . . . ..364\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning. . . .371\n10 SequenceModeling:RecurrentandRecursiveNets373\n10.1UnfoldingComputational Graphs. . . . . . . . . . . . . . . . . .375\n10.2RecurrentNeuralNetworks. . .. . . . . . . . . . . . . . . . . .378\n10.3BidirectionalRNNs . . . . . . . . . . . . . . . . . . . . . . . . . .394\n10.4Encoder-DecoderSequence-to-SequenceArchitectures. . . . . ..396",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "10.5DeepRecurrentNetworks. . . . . . . . . . . . . . . . . . . . . .398\n10.6RecursiveNeuralNetworks. . . . .. . . . . . . . . . . . . . . . . 400\n10.7TheChallengeofLong-TermDependencies. . . . . . . . . . . . .401\n10.8EchoStateNetworks. . . . . . . . . . . . . . . . . . . . . . . . .404\n10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales. . ..406\n10.10 TheLongShort-TermMemoryandOtherGatedRNNs. .. . . .408\n10.11 Optimization forLong-TermDependencies. . . . . . . . . . . . .413",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "10.12 Explicit Memory. . . . . . . . . . . . . . . . . . . . . . . . . . . 416\n11 PracticalMethodology 421\n11.1PerformanceMetrics. . . . . . . . . . . . . . . . . . . . . . . . .422\n11.2DefaultBaselineModels. . . . . . . . . . . . . . . . . . . . . . .425\n11.3DeterminingWhethertoGatherMoreData. . . . . . . . . . . . 426\n11.4SelectingHyperparameters. . . . . . . . . . . . . . . . . . . . . .427\n11.5DebuggingStrategies. . . . .. . . . . . . . . . . . . . . . . . . .436",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "11.6Example:Multi-DigitNumberRecognition. . . . .. . . . . . . . 440\n12 Applications 443\n12.1Large-ScaleDeepLearning.. . . . . . . . . . . . . . . . . . . . .443\n12.2ComputerVision. . . . . . . . . . . . . . . . . . . . . . . . . . .452\n12.3SpeechRecognition . . . . . .. . . . . . . . . . . . . . . . . . . .458\n12.4NaturalLanguageProcessing. . .. . . . . . . . . . . . . . . . .461\n12.5OtherApplications. . . . . . . . . . . . . . . . . . . . . . . . . .478\nIIIDeepLearningResearch 486",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "IIIDeepLearningResearch 486\n13 LinearFactorModels 489\n13.1ProbabilisticPCAandFactorAnalysis. . . . . . .. . . . . . . . 490\n13.2IndependentComponentAnalysis(ICA). . . . . . . . . . . . . .491\n13.3SlowFeatureAnalysis. . . . . .. . . . . . . . . . . . . . . . . .493\n13.4SparseCoding. . . . . .. . . . . . . . . . . . . . . . . . . . . . .496\ni v",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n13.5ManifoldInterpretation ofPCA. . . . . . . . . . . . . . . . . . .499\n14 Autoencoders 502\n14.1Undercomplete Autoencoders. . . . . . . . . . . . . . . . . . . .503\n14.2RegularizedAutoencoders. . . . . . . . . . . . . . . . . . . . . .504\n14.3RepresentationalPower,LayerSizeandDepth. . . . . .. . . . .508\n14.4StochasticEncodersandDecoders. . . . . . . . . . . . . . . . . .509\n14.5DenoisingAutoencoders. .. . . . . . . . . . . . . . . . . . . . .510",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "14.6LearningManifoldswithAutoencoders. . . . . .. . . . . . . . . 515\n14.7ContractiveAutoencoders.. . . . . . . . . . . . . . . . . . . . .521\n14.8PredictiveSparseDecomposition. . . . . . . . . . . . . . . . . .523\n14.9ApplicationsofAutoencoders. . . . .. . . . . . . . . . . . . . .524\n15 RepresentationLearning 526\n15.1GreedyLayer-WiseUnsupervisedPretraining. . . . . .. . . . .528\n15.2TransferLearningandDomainAdaptation. . . .. . . . . . . . .536",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "15.3Semi-SupervisedDisentanglingofCausalFactors. . . . .. . . .541\n15.4DistributedRepresentation. . . . . . . . . . . . . . . . . . . . . .546\n15.5ExponentialGainsfromDepth. . . . . . . . . . . . . . . . . . .553\n15.6ProvidingCluestoDiscoverUnderlyingCauses. . . .. . . . . .554\n16 StructuredProbabilisticModelsforDeepLearning558\n16.1TheChallengeofUnstructuredModeling.. . . . . . . . . . . . .559\n16.2UsingGraphstoDescribeModelStructure. .. . . . . . . . . . .563",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "16.3SamplingfromGraphicalModels. . .. . . . . . . . . . . . . . .580\n16.4AdvantagesofStructuredModeling .. . . . . . . . . . . . . . . .582\n16.5LearningaboutDependencies. . . .. . . . . . . . . . . . . . . . 582\n16.6InferenceandApproximateInference. . . . . . . . . . . . . . . .584\n16.7TheDeepLearningApproachtoStructuredProbabilisticModels585\n17 MonteCarloMethods 590\n17.1SamplingandMonteCarloMethods. . . . . . . . . . . . . . . . 590",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "17.2ImportanceSampling. . . . . . . . . . . . . . . . . . . . . . . . .592\n17.3MarkovChainMonteCarloMethods. . . . .. . . . . . . . . . .595\n17.4GibbsSampling . . . . . . .. . . . . . . . . . . . . . . . . . . . .599\n17.5TheChallengeofMixingbetweenSeparatedModes. . . . . . ..599\n18 ConfrontingthePartitionFunction 605\n18.1TheLog-LikelihoodGradient.. . . . . . . . . . . . . . . . . . .606\n18.2StochasticMaximumLikelihoodandContrastiveDivergence. . .607\nv",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n18.3Pseudolikelihood. . . . . . . . . . . . . . . . . . . . . . . . . . .615\n18.4ScoreMatchingandRatioMatching. . . . . . . . . . . . . . . . 617\n18.5DenoisingScoreMatching. . . . . . . . . . . . . . . . . . . . . .619\n18.6Noise-ContrastiveEstimation. . . . .. . . . . . . . . . . . . . .620\n18.7EstimatingthePartitionFunction. . . . . . . . . . . . . . . . . .623\n19 ApproximateInference 631\n19.1InferenceasOptimization .. . . . . . . . . . . . . . . . . . . . .633",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "19.2ExpectationMaximization . .. . . . . . . . . . . . . . . . . . . .634\n19.3MAPInferenceandSparseCoding.. . . . . . . . . . . . . . . .635\n19.4VariationalInferenceandLearning. . . . . . . . . . . . . . . . .638\n19.5LearnedApproximateInference. . .. . . . . . . . . . . . . . . .651\n20 DeepGenerativeModels 654\n20.1BoltzmannMachines. . . . . . . . . . . . . . . . . . . . . . . . .654\n20.2RestrictedBoltzmannMachines. . . . . . .. . . . . . . . . . . .656",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "20.3DeepBeliefNetworks.. . . . . . . . . . . . . . . . . . . . . . . . 660\n20.4DeepBoltzmannMachines. . . . . . . . . . . . . . . . . . . . . .663\n20.5BoltzmannMachinesforReal-ValuedData. . . . . . . . . . . . .676\n20.6ConvolutionalBoltzmannMachines . . . . . . . . . . . . . . . . .683\n20.7BoltzmannMachinesforStructuredorSequentialOutputs. . . .685\n20.8OtherBoltzmannMachines. . . . .. . . . . . . . . . . . . . . . 686\n20.9Back-PropagationthroughRandomOperations. . . . . .. . . .687",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "20.10 DirectedGenerativeNets. . . . . . . . . . . . . . . . . . . . . . .692\n20.11 DrawingSamplesfromAutoencoders. . . . .. . . . . . . . . . .711\n20.12 Generativ eStochasticNetworks. . .. . . . . . . . . . . . . . . . 714\n20.13 OtherGenerationSchemes. . . . . . . . . . . . . . . . . . . . . . 716\n20.14 EvaluatingGenerativeModels . . . . . . . . . . . . . . . . . . . .717\n20.15 Conclus ion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .720\nBibliography 721\nIndex 777\nv i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "W e b s i t e\nwww.deeplearningb ook.org\nThisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa\nvarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\nmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\nvii",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 2\nL i n e ar A l ge b ra\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.\nAgoodunderstandingoflinearalgebraisessentialforunderstandingandworking\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\nthekeylinearalgebraprerequisites.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "thekeylinearalgebraprerequisites.\nIfyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Shilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\ntopicsthatarenotessentialforunderstandingdeeplearning.\n2.1Scalars,Vectors,MatricesandTensors\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\n•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\nWewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.\nWhenweintroducethem,wespecifywhatkindofnumbertheyare.For\n31",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nexample,wemightsay“Let s∈ Rbetheslopeoftheline,”whiledeﬁninga\nreal-valuedscalar,or“Let n∈ Nbethenumberofunits,”whiledeﬁninga\nnaturalnumberscalar.\n•Vectors: Avectorisanarrayofnumbers.Thenumbersarearrangedin\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\nTypicallywegivevectorslowercasenameswritteninboldtypeface,such\nasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalic\ntypeface,withasubscript.Theﬁrstelementofxis x 1,thesecondelement",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "is x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\nwewritethemasacolumnenclosedinsquarebrackets:\nx=\nx 1\nx 2\n...\nx n\n. (2.1)\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\ngivingthecoordinatealongadiﬀerentaxis.\nSometimesweneedtoindexasetofelementsofavector.Inthiscase,we",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "deﬁneasetcontainingtheindicesandwritethesetasasubscript.For\nexample,toaccess x 1, x 3and x 6,wedeﬁnetheset S={1 ,3 ,6}andwrite\nx S.Weusethe−signtoindexthecomplementofaset.Forexamplex − 1is\nthevectorcontainingallelementsofxexceptfor x 1,andx − Sisthevector\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6.\n•Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁed\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "aheightof mandawidthof n,thenwesaythatA∈ Rm n ×. Weusually\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\nofthenumberswithverticalcoordinate ibywritinga“”forthehorizontal :\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\n3 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nA =\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\nA 3 1 , A 3 2 ,\n ⇒ A=A 1 1 , A 2 1 , A 3 1 ,\nA 1 2 , A 2 2 , A 3 2 ,\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\nmaindiagonal.\nthe-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\n. (2.2)\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "asingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\nofthematrixcomputedbyapplyingthefunctionto. fA\n•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.\nInthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\nvariablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\nbywriting A i , j , k.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "bywriting A i , j , k.\nOneimportantoperationonmatricesisthetranspose. Thetransposeofa\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\nﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\nmatrixasAA,anditisdeﬁnedsuchthat\n(A) i , j= A j , i . (2.3)\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\n3 3",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ndeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\nx= [ x 1 , x 2 , x 3].\nAscalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\ncanseethatascalarisitsowntranspose: a a= .\nWecanaddmatricestoeachother,aslongastheyhavethesameshape,just\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .\nWecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "byperformingthatoperationoneachelementofamatrix:D= a·B+ cwhere\nD i , j= a B· i , j+ c.\nInthecontextofdeeplearning,wealsousesomelessconventionalnotation.\nWeallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\nmatrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedinto\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\niscalled .broadcasting\n2.2MultiplyingMatricesandVectors",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "2.2MultiplyingMatricesandVectors\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\norderforthisproducttobedeﬁned,Amusthavethesamenumberofcolumnsas\nBhasrows.IfAisofshape m n×andBisofshape n p×,thenCisofshape\nm p×.Wecanwritethematrixproductjustbyplacingtwoormorematrices\ntogether,e.g.\nCAB= . (2.4)\nTheproductoperationisdeﬁnedby\nC i , j=\nkA i , k B k, j . (2.5)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "C i , j=\nkA i , k B k, j . (2.5)\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\nelement-wiseproductHadamardproduct or ,andisdenotedas.AB\nThedotproductbetweentwovectorsxandyofthesamedimensionality\nisthematrixproductxy.WecanthinkofthematrixproductC=ABas\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\n3 4",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\nanalysis ofmatrices moreconvenient.For example, matrix m ultiplication is\ndistributive:\nABCABAC (+) = + . (2.6)\nItisalsoassociative:\nABCABC ( ) = ( ) . (2.7)\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\nvectorsiscommutative:\nxyy= x . (2.8)\nThetransposeofamatrixproducthasasimpleform:\n( )AB= BA. (2.9)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "( )AB= BA. (2.9)\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\nxy=\nxy\n= yx . (2.10)\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\nthereadershouldbeawarethatmanymoreexist.\nWenowknowenoughlinearalgebranotationtowritedownasystemoflinear\nequations:\nAxb= (2.11)\nwhereA∈ Rm n ×isaknownmatrix,b∈ Rmisaknownvector,andx∈ Rnisa",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "vectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\nconstraint.Wecanrewriteequationas:2.11\nA 1 : ,x= b 1 (2.12)\nA 2 : ,x= b 2 (2.13)\n. . . (2.14)\nA m , :x= b m (2.15)\nor,evenmoreexplicitly,as:\nA 1 1 , x 1+A 1 2 , x 2+ +···A 1 , n x n= b 1 (2.16)\n3 5",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n\n100\n010\n001\n\nFigure2.2:Exampleidentitymatrix:ThisisI 3.\nA 2 1 , x 1+A 2 2 , x 2+ +···A 2 , n x n= b 2 (2.17)\n. . . (2.18)\nA m , 1 x 1+A m , 2 x 2+ +···A m , n x n= b m . (2.19)\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\nequationsofthisform.\n2.3IdentityandInverseMatrices\nLinearalgebraoﬀersapowerfultoolcalledmatrixinversionthatallowsusto\nanalyticallysolveequationformanyvaluesof. 2.11 A",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "analyticallysolveequationformanyvaluesof. 2.11 A\nTodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentity\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\nn-dimensionalvectorsasI n.Formally,I n∈ Rn n ×,and\n∀∈x Rn,I nxx= . (2.20)\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\ndiagonalare1,whilealloftheotherentriesarezero.Seeﬁgureforanexample.2.2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "ThematrixinverseofAisdenotedasA− 1,anditisdeﬁnedasthematrix\nsuchthat\nA− 1AI= n . (2.21)\nWecannowsolveequationbythefollowingsteps: 2.11\nAxb= (2.22)\nA− 1AxA= − 1b (2.23)\nI nxA= − 1b (2.24)\n3 6",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nxA= − 1b . (2.25)\nOfcourse,thisprocessdependsonitbeingpossibletoﬁndA− 1.Wediscuss\ntheconditionsfortheexistenceofA− 1inthefollowingsection.\nWhenA− 1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform.\nIntheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\ntimesfordiﬀerentvaluesofb.However,A− 1isprimarilyusefulasatheoretical\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "BecauseA− 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\nestimatesof.x\n2.4LinearDependenceandSpan\nInorderforA− 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\nsolutionsorinﬁnitelymanysolutionsforsomevaluesofb. Itisnotpossibleto\nhavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;if\nbothandaresolutionsthen xy\nzxy = α+(1 )− α (2.26)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "bothandaresolutionsthen xy\nzxy = α+(1 )− α (2.26)\nisalsoasolutionforanyreal. α\nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\nofAasspecifyingdiﬀerentdirectionswecantravelfromtheorigin(thepoint\nspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareof\nreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelin\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\ncolumn: i\nAx=\nix iA : , i . (2.27)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "column: i\nAx=\nix iA : , i . (2.27)\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\neachvectorv( ) ibyacorrespondingscalarcoeﬃcientandaddingtheresults:\n\nic iv( ) i. (2.28)\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\noftheoriginalvectors.\n3 7",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\nspacerangeortheof.A\nInorderforthesystemAx=btohaveasolutionforallvaluesofb∈ Rm,\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\nimmediately thatAmusthaveatleast mcolumns,i.e., n m≥. Otherwise, the",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "dimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\n3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\nifandonlyifliesonthatplane.b\nHaving n m≥isonlyanecessaryconditionforeverypointtohaveasolution.\nItisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnsto\nberedundant.Considera2×2matrixwherebothofthecolumnsareidentical.\nThishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "replicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\nencompassallof R2,eventhoughtherearetwocolumns.\nFormally,thiskindofredundancyisknownaslineardependence.Asetof\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\noftheothervectors. Ifweaddavectortoasetthatisalinearcombinationof\ntheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’s\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "thematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\nconditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor 2.11\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\nmcolumnsmayhavemorethanonesuchset.\nInorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "equationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\nwayofparametrizing eachsolution.\nTogether,thismeansthatthematrixmustbesquare,thatis,werequirethat\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\nwithlinearlydependentcolumnsisknownas.singular\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\nequation.However,wecannotusethemethodofmatrixinversiontoﬁndthe\n3 8",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nsolution.\nSofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\nalsopossibletodeﬁneaninversethatismultipliedontheright:\nAA− 1= I . (2.29)\nForsquarematrices,theleftinverseandrightinverseareequal.\n2.5Norms\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\nisgivenby\n||||x p=\ni| x i|p 1\np\n(2.30)\nfor p , p . ∈ R≥1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "||||x p=\ni| x i|p 1\np\n(2.30)\nfor p , p . ∈ R≥1\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatisﬁes\nthefollowingproperties:\n• ⇒ f() = 0 xx= 0\n• ≤ f(+) xy f f ()+x ()y(thetriangleinequality)\n•∀∈ || α R , f α(x) = α f()x\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\nEuclideandistancefromtheorigintothepointidentiﬁedbyx.The L2normis",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "usedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\nthesquared L2norm,whichcanbecalculatedsimplyasxx.\nThesquared L2normismoreconvenienttoworkwithmathematically and\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "ontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\n3 9",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\nthe L1norm.The L1normmaybesimpliﬁedto\n||||x 1=\ni| x i| . (2.31)\nThe L1normiscommonlyusedinmachinelearningwhenthediﬀerencebetween\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\nawayfrom0by,the  L1normincreasesby. ",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "awayfrom0by,the  L1normincreasesby. \nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\nelements.Someauthorsrefertothisfunctionasthe“ L0norm,”butthisisincorrect\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\nscalingthevectorby αdoesnotchangethenumberofnonzeroentries. The L1\nnormisoftenusedasasubstituteforthenumberofnonzeroentries.\nOneothernormthatcommonlyarisesinmachinelearningisthe L∞norm,\nalsoknownasthemaxnorm.Thisnormsimpliﬁestotheabsolutevalueofthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "elementwiththelargestmagnitudeinthevector,\n||||x ∞= max\ni| x i| . (2.32)\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\nFrobeniusnorm:\n|||| A F=\ni , jA2\ni , j , (2.33)\nwhichisanalogoustothe L2normofavector.\nThedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,\nxyx= |||| 2||||y 2cos θ (2.34)\nwhereistheanglebetweenand. θ xy\n2.6SpecialKindsofMatricesandVectors",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "2.6SpecialKindsofMatricesandVectors\nSomespecialkindsofmatricesandvectorsareparticularlyuseful.\nDiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\nthemaindiagonal. Formally,amatrixDisdiagonalifandonlyif D i , j=0for\n4 0",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nall i= j. Wehavealreadyseenoneexampleofadiagonalmatrix: theidentity\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\nDiagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\nisverycomputationally eﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeach\nelement x iby v i.Inotherwords,diag(v)x=vx.Invertingasquarediagonal",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "matrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\nandinthatcase,diag(v)− 1=diag([1 /v 1 , . . . ,1 /v n]).Inmanycases,wemay\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\nmatricestobediagonal.\nNotalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "possibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\nelementsofthevectorifiswiderthanitistall. D\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\nAA= . (2.35)\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "ifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric.\nA isavectorwith : unitvectorunitnorm\n||||x 2= 1 . (2.36)\nAvectorxandavectoryareorthogonaltoeachotherifxy= 0.Ifboth\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm.\nIfthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\northonormal.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "orthonormal.\nAnorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\nmalandwhosecolumnsaremutuallyorthonormal:\nAAAA= = I . (2.37)\n4 1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nThisimpliesthat\nA− 1= A, (2.38)\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\nPaycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\n2.7Eigendecomposition\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\nconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "bythewaywechoosetorepresentthem.\nForexample,integerscanbedecomposedintoprimefactors.Thewaywe\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\norinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentation\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\nintegermultipleofwillbedivisibleby. 12 3\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "showusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\nrepresentationofthematrixasanarrayofelements.\nOneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\neigenvalues.\nAneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\nplicationbyaltersonlythescaleof: A v\nAvv= λ . (2.39)\nThescalar λisknownastheeigenvaluecorrespondingtothiseigenvector.(One",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "canalsoﬁndalefteigenvectorsuchthatvA= λv, butweareusually\nconcernedwithrighteigenvectors).\nIfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s ∈ R= 0.\nMoreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\nforuniteigenvectors.\nSupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\nv( ) n},withcorrespondingeigenvalues { λ 1 , . . . , λ n}.Wemayconcatenateallofthe\n4 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n\u0000 \u0000 \u0000    \n\u0000\u0000\u0000 \n                     \n\u0000 \u0000 \u0000    \n\n\u0000\u0000\u0000\n   \n                                                     \nFigure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehave\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue λ 1andv( 2 )with",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "eigenvalue λ 2. ( L e f t )Weplotthesetofallunitvectorsu∈ R2asaunitcircle. ( R i g h t )We\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\ncanseethatitscalesspaceindirectionv( ) iby λ i.\neigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , . . . ,\nλ n].The ofisthengivenby eigendecompositionA\nAVλV = diag()− 1. (2.40)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "AVλV = diag()− 1. (2.40)\nWehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-\ntorsallowsustostretchspaceindesireddirections. Ho wever,weoftenwantto\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\nNoteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n4 3",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.\nFortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassof\nmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetric\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\nandeigenvalues:\nAQQ = Λ, (2.41)\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Λisa\ndiagonalmatrix.TheeigenvalueΛ i , iisassociatedwiththeeigenvectorincolumn i",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "ofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\nscalingspaceby λ iindirectionv( ) i.Seeﬁgureforanexample.2.3\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Λ",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "indescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\nifalloftheeigenvaluesareunique.\nTheeigendecompositionof amatrix tellsus many usefulfactsabout the\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\nTheeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\nquadraticexpressionsoftheform f(x) =xAxsubjectto||||x 2= 1.Wheneverx\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "Themaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\nAmatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.A\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁ-\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,and\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positive\nsemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx ,Ax≥0.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "PositivedeﬁnitematricesadditionallyguaranteethatxAxx = 0 ⇒ = 0.\n2.8SingularValueDecomposition\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\n4 4",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\nexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwe\nmustuseasingularvaluedecompositioninstead.\nRecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\namatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewrite\nAas\nAVλV = diag()− 1. (2.42)\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "asaproductofthreematrices:\nAUDV = . (2.43)\nSupposethatAisan m n×matrix.ThenUisdeﬁnedtobean m m×matrix,\nD V tobeanmatrix,and m n× tobeanmatrix. n n×\nEachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesU\nandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobe\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\ncolumnsofareknownasasthe V right-singularvectors.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "columnsofareknownasasthe V right-singularvectors.\nWecanactuallyinterpretthesingularvaluedecompositionofAintermsof\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\neigenvectorsofAA.Theright-singularvectorsofAaretheeigenvectorsofAA.\nThenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAA.\nThesameistrueforAA.\nPerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\nsection.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "section.\n2.9TheMoore-PenrosePseudoinverse\nMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewant\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\nAxy= (2.44)\n4 5",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nbyleft-multiplyingeachsidetoobtain\nxBy= . (2.45)\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\nuniquemappingfromto.AB\nIfAistallerthanitiswide, thenitispossibleforthisequationtohave\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\nsolutions.\nTheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\nthesecases.Thepseudoinverseofisdeﬁnedasamatrix A\nA+=lim\nα  0(AAI+ α)− 1A. (2.46)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "A+=lim\nα  0(AAI+ α)− 1A. (2.46)\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-\ntion,butrathertheformula\nA+= VD+U, (2.47)\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\nelementsthentakingthetransposeoftheresultingmatrix.\nWhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\npseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovides",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\nsolutions.\nWhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.\nInthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\npossibletointermsofEuclideannorm y ||−||Axy 2.\n2.10TheTraceOperator\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\nTr() =A\niA i , i . (2.48)\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\ndiﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "4 6",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\nanalternativewayofwritingtheFrobeniusnormofamatrix:\n|||| A F=\nTr(AA) . (2.49)\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\nmanipulatetheexpressionusingmanyusefulidentities. Forexample,thetrace\noperatorisinvarianttothetransposeoperator:\nTr() = Tr(AA) . (2.50)\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "movingthelastfactorintotheﬁrstposition,iftheshapesofthecorresponding\nmatricesallowtheresultingproducttobedeﬁned:\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\normoregenerally,\nTr(n\ni = 1F( ) i) = Tr(F( ) nn − 1\ni = 1F( ) i) . (2.52)\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\ndiﬀerentshape.Forexample,forA∈ Rm n ×andB∈ Rn m ×,wehave\nTr( ) = Tr( )ABBA (2.53)\neventhoughAB∈ Rm m ×andBA∈ Rn n ×.\nAnotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a).\n2.11TheDeterminant",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "2.11TheDeterminant\nThedeterminant ofa squarematrix, denoted det(A), isa functionmapping\nmatricesto realscalars.Thedeterminant isequal totheproductof allthe\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\nthetransformationpreservesvolume.\n4 7",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n2.12Example:PrincipalComponentsAnalysis\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\ncanbederivedusingonlyknowledgeofbasiclinearalgebra.\nSupposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.\nWewouldliketoloseaslittleprecisionaspossible.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "Wewouldliketoloseaslittleprecisionaspossible.\nOnewaywecanencodethesepointsistorepresentalower-dimensionalversion\nofthem.Foreachpointx( ) i∈ Rnwewillﬁndacorrespondingcodevectorc( ) i∈ Rl.\nIf lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\noriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecode\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\ninputgivenitscode, .xx ≈ g f(())\nPCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "decoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\ninto Rn.Let,where g() = cDcD∈ Rn l ×isthematrixdeﬁningthedecoding.\nComputingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.To\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\ntoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unless\nl n= )\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "theproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\nnorm.\nInordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrst\nthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗for\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\ninputpointxanditsreconstruction, g(c∗).Wecanmeasurethisdistanceusinga\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\nc∗= argmin\nc||− ||x g()c 2 . (2.54)\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "bothareminimizedbythesamevalueofc.Bothareminimizedbythesame\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\n4 8",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nmonotonically increasingfornon-negative arguments.\nc∗= argmin\nc||− ||x g()c2\n2 . (2.55)\nThefunctionbeingminimizedsimpliﬁesto\n( ())x− gc( ())x− gc (2.56)\n(bythedeﬁnitionofthe L2norm,equation)2.30\n= xxx−g g ()c−()cxc+( g)g()c (2.57)\n(bythedistributiveproperty)\n= xxx−2g g ()+c ()cg()c (2.58)\n(becausethescalar g()cxisequaltothetransposeofitself).\nWecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,\nsincethistermdoesnotdependon:c\nc∗= argmin",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "sincethistermdoesnotdependon:c\nc∗= argmin\nc−2xg g ()+c ()cg .()c (2.59)\nTomakefurtherprogress,wemustsubstituteinthedeﬁnitionof: g()c\nc∗= argmin\nc−2xDcc+DDc (2.60)\n= argmin\nc−2xDcc+I lc (2.61)\n(bytheorthogonalityandunitnormconstraintson)D\n= argmin\nc−2xDcc+c (2.62)\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\nyoudonotknowhowtodothis):\n∇ c(2−xDcc+c) = 0 (2.63)\n−2Dxc+2= 0 (2.64)\ncD= x . (2.65)\n4 9",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nThismakesthealgorithmeﬃcient: wecanoptimallyencodexjustusinga\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\nf() = xDx . (2.66)\nUsingafurthermatrixmultiplication, wecanalsodeﬁnethePCAreconstruction\noperation:\nr g f () = x (()) = xDDx . (2.67)\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "pointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\noferrorscomputedoveralldimensionsandallpoints:\nD∗= argmin\nD\ni , j\nx( ) i\nj− r(x( ) i) j2\nsubjecttoDDI= l(2.68)\nToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecase\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\nd∗= argmin\nd\ni||x( ) i−ddx( ) i||2\n2subjectto||||d 2= 1 .(2.69)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "2subjectto||||d 2= 1 .(2.69)\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\nscalarvaluedx( ) iontherightofthevectord.Itismoreconventionaltowrite\nscalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\nsuchaformulaas\nd∗= argmin\nd\ni||x( ) i−dx( ) id||2\n2subjectto||||d 2= 1 ,(2.70)\nor,exploitingthefactthatascalarisitsowntranspose,as\nd∗= argmin\nd\ni||x( ) i−x( ) i dd||2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "d∗= argmin\nd\ni||x( ) i−x( ) i dd||2\n2subjectto||||d 2= 1 .(2.71)\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .\n5 0",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\nThiswillallowustousemorecompactnotation.LetX∈ Rm n ×bethematrix\ndeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i.\nWecannowrewritetheproblemas\nd∗= argmin\nd||−XXdd||2\nFsubjecttodd= 1 .(2.72)\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\nportionasfollows:\nargmin\nd||−XXdd||2\nF (2.73)\n= argmin\ndTr",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "argmin\nd||−XXdd||2\nF (2.73)\n= argmin\ndTr\nXXdd −\nXXdd −\n(2.74)\n(byequation)2.49\n= argmin\ndTr(XXX−Xdd−ddXXdd+XXdd)(2.75)\n= argmin\ndTr(XX)Tr(−XXdd)Tr(−ddXX)+Tr(ddXXdd)\n(2.76)\n= argmin\nd−Tr(XXdd)Tr(−ddXX)+Tr(ddXXdd)(2.77)\n(becausetermsnotinvolvingdonotaﬀectthe) d argmin\n= argmin\nd−2Tr(XXdd)+Tr(ddXXdd)(2.78)\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\n= argmin\nd−2Tr(XXdd)+Tr(XXdddd)(2.79)\n(usingthesamepropertyagain)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "(usingthesamepropertyagain)\nAtthispoint,were-introducetheconstraint:\nargmin\nd−2Tr(XXdd)+Tr(XXdddd)subjecttodd= 1(2.80)\n= argmin\nd−2Tr(XXdd)+Tr(XXdd)subjecttodd= 1(2.81)\n(duetotheconstraint)\n= argmin\nd−Tr(XXdd)subjecttodd= 1(2.82)\n5 1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n= argmax\ndTr(XXdd)subjecttodd= 1(2.83)\n= argmax\ndTr(dXXdd )subjecttod= 1(2.84)\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,\ntheoptimaldisgivenbytheeigenvectorofXXcorrespondingtothelargest\neigenvalue.\nThisderivationisspeciﬁctothecaseof l=1andrecoversonlytheﬁrst\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "largesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\nwritingthisproofasanexercise.\nLinearalgebraisoneofthefundamentalmathematical disciplinesthatis\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\nubiquitousinmachinelearningisprobabilitytheory,presentednext.\n5 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 2\nA p p l i cat i on s\nInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-\nputervision,speechrecognition,naturallanguageprocessing,andotherapplication\nareasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork\nimplementationsrequiredformostseriousAIapplications.Next,wereviewseveral\nspeciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve. Whileone\ngoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "varietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\ntasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.\nLanguagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\nvocabulary)perinputfeature.\n12. 1 L arge- S c a l e D eep L earni n g\nDeeplearningisbasedonthephilosophyofconnectionism: whileanindividual\nbiologicalneuronoranindividualfeatureinamachinelearningmodelisnot\nintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "exhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\nnumberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe\nimprovementinneuralnetwork’saccuracyandtheimprovementofthecomplexity\noftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\nthesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3\ngrownexponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksare\nonlyaslargeasthenervoussystemsofinsects.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "onlyaslargeasthenervoussystemsofinsects.\nBecausethesizeofneuralnetworksisofparamountimportance,deeplearning\n443",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nrequireshighperformancehardwareandsoftwareinfrastructure.\n12.1.1FastCPUImplementations\nTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.\nToday,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPU\ncomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\ntheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\nnotmanagethehighcomputational workloadrequiredbyneuralnetworks.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "AdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyond\nthescopeofthisbook,butweemphasizeherethatcarefulimplementation for\nspeciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\nCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-point\narithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-\npointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover\nastrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformance",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "characteristics,sosometimesﬂoating-pointimplementations canbefastertoo.\nTheimportantprincipleisthatcarefulspecializationofnumericalcomputation\nroutinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouse\nﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemisses\nandusingvectorinstructions.Manymachinelearningresearchersneglectthese\nimplementationdetails,butwhentheperformanceofanimplementation restricts\nthesizeofthemodel,theaccuracyofthemodelsuﬀers.\n12.1.2GPUImplementations",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "12.1.2GPUImplementations\nMostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\nunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\nthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\nvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\nperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\nbeneﬁcialforneuralnetworksaswell.\nVideogamerenderingrequiresperformingmanyoperationsinparallelquickly.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "Modelsof characters and environments arespeciﬁed intermsof listsof 3-D\ncoordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and\ndivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D\non-screencoordinates.Thegraphicscardmustthenperformmanycomputations\nateachpixelinparalleltodeterminethecolorofeachpixel. Inbothcases,the\n4 4 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncomputations arefairlysimpleanddonotinvolvemuchbranchingcomparedto\nthecomputational workloadthataCPUusuallyencounters.Forexample,each\nvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno\nneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply\nby.Thecomputations arealsoentirelyindependentofeachother,andthusmay\nbeparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuﬀersof",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "memory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject\ntoberendered.Together,thisresultsingraphicscardshavingbeendesignedto\nhaveahighdegreeofparallelismandhighmemorybandwidth,atthecostof\nhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditional\nCPUs.\nNeuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\nreal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\nlargeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "eachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\nbuﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer\nsothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.\nGPUsoﬀeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.\nNeuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor\nsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural\nnetworkscanbedividedintomultipleindividual“neurons”thatcanbeprocessed",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "independentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily\nbeneﬁtfromtheparallelismofGPUcomputing.\nGPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor\ngraphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustom\nsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto\npixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe\nbasedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputingby",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "writingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrau e t a l .\n()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005\nreportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,\nChellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto\nacceleratesupervisedconvolutionalnetworks.\nThepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "code,notjustrenderingsubroutines. NVIDIA’sCUDAprogramming language\nprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\nrelativelyconvenientprogramming model,massiveparallelism,andhighmemory\n4 4 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nbandwidth,GP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming.\nThisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\navailable(,; ,). Raina e t a l .2009Ciresan e t a l .2010\nWritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospe-\ncialists. ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery\ndiﬀerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually\ndesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "writablememorylocationsarenotcached,soitcanactuallybefastertocompute\nthesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.\nGPUcodeisalsoinherentlymulti-threaded andthediﬀerentthreadsmustbe\ncoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif\ntheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan\neachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\ntransaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofread",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "orwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n\nthreads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower\nof2. TheexactspeciﬁcationsdiﬀerbetweenmodelsofGPU.Anothercommon\nconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthe\nsameinstructionsimultaneously.Thismeansthatbranchingcanbediﬃculton\nGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp\nexecutesthesameinstructionduringeachcycle,soifdiﬀerentthreadswithinthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "samewarpneedtoexecutediﬀerentcodepaths,thesediﬀerentcodepathsmust\nbetraversedsequentiallyratherthaninparallel.\nDuetothediﬃcultyofwritinghighperformanceGPUcode,researchersshould\nstructuretheirworkﬂowtoavoidneedingtowritenewGPUcodeinordertotest\nnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\nofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then\nspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "machinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciﬁesallofits\nmachinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010\nBastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010\nhigh-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\nmultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\neitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l .\n2011b)providesimilarfeatures.\n4 4 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.1.3Large-ScaleDistributedImplementations\nInmanycases,thecomputational resourcesavailableonasinglemachineare\ninsuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinference\nacrossmanymachines.\nDistributinginferenceissimple,becauseeachinputexamplewewanttoprocess\ncanberunbyaseparatemachine.Thisisknownas .dataparallelism\nItisalsopossibletogetmodelparallelism,wheremultiplemachineswork\ntogetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "model.Thisisfeasibleforbothinferenceandtraining.\nDataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\noftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\nreturnsintermsofoptimization performance.Itwouldbebettertoallowmultiple\nmachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\nthestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:\nthegradientatstepisafunctionoftheparametersproducedbystep. t t−1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "Thiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-\ngio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare\nthememoryrepresentingtheparameters.Eachcorereadsparameterswithouta\nlock,thencomputesagradient,thenincrementstheparameterswithoutalock.\nThisreducestheaverageamountofimprovementthateachgradientdescentstep\nyields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreased\nrateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012\ntogradientdescent,wheretheparametersaremanagedbyaparameterserver\nratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\nremainstheprimarystrategyfortraininglargedeepnetworksandisusedby\nmostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,\n2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescale\nofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "distributednetworkswithrelativelylow-costhardwareavailableintheuniversity\nsetting( ,). Coates e t a l .2013\n12.1.4ModelCompression\nInmanycommercialapplications,itismuchmoreimportantthatthetimeand\nmemorycostofrunninginferenceinamachinelearningmodelbelowthanthat\nthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire\n4 4 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npersonalization,itispossibletotrainamodelonce,thendeployittobeusedby\nbillionsofusers.Inmanycases,theenduserismoreresource-constrainedthan\nthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\npowerfulcomputercluster,thendeployitonmobilephones.\nAkeystrategyforreducingthecostofinferenceismodelcompression(Bu-\nciluˇa2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\nexpensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "storeandevaluate.\nModelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\nprimarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththe\nlowestgeneralization errorisanensembleofseveralindependentlytrainedmodels.\nEvaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel\ngeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).\nTheselargemodelslearnsomefunction f(x),butdosousingmanymore\nparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "thelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunction\nf(x),wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simply\nbyapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,\nmodeltomatch f(x)onthesepoints.Inordertomosteﬃcientlyusethecapacity\nofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution\nresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan\nbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "modeltrainedontheoriginaltrainingset.\nAlternatively,onecantrainthesmallermodelonlyontheoriginaltraining\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,).\n12.1.5DynamicStructure\nOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems\nthathavedynamicstructureinthegraphdescribingthecomputationneeded\ntoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "subsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\nnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\noffeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\nformofdynamicstructureinsideneuralnetworksissometimescalledconditional\ncomputation(,; ,). Sincemanycomponentsof Bengio2013Bengio e t a l .2013b\nthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\n4 4 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.\nDynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\ngenerallythroughoutthesoftwareengineeringdiscipline. Thesimplestversions\nofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\nsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\nbeappliedtoaparticularinput.\nAvenerablestrategyforacceleratinginferenceinaclassiﬁeristouseacascade",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "ofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\npresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\nwemustuseasophisticatedclassiﬁerwithhighcapacity,thatisexpensivetorun.\nHowever,becausetheobjectisrare,wecanusuallyusemuchlesscomputation\ntorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain\nasequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacity,\nandaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "wedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁer\nistrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe\nclassiﬁersinasequence,abandoninganyexampleassoonasanyoneelementin\nthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\nhighconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecost\noffullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascade\ncanachievehighcapacity.Onewayistomakethelatermembersofthecascade",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "individuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\nhighcapacity,becausesomeofitsindividualmembersdo. Itisalsopossibleto\nmakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\nasawholehashighcapacityduetothecombinationofmanysmallmodels.Viola\nandJones2001()usedacascadeofboosteddecisiontreestoimplementafastand\nrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiﬁer\nlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "areexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades\nusestheearliermodelstoimplementasortofhardattentionmechanism:the\nearlymembersofthecascadelocalizeanobjectandlatermembersofthecascade\nperformfurtherprocessinggiventhelocationoftheobject.Forexample,Google\ntranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade\nthatﬁrstlocatestheaddressnumberwithonemachinelearningmodelandthen\ntranscribesitwithanother(Goodfellow2014d e t a l .,).",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.\nAsimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\n4 4 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\nsplittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\ndonewiththeprimarygoalofacceleratinginferencecomputations.\nInthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect\nwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,\ngiventhecurrentinput.Theﬁrstversionofthisideaiscalledthemixtureof\nexperts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "ofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,\nandtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputof\ntheexperts.Inthatcase, theuseofthegaterdoesnotoﬀerareductionin\ncomputational cost,butifasingleexpertischosenbythegaterforeachexample,\nweobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002\ncanconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\nwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "whenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossible\ntousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)\nallthegaterconﬁgurations. Todealwiththisproblem,severalapproacheshave\nbeenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b\nseveralestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l .\n()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "gradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\nanactualreductionincomputational costwithoutimpactingnegativelyonthe\nqualityoftheapproximation.\nAnother kindof dynamicstructure isa switch, where ahidden unit can\nreceiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicrouting\napproachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993\nSofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,\nandthusdonotachieveallofthepossiblecomputational beneﬁtsofdynamic\nstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1\nOnemajorobstacletousingdynamicallystructuredsystemsisthedecreased\ndegreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranches\nfordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\nasmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "canwritemorespecializedsub-routinesthatconvolveeachexamplewithdiﬀerent\nkernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumns\nofweights.Unfortunately, thesemorespecializedsubroutinesarediﬃcultto\nimplementeﬃciently.CPUimplementations willbeslowduetothelackofcache\n4 5 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncoherenceandGPUimplementations willbeslowduetothelackofcoalesced\nmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptake\ndiﬀerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe\nexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroups\nofexamplessimultaneously. Thiscanbeanacceptablestrategyforminimizing\nthetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄinesetting.In",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "areal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning\ntheworkloadcanresultinload-balancing issues.Forexample,ifweassignone\nmachinetoprocesstheﬁrststepinacascadeandanothermachinetoprocess\nthelaststepinacascade,thentheﬁrstwilltendtobeoverloadedandthelast\nwilltendtobeunderloaded. Similarissuesariseifeachmachineisassignedto\nimplementdiﬀerentnodesofaneuraldecisiontree.\n12.1.6SpecializedHardwareImplementationsofDeepNetworks",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "Sincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\nonspecializedhardwareimplementations thatcouldspeeduptrainingand/or\ninferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\nspecializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l .\n2003MisraandSaha2010 ; ,).\nDiﬀerentformsofspecializedhardware(GrafandJackel1989Meadand ,;\nIsmail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "beendevelopedoverthelastdecades,eitherwithASICs(application-speciﬁcinte-\ngratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),\nanalog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-\nmentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations\n(combiningdigitalandanalogcomponents).InrecentyearsmoreﬂexibleFPGA\n(ﬁeldprogrammable gatedarray)implementations(wheretheparticularsofthe\ncircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\nandGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoatingpoint\nnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\nleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;\nandHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,\n2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\nhasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "hardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\nresearchonspecializedhardwarefordeepnetworksisthattherateofprogressof\nasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\n4 5 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncomputingspeedhavecomefromparallelization acrosscores(eitherinCPUsor\nGPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneural\nnetworkera)wherethehardwareimplementations ofneuralnetworks(whichmight\ntaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\ntherapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\nhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "designsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\ngeneral-public applicationsofdeeplearning(e.g.,withspeech,computervisionor\nnaturallanguage).\nRecentworkonlow-precisionimplementationsofbackprop-based neuralnets\n(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests\nthatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeep\nneuralnetworkswithback-propagation. Whatisclearisthatmoreprecisionis",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "requiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\nﬁxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare\nrequiredpernumber.Traditionalﬁxedpointnumbersarerestrictedtoaﬁxed\nrange(whichcorrespondstoagivenexponentinaﬂoatingpointrepresentation).\nDynamicﬁxedpointrepresentationssharethatrangeamongasetofnumbers\n(suchasalltheweightsinonelayer).Usingﬁxedpointratherthanﬂoatingpoint\nrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "powerrequirementsandcomputingtimeneededforperformingmultiplications,\nandmultiplications arethemostdemandingoftheoperationsneededtouseor\ntrainamoderndeepnetworkwithbackprop.\n12. 2 C om p u t er V i s i on\nComputervisionhastraditionallybeenoneofthemostactiveresearchareasfor\ndeeplearningapplications,becausevisionisataskthatiseﬀortlessforhumans\nandmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983\nthemostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "ofobjectrecognitionoropticalcharacterrecognition.\nComputervisionisaverybroadﬁeldencompassingawidevarietyofways\nofprocessingimages,andanamazingdiversityofapplications. Applicationsof\ncomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizing\nfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof\nthelattercategory,onerecentcomputervisionapplicationistorecognizesound\nwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l .",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch\n4 5 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut\nratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\nlearningforcomputervisionisusedforobjectrecognitionordetectionofsome\nform,whetherthismeansreportingwhichobjectispresentinanimage,annotating\nanimagewithboundingboxesaroundeachobject,transcribingasequenceof\nsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\nobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "ofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\nusingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o\ncomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\nimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\nremovingobjectsfromimages.\n12.2.1Preprocessing\nManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\ninputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturesto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "represent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\nprocessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\nsame,reasonablerange,like[0,1]or[-1,1]. Mixingimagesthatliein[0,1]with\nimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\nthesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\ncomputervisionarchitectures requireimagesofastandardsize,soimagesmustbe\ncroppedorscaledtoﬁtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust\nthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,\n1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically\nscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\nimage( ,). Hadsell e t a l .2007\nDatasetaugmentation maybeseenasawayofpreprocessingthetrainingset\nonly.Datasetaugmentationisanexcellentwaytoreducethegeneralization error",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "ofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\nthemodelmanydiﬀerentversionsofthesameinput(forexample,thesameimage\ncroppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthe\nmodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\nensembleapproach,andhelpstoreducegeneralization error.\nOtherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith\nthegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "amountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\n4 5 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nvariationinthedatacanbothreducegeneralization errorandreducethesizeof\nthemodelneededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmaller\nmodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing\nofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput\ndatathatiseasyforahumandesignertodescribeandthatthehumandesigner\nisconﬁdenthasnorelevancetothetask.Whentrainingwithlargedatasetsand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "largemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust\nletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For\nexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing\nstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky\ne t a l .,).2012\n12.2.1.1ContrastNormalization\nOneofthemostobvioussourcesofvariationthatcanbesafelyremoved for\nmanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "magnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage.\nTherearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\ndeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\nimageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\nX∈ Rr c××3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving\nthegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe\nentireimageisgivenby\n1\n3 r cr \ni=1c \nj=13 \nk=1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "1\n3 r cr \ni=1c \nj=13 \nk=1\nX i , j , k−¯ X2(12.1)\nwhere ¯ Xisthemeanintensityoftheentireimage:\n¯ X=1\n3 r cr \ni=1c \nj=13 \nk=1X i , j , k . (12.2)\nGlobalcontrastnormalization(GCN)aimstopreventimagesfromhaving\nvaryingamountsofcontrastbysubtractingthemeanfromeachimage, then\nrescalingitsothatthe standarddeviation across its pixelsis equaltosome\nconstant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\nchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequal",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "intensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation\ncontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\n4 5 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\nmotivatesintroducingasmall,positiveregularizationparameter λtobiasthe\nestimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\ntobeatleast .Givenaninputimage X,GCNproducesanoutputimage X,\ndeﬁnedsuchthat\nX\ni , j , k= sX i , j , k−¯ X\nmax\n ,\nλ+1\n3 r cr\ni=1c\nj=13\nk=1\nX i , j , k−¯ X2 .(12.3)\nDatasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "tocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\ntopracticallyignorethesmalldenominator problembysetting λ= 0andavoid\ndivisionby0inextremelyrarecasesbysetting toanextremelylowvaluelike\n10−8. Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a\ndataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\nintensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011\n λ = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\ncloseto1,asdoneby (). Goodfellow e t a l .2013a\nThestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\noftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\npreferabletodeﬁneGCNintermsofstandarddeviationratherthan L2norm\nbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "basedonstandarddeviationallowsthesame stobeusedregardlessofimage\nsize.However,theobservationthatthe L2normisproportionaltothestandard\ndeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\nexamplestoasphericalshell.Seeﬁgureforanillustration.Thiscanbea 12.1\nusefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\ninspaceratherthanexactlocations.Respondingtomultipledistancesinthe\nsamedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerent",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "biases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover.\nAdditionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\nmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\nreducingeachexampletoadirectionratherthanadirectionandadistance.\nCounterintuitively,thereisapreprocessingoperationknownasspheringand\nitisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\nlieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave\n4 5 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n− 1 5 0 0 1 5 . . .\nx 0− 1 5 .0 0 .1 5 .x 1Rawinput\n− 1 5 0 0 1 5 . . .\nx 0GCN, = 10 λ− 2\n− 1 5 0 0 1 5 . . .\nx 0GCN, = 0 λ\nFigure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm.\n( C e n t e r )GCNwith λ= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse\ns= 1and = 10− 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation\nratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "GCN,with λ >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\nvariationintheirnorm.Weleaveandthesameasbefore. s \nequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\nsphericalcontours.Spheringismorecommonlyknownas .whitening\nGlobalcontrastnormalization willoftenfailtohighlightimagefeatureswe\nwouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge\ndarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "theshadowofabuilding)thenglobalcontrastnormalization willensurethereisa\nlargediﬀerencebetweenthebrightnessofthedarkareaandthebrightnessofthe\nlightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.\nThismotivateslocalcontrastnormalization.Localcontrastnormalization\nensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\ntheimageasawhole.Seeﬁgureforacomparisonofglobalandlocalcontrast 12.2\nnormalization.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "normalization.\nVariousdeﬁnitionsoflocalcontrastnormalization arepossible.Inallcases,\nonemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingby\nastandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\nandstandarddeviationofallpixelsinarectangularwindowcenteredonthe\npixeltobemodiﬁed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008\nandweightedstandarddeviationusingGaussianweightscenteredonthepixelto\nbemodiﬁed. Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolor\n4 5 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nInputimage GCN LCN\nFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀects\nofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\nscale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\ncontrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstant\nintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,\nsuchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "normalizationkernelbeingtoohigh.\nchannelsseparatelywhileotherscombineinformationfromdiﬀerentchannelsto\nnormalizeeachpixel( ,). Sermanet e t a l .2012\nLocalcontrastnormalization canusuallybeimplemented eﬃcientlybyusing\nseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\nlocalstandarddeviations,thenusingelement-wisesubtractionandelement-wise\ndivisionondiﬀerentfeaturemaps.\nLocalcontrastnormalization isadiﬀerentiable operationandcanalsobeusedas",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "anonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\noperationappliedtotheinput.\nAswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal\ncontrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast\nnormalization typicallyactsonsmallerwindows,itisevenmoreimportantto\nregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\nthesameaseachother,andthusmorelikelytohavezerostandarddeviation.\n4 5 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.2.1.2DatasetAugmentation\nAsdescribedinsection,itiseasytoimprovethegeneralization ofaclassiﬁer 7.4\nbyincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\nexamplesthathavebeenmodiﬁedwithtransformationsthatdonotchangethe\nclass.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenableto\nthisform ofdataset augmentationbecause theclass isinvariant toso many\ntransformationsandtheinputcanbeeasilytransformedwithmanygeometric",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "operations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,\nrotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecialized\ncomputervisionapplications,moreadvancedtransformationsarecommonlyused\nfordatasetaugmentation. Theseschemesincluderandomperturbationofthe\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\ntheinput( ,). LeCun e t a l .1998b\n12. 3 S p eec h R ec ogn i t i o n\nThetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "naturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\nthespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput\nvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\nspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\nfeatures,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011\nfromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "asequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)\ntaskconsistsofcreatingafunction f∗\nASRthatcomputesthemostprobablelinguistic\nsequencegiventheacousticsequence: y X\nf∗\nASR() = argmaxX\nyP∗( = ) y X|X (12.4)\nwhere P∗isthetrueconditionaldistributionrelatingtheinputsXtothetargets\ny.\nSincethe1980sanduntilabout2009–2012,state-of-theartspeechrecognition\nsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\nmodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "phonemes(,),whileHMMsmodeledthesequenceofphonemes. Bahl e t a l .1987\nTheGMM-HMM modelfamilytreats acousticwaveformsasbeinggenerated\nbythefollowingprocess: ﬁrstanHMMgeneratesasequenceofphonemesand\ndiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach\n4 5 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\naudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\nspeechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswere\napplied,andnumerousASRsystemsfromthelate1980sandearly1990sused\nneuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;\nFallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "performanceofASRbasedonneuralnetsapproximately matchedtheperformance\nofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved\n26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993\nphonemestodiscriminatebetween), whichwasbetterthanorcomparableto\nHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\nrecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.\nHowever,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "speechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystems\nonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\nforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\nacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\nfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.\nLater,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition\naccuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "forthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).\nStartingin2009,speechresearchersappliedaformofdeeplearningbasedon\nunsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\nbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\nmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III\nTosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\ndeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "Thesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow\n(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates\nforthatcenterframe.Trainingsuchdeepnetworkshelpedtosigniﬁcantlyimprove\ntherecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a\nphonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b\nanalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "recognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed\ne t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011\nbyworktoexpandthearchitecturefromphonemerecognition(whichiswhat\nTIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012\nwhichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof\nwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\n4 5 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\nontechniquessuchasrectiﬁedlinearunitsanddropout(,; Zeiler e t a l .2013Dahl\ne t a l .,). Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\nstartedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\ne t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a\narenowdeployedinproductssuchasmobilephones.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "arenowdeployedinproductssuchasmobilephones.\nLater,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\nratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\nofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\nunnecessaryordidnotbringanysigniﬁcantimprovement.\nThesebreakthroughs inrecognitionperformanceforworderrorrateinspeech\nrecognitionwereunprecedented (around30%improvement)andwerefollowinga",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "longperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith\nthetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof\ntrainingsets(seeﬁgure2.4ofDengandYu2014()).Thiscreatedarapidshiftin\nthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly\ntwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep\nneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning\nalgorithmsandarchitectures forASR,whichisstillongoingtoday.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l .\n2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\ntime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\ntwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\nlongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\nfrequencyofspectralcomponents.\nAnotherimportantpush, stillongoing,hasbeentowardsend-to-enddeep",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "learningspeechrecognitionsystemsthatcompletelyremovetheHMM.Theﬁrst\nmajorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained\nadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10\nphonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves\ne t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables\nfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\nordinarydepthduetoastackoflayers,anddepthduetotimeunfolding. This",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "workbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See\nPascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,\nappliedinothersettings.\nAnothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\nsystemlearnhowto“align”theacoustic-levelinformationwiththephonetic-level\n4 6 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ninformation( ,;,). Chorowski e t a l .2014Lu e t a l .2015\n12. 4 Nat u ra l L an gu a g e Pro c es s i n g\nNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas\nEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\nspecializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimple\nprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\ndescription. Naturallanguageprocessingincludesapplicationssuchasmachine",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "translation,inwhichthelearnermustreadasentenceinonehumanlanguageand\nemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\narebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequences\nofwords,charactersorbytesinanaturallanguage.\nAswiththeotherapplicationsdiscussedinthischapter,verygenericneural\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.\nHowever,toachieveexcellentperformanceandtoscalewelltolargeapplications,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "somedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelof\nnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\nsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\nofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\nnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\nanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\nbeendevelopedtomakemodelsofsuchaspaceeﬃcient,bothinacomputational",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "andinastatisticalsense.\n12.4.1-grams n\nAlanguagemodeldeﬁnesaprobabilitydistributionoversequencesoftokens\ninanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\nbeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\nearliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequences\noftokenscalled-grams.An-gramisasequenceoftokens. n n n\nModelsbasedon n-gramsdeﬁnetheconditionalprobabilityofthe n-thtoken\ngiventhepreceding n−1tokens.Themodelusesproductsoftheseconditional",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "distributionstodeﬁnetheprobabilitydistributionoverlongersequences:\nP x(1 , . . . , x τ) = ( P x1 , . . . , x n−1)τ\nt n=P x( t| x t n−+1 , . . . , x t−1) .(12.5)\n4 6 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nThisdecompositionisjustiﬁedbythechainruleofprobability.Theprobability\ndistributionovertheinitialsequence P( x1 , . . . , x n−1)maybemodeledbyadiﬀerent\nmodelwithasmallervalueof. n\nTraining n-grammodelsisstraightforwardbecausethemaximumlikelihood\nestimatecanbecomputedsimplybycountinghowmanytimeseachpossible n\ngramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore\nbuildingblockofstatisticallanguagemodelingformanydecades(Jelinekand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "Mercer1980Katz1987ChenandGoodman1999 ,;,; ,).\nForsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram\nfor n=2,andtrigramfor n=3. ThesenamesderivefromtheLatinpreﬁxesfor\nthecorrespondingnumbersandtheGreeksuﬃx“-gram”denotingsomethingthat\niswritten.\nUsuallywetrainbothan n-grammodelandan n−1 grammodelsimultaneously.\nThismakesiteasytocompute\nP x( t| x t n−+1 , . . . , x t−1) =P n( x t n−+1 , . . . , x t)\nP n−1( x t n−+1 , . . . , x t−1)(12.6)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "P n−1( x t n−+1 , . . . , x t−1)(12.6)\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\ninferencein P n,wemustomittheﬁnalcharacterfromeachsequencewhenwe\ntrain P n−1.\nAsanexample,wedemonstratehowatrigrammodelcomputestheprobability\nofthesentence“THEDOGRANAWAY.”Theﬁrstwordsofthesentencecannotbe\nhandledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\ncontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "abilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N).\nFinally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\ntionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6\nweobtain:\nP P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N .\n(12.7)\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "thoughthetuple ( x t n−+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo\ndiﬀerentkindsofcatastrophicoutcomes.When P n−1iszero,theratioisundeﬁned,\nsothemodeldoesnotevenproduceasensibleoutput.When P n−1isnon-zerobut\nP niszero,thetestlog-likelihoodis−∞. Toavoidsuchcatastrophicoutcomes,\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\n4 6 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.\nSee ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\ntechniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext\nsymbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniform\norDirichletprioroverthecountparameters.Anotherverypopularideaistoform\namixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "higher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing\nmorelikelytoavoidcountsofzero.Back-oﬀmethodslook-upthelower-order\nn-gramsifthefrequencyofthecontext x t−1 , . . . , x t n−+1istoosmalltousethe\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\ncontexts x t n k −+ , . . . , x t−1,forincreasing k,untilasuﬃcientlyreliableestimateis\nfound.\nClassical n-grammodelsareparticularlyvulnerabletothecurseofdimension-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "ality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha\nmassivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset.\nOnewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor\nlookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,\nsimilarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely\nlocalpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2\nisevenmoreseverethanusual,becauseanytwodiﬀerentwordshavethesamedis-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "tancefromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuch\ninformationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythe\nsamecontextareusefulforlocalgeneralization. T oovercometheseproblems,a\nlanguagemodelmustbeabletoshareknowledgebetweenonewordandother\nsemanticallysimilarwords.\nToimprovethestatisticaleﬃciencyof n-grammodels,class-basedlanguage\nmodels(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "thenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\nareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\nsetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\notherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\nIDstorepresentthecontextontherightsideoftheconditioningbar.Composite\nmodelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀare\nalsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "inwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\nlostinthisrepresentation.\n4 6 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.4.2NeuralLanguageModels\nNeurallanguagemodelsorNLMsare aclassoflanguagemodeldesigned\ntoovercomethecurseofdimensionalityproblemformodelingnaturallanguage\nsequencesbyusingadistributedrepresentationofwords( ,). Bengio e t a l .2001\nUnlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize\nthattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\nfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "word(anditscontext)andothersimilarwordsandcontexts.Thedistributed\nrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthe\nmodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\nworddogandthewordcatmaptorepresentationsthatsharemanyattributes,then\nsentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby\nthemodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere\naremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "transferringinformationfromeachtrainingsentencetoanexponentiallylarge\nnumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\nmodeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\nlength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\nexponentialnumberofsimilarsentences.\nWesometimescallthesewordrepresentationswordembeddings.Inthis\ninterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "tothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\nspaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\naone-hotvector,soeverypairofwordsisatEuclideandistance√\n2fromeach\nother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\n(oranypairofwordssharingsome“features”learnedbythemodel)arecloseto\neachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.\nFigurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow 12.3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.\nNeuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,a\nhiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”Usually\nNLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause\nnaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\nlayerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\nrepresented.\nThebasicideaofusingdistributedrepresentationstoimprovemodelsfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "naturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\nusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\n4 6 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmultiplelatentvariables(MnihandHinton2007,).\n− − − − − 3432302826−14−13−12−11−10−9−8−7−6\nCanadaEuropeOntario\nNorthEnglish\nCanadianUnionAfricanAfrica\nBritishFrance\nRussianChina\nGermanyFrench\nAssemblyEU JapanIraq\nSouthEuropean\n350355360365370375380 . . . . . . .171819202122\n1995199619971998199920002001\n200220032004\n20052006200720082009\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "machinetranslationmodel( ,),zoominginonspeciﬁcareaswhere Bahdanau e t a l .2015\nsemanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\nappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\nforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\ndimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.\n12.4.3High-DimensionalOutputs\nInmanynaturallanguageapplications,weoftenwantourmodelstoproduce",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "words(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\nvocabularies,itcanbeverycomputationally expensivetorepresentanoutput\ndistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\napplications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto\nrepresentingsuchadistributionistoapplyanaﬃnetransformationfromahidden\nrepresentationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\nwehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "componentofthisaﬃnetransformationisverylarge,becauseitsoutputdimension\nis|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh\ncomputational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall\n|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\ntimeaswellastesttime—wecannotcalculateonlythedotproductwiththeweight\nvectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer\nthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "attesttime(tocomputeprobabilities forallorselectedwords).Forspecialized\n4 6 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nlossfunctions,thegradientcanbecomputedeﬃciently( ,),but Vincent e t a l .2015\nthestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\nmanydiﬃculties.\nSupposethathisthetophiddenlayerusedtopredicttheoutputprobabilities\nˆy.IfweparametrizethetransformationfromhtoˆywithlearnedweightsW\nandlearnedbiasesb,thentheaﬃne-softmaxoutputlayerperformsthefollowing\ncomputations:\na i= b i+\njW i j h j∀∈{ ||} i1 , . . . , V , (12.8)\nˆ y i=ea i\n|| V\ni=1 eai . (12.9)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "ˆ y i=ea i\n|| V\ni=1 eai . (12.9)\nIfhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe\nthousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe\ncomputationofmostneurallanguagemodels.\n12.4.3.1UseofaShortList\nTheﬁrstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003\nofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\nsizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "builtuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost\nfrequentwords(handledbytheneuralnet)andatail T= V L\\ofmorerarewords\n(handledbyan n-grammodel). Tobeabletocombinethetwopredictions,the\nneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\nCbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\nunittoprovideanestimateof P( i C ∈| T ).Theextraoutputcanthenbeusedto\nachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "P y i C (= |) =1 i∈ L P y i C, i P i C (= | ∈ − L)(1 (∈| T ))\n+1 i∈ T P y i C, i P i C (= | ∈ T)(∈| T )(12.10)\nwhere P( y= i C, i| ∈ L)isprovidedbytheneurallanguagemodeland P( y= i|\nC, i∈ T) isprovidedbythe n-grammodel.Withslightmodiﬁcation,thisapproach\ncanalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmax\nlayer,ratherthanaseparatesigmoidunit.\nAnobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\nalizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "4 6 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nwords,where,arguably,itistheleastuseful. Thisdisadvantagehasstimulated\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\ndescribedbelow.\n12.4.3.2HierarchicalSoftmax\nAclassicalapproach(,)toreducingthecomputational burden Goodman2001\nofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose\nprobabilities hierarchically .Insteadofnecessitatinganumberofcomputations\nproportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "the|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand\nBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\nmodels.\nOnecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\nofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.\nThesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,\nthetreehasdepth O(log|| V). Theprobabilityofachoosingawordisgivenby\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "everynodeonapathfromtherootofthetreetotheleafcontainingtheword.\nFigureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\nhowtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\nthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\nsummationoverallofthepathsthatleadtothatword.\nTopredicttheconditionalprobabilities requiredateachnodeofthetree,we\ntypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "samecontext Casinputtoallofthesemodels.Becausethecorrectoutputis\nencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic\nregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,\ncorrespondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.\nBecausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowaslog|| V\nratherthan|| V),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnot\nonlythegradientwithrespecttotheoutputparametersbutalsothegradients",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "withrespecttothehiddenlayeractivations.\nItispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\ntheexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow\ntochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\ndoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword\nisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in\n4 6 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n( 1) ( 0)\n( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\nw 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\nFigure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords.\nInternalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "ofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)\ncontainstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}\nand{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which\nrespectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissuﬃcientlybalanced,\nthemaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof\nthenumberofwords|| V: thechoiceofoneoutof|| Vwordscanbeobtainedbydoing",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "O(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,\ncomputingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,\nassociatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom\ntheroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree\ntowardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct\nofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "nodeindexedbythepreﬁxofthesebits.Forexample,node(1 ,0)correspondstothe\npreﬁx( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:\nP w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)\n= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)\n4 6 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npractice,thecomputational savingsaretypicallynotworththeeﬀortbecausethe\ncomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation\nintheneurallanguagemodel.Forexample,supposethereare lfullyconnected\nhiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits\nrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese\nwords.Inthisexample,thenumberofoperationsneededtocomputethehidden\nactivationsgrowsasas O( l n2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "activationsgrowsasas O( l n2\nh)whiletheoutputcomputations growas O( n h n b).\nAslongas n b≤ l n h,wecanreducecomputationmorebyshrinking n hthanby\nshrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely\nexceedsamillionwordsandlog2(106)≈20,itispossibletoreduce n btoabout,20\nbut n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing\natreewithabranchingfactorof,onecaninsteaddeﬁneatreewithdepthtwo 2\nandabranchingfactorof\n|| V.Suchatreecorrespondstosimplydeﬁningaset",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "|| V.Suchatreecorrespondstosimplydeﬁningaset\nofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth\ntwocapturesmostofthecomputational beneﬁtofthehierarchicalstrategy.\nOnequestionthatremainssomewhatopenishowtobestdeﬁnetheseword\nclasses,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexisting\nhierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005\njointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexact",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "optimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword\nhierarchyisadiscreteone,notamenabletogradient-basedoptimization. However,\nonecouldusediscreteoptimization toapproximately optimizethepartitionof\nwordsintowordclasses.\nAnimportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\ntionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewantto\ncomputetheprobabilityofspeciﬁcwords.\nOfcourse,computingtheprobabilityofall|| Vwordswillremainexpensive",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "evenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe\nmostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\nprovideaneﬃcientandexactsolutiontothisproblem.\nAdisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\ntoapoorchoiceofwordclasses.\n12.4.3.3ImportanceSampling\nOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "computingthecontributionofthegradientfromallofthewordsthatdonotappear\n4 6 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ninthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\nmodel.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,\nitispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced\ninequation,thegradientcanbewrittenasfollows: 12.8\n∂ P y C log(|)\n∂ θ=∂logsoftmax y()a\n∂ θ(12.13)\n=∂\n∂ θlogea y\n\ni ea i(12.14)\n=∂\n∂ θ( a y−log\niea i) (12.15)\n=∂ a y\n∂ θ−\niP y i C (= |)∂ a i\n∂ θ(12.16)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "=∂ a y\n∂ θ−\niP y i C (= |)∂ a i\n∂ θ(12.16)\nwhereaisthevectorofpre-softmaxactivations(orscores),withoneelement\nperword.Theﬁrsttermisthepositivephaseterm(pushing a yup)whilethe\nsecondtermisthenegativephaseterm(pushing a idownforall i,withweight\nP( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith\naMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.\nSamplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,\nwhichispreciselywhatwearetryingtoavoid.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "whichispreciselywhatwearetryingtoavoid.\nInsteadofsamplingfromthemodel,onecansamplefromanotherdistribution,\ncalledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect\nforthebiasintroducedbysamplingfromthewrongdistribution(Bengioand\nSénécal2003BengioandSénécal2008 ,; ,).Thisisanapplicationofamoregeneral\ntechniquecalledimportancesampling,whichwillbedescribedinmoredetail\ninsection.Unfortunately,evenexactimportancesamplingisnoteﬃcient 17.2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "becauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan\nonlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor\nthisapplicationiscalledbiasedimportancesampling,wheretheimportance\nweightsarenormalizedtosumto1.Whennegativeword n iissampled,the\nassociatedgradientisweightedby\nw i=p n i /q n iN\nj=1 p n j /q n j. (12.17)\nTheseweightsareusedtogivetheappropriateimportancetothe mnegative\nsamplesfrom qusedtoformtheestimatednegativephasecontributiontothe\n4 7 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ngradient:\n|| V\ni=1P i C(|)∂ a i\n∂ θ≈1\nmm \ni=1w i∂ a n i\n∂ θ. (12.18)\nAunigramorabigramdistributionworkswellastheproposaldistribution q.Itis\neasytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\ntheparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently.\nImportancesamplingisnotonlyusefulforspeedingupmodelswithlarge\nsoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "sparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n\nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorv\nwhere v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe\ndocument.Alternately, v icanindicatethenumberoftimesthatword iappears.\nMachinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\nforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\nmaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "mostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\neveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\ncomputational beneﬁttousingsparseoutputs,becausethemodelmaychooseto\nmakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto\nbecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.\nDauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing\nimportancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "the“positivewords”(thosethatarenon-zerointhetarget)andanequalnumber\nof“negativewords.”Thenegativewordsarechosenrandomly,usingaheuristicto\nsamplewordsthataremorelikelytobemistaken. Thebiasintroducedbythis\nheuristicoversamplingcanthenbecorrectedusingimportanceweights.\nInallofthesecases,thecomputational complexityofgradientestimationfor\ntheoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\nratherthanproportionaltothesizeoftheoutputvector.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "ratherthanproportionaltothesizeoftheoutputvector.\n12.4.3.4Noise-ContrastiveEstimationandRankingLoss\nOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\ntionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\nexampleistherankinglossproposedbyCollobertandWeston2008a(),which\nviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\nmakethescoreofthecorrectword a yberankedhighincomparisontotheother\n4 7 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nscores a i.Therankinglossproposedthenis\nL=\nimax(01 ,− a y+ a i) . (12.19)\nThegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is\ngreaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith\nthiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which\nareusefulinsomeapplications,includingspeechrecognitionandtextgeneration\n(includingconditionaltextgenerationtaskssuchastranslation).",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-\ncontrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\nbeensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;\nandKavukcuoglu2013,).\n12.4.4CombiningNeuralLanguageModelswith-grams n\nAmajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels\nachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)\nwhilerequiringverylittlecomputationtoprocessanexample(bylookingup",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "onlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\ntoaccessthecounts,thecomputationusedfor n-gramsisalmostindependent\nofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameters\ntypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\nthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\nembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\nthecomputationtimeperexample.Someothermodels,suchastiledconvolutional",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "networks,canaddparameterswhilereducingthedegreeofparametersharing\ninordertomaintainthesameamountofcomputation. However,typicalneural\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\nproportionaltothenumberofparameters.\nOneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\nconsistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio\ne t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "theensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearning\nprovidesmanywaysofcombiningtheensemblemembers’predictions,including\nuniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()\nextendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.\nItisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\ntrainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining\n4 7 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\naneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\noutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare\nindicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese\nvariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity\nishuge—thenewportionofthearchitecturecontainsupto|| s Vnparameters—but\ntheamountofaddedcomputationneededtoprocessaninputisminimalbecause\ntheextrainputsareverysparse.\n12.4.5NeuralMachineTranslation",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "12.4.5NeuralMachineTranslation\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\nemittingasentencewiththeequivalentmeaninginanotherlanguage. Mac hine\ntranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\noftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\ntranslationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.For\nexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "directlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggests\nmanyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecond\ncomponentofthetranslationsystem,alanguagemodel,evaluatestheproposed\ntranslations,andcanscore“redapple”asbetterthan“applered.”\nTheearliestuseofneuralnetworksformachinetranslationwastoupgradethe\nlanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk\ne t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "usedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor\nmachinetranslationincludenotjusttraditionalback-oﬀ n-grammodels(Jelinek\nandMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum\nentropylanguagemodels(,),inwhichanaﬃne-softmaxlayer Berger e t a l .1996\npredictsthenextwordgiventhepresenceoffrequent-gramsinthecontext. n\nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\nsentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "aninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\nconditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1\nthatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditional\ndistributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable\noralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014\nmachinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "inthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\nreplacestheestimateprovidedbyconditional-grammodels. n\n4 7 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nD e c ode rO ut put ob j e c t   ( E ngl i s h \ns e nt e nc e )\nI nt e r m e di at e ,   s e m a n t i c   r e pr e s e nt a t i o n\nSourc e   ob j e c t   ( F r e nc h  s e n t e nc e   or   i m a g e )E nc ode r\nFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\nrepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.\nByusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "fromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\ntheinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\nrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\ntranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\ntomachinetranslationbutalsotocaptiongenerationfromimages.\nAdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\npreprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewould",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "liketouseamodelthatcanaccommodatevariablelengthinputsandvariable\nlengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\nofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\ngivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\nwhentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequence\nandemitsadatastructurethatsummarizestheinputsequence.Wecallthis\nsummarythe“context” C.Thecontext Cmaybealistofvectors,oritmaybea",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "vectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN\n(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional\nnetwork(KalchbrennerandBlunsom2013,). Asecondmodel,usuallyanRNN,\nthenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This\ngeneralideaofanencoder-decoderframeworkformachinetranslationisillustrated\ninﬁgure.12.5\nInordertogenerateanentiresentenceconditionedonthesourcesentence,the\nmodelmusthaveawaytorepresenttheentiresourcesentence. Earliermodels",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "wereonlyabletorepresentindividualwordsorphrases. Fromarepresentation\n4 7 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentences\nthathavethesamemeaninghavesimilarrepresentationsregardlessofwhether\ntheywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas\nexploredﬁrstusingacombinationofconvolutionsandRNNs(Kalchbrennerand\nBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed\ntranslations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014\n12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\nα( t − 1 )α( t − 1 )α( ) tα( ) tα( + 1 ) tα( + 1 ) t\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\n× × × × × ×+\nFigure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015\nessentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage\noffeaturevectorsh( ) twithweights α( ) t.Insomeapplications,thefeaturevectorshare",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "hiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\nweights α( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval\n[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage\napproximatesreadingthatonespeciﬁctimestepprecisely.Theweights α( ) tareusually\nproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\nofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "indexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The\nattentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximation\nthatcanbetrainedwithexistingoptimizationalgorithms.\nUsingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofavery\nlongsentenceofsay60wordsisverydiﬃcult. Itcanbeachievedbytraininga\nsuﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\ne t a l .()and2014aSutskever2014 e t a l .().However,amoreeﬃcientapproachis",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "toreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\n4 7 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\nfocusingonadiﬀerentpartoftheinputsentenceinordertogatherthesemantic\ndetailsthatarerequiredtoproducethenextoutputword. Thatisexactlythe\nideathat ()ﬁrstintroduced.Theattentionmechanismused Bahdanau e t a l .2015\ntofocusonspeciﬁcpartsoftheinputsequenceateachtimestepisillustratedin\nﬁgure.12.6\nWecanthinkofanattention-basedsystemashavingthreecomponents:",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "1.Aprocessthat“ r e a d s”rawdata(suchassourcewordsinasourcesentence),\nandconvertsthemintodistributedrepresentations,withonefeaturevector\nassociatedwitheachwordposition.\n2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe\nunderstoodasa“” containingasequenceoffacts,whichcanbe m e m o r y\nretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall\nofthem.\n3.Aprocessthat“”thecontentofthememorytosequentiallyperform e x p l o i t s",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "atask,ateachtimestephavingtheabilityputattentiononthecontentof\nonememoryelement(orafew,withadiﬀerentweight).\nThethirdcomponentgeneratesthetranslatedsentence.\nWhenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\ningwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\nthecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\nkindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "wordembeddingsinanother(Kočiský2014 e t a l .,),yieldingloweralignmenterror\nratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.\nThereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,\n2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcient\ncross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014\n12.4.6HistoricalPerspective\nTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()inoneoftheﬁrstexplorationsofback-propagation, withsymbols 1986a\ncorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturing\ntherelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\nsuchas(Colin,Mother,Victoria). The ﬁrstlayeroftheneuralnetworklearned\narepresentationofeachfamilymember.Forexample, thefeaturesforColin\n4 7 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas\nin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas\ncomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe\ndesiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois\nthemotherofColin.\nTheideaofforminganembeddingforasymbolwasextendedtotheideaofan\nembeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "usingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.\nThehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\npopularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Following\nthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural\nnetworkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented\ntheinputasasequenceofcharacters.\nBengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced\nneurallanguagemodels,whichproduceinterpretable wordembeddings.These",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "neuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbols\ninthe1980stomillionsofwords(includingpropernounsandmisspellings)in\nmodernapplications.Thiscomputational scalingeﬀortledtotheinventionofthe\ntechniquesdescribedaboveinsection.12.4.3\nInitially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\nimprovedlanguage modeling performance( ,).Tothisday, Bengio e t a l .2001\nnewtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015\nmodelingindividualbytesofUnicodecharacters.\nTheideasbehindneurallanguagemodelshavebeenextendedintoseveral\nnaturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004\nCollobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,\nsometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,\n2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross\ntasks.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "tasks.\nTwo-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\nalyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\nreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-\ncationtovisualizationwordembeddingsbyJosephTurianin2009.\n4 7 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12. 5 O t h er A p p l i c a t i o n s\nInthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\narediﬀerentfromthestandardobjectrecognition,speechrecognitionandnatural\nlanguageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\nscopeevenfurthertotasksthatremainprimarilyresearchareas.\n12.5.1RecommenderSystems\nOneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\ntechnologysectoristheabilitytomakerecommendations ofitemstopotential",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "usersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\nadvertisinganditemrecommendations (oftentheserecommendations arestillfor\nthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\nauserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\nbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\nmaydependonthevalueoftheproduct)ifanadisshownorarecommendation is\nmaderegardingthatproducttothatuser.Theinternetiscurrentlyﬁnancedin",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "greatpartbyvariousformsofonlineadvertising. Therearemajorpartsofthe\neconomythatrelyononlineshopping. CompaniesincludingAmazonandeBay\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations .\nSometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude\nselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto\nwatch,recommendingjokes,recommendingadvicefromexperts,matchingplayers\nforvideogames,ormatchingpeopleindatingservices.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "forvideogames,ormatchingpeopleindatingservices.\nOften,thisassociationproblemishandledlikeasupervisedlearningproblem:\ngivensomeinformationabouttheitemandabouttheuser,predicttheproxyof\ninterest(userclicksonad,userentersarating,userclicksona“like”button,user\nbuysproduct,userspendssomeamountofmoneyontheproduct,userspends\ntimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera\nregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "classiﬁcationproblem(predictingtheconditionalprobabilityofsomediscrete\nevent).\nTheearlyworkonrecommendersystemsreliedonminimalinformationas\ninputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\nonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\nthetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1\nanduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\n4 7 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\ncuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\nthenameofcollaborativeﬁltering.Bothnon-parametric approaches(suchas\nnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\npreferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\nonlearningadistributedrepresentation(alsocalledanembedding)foreachuser",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "andforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\nsimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\nofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween\ntheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\ndependonlyoneithertheuserIDortheitemID).LetˆRbethematrixcontaining\nourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith\nitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "akindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\ningeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\npredictionisthusobtainedasfollows:\nˆ R u , i= b u+ c i+\njA u , j B j , i . (12.20)\nTypicallyonewantstominimizethesquarederrorbetweenpredictedratings\nˆ R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe\nconvenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoor\nthree),ortheycanbeusedtocompareusersoritemsagainsteachother,just",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "likewordembeddings. One waytoobtaintheseembeddingsisbyperforminga\nsingularvaluedecompositionofthematrixRofactualtargets(suchasratings).\nThiscorrespondstofactorizingR=UDV(oranormalizedvariant)intothe\nproductoftwofactors,thelowerrankmatricesA=UDandB=V.One\nproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\nasiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\npayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "ofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\nbasedoptimization. TheSVDandthebilinearpredictionofequation both12.20\nperformedverywellinthecompetitionfortheNetﬂixprize( , BennettandLanning\n2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsby\nalargesetofanonymoususers. Manymachinelearningexpertsparticipatedin\nthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\nresearchinrecommendersystemsusingadvancedmachinelearningandyielded",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "improvementsinrecommendersystems.Eventhoughitdidnotwinbyitself,\nthesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels\n4 7 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npresentedbymostofthecompetitors,includingthewinners( ,; Töscher e t a l .2009\nKoren2009,).\nBeyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrst\nusesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirected\nprobabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement\noftheensembleofmethodsthatwontheNetﬂixcompetition(Töscher2009 e t a l .,;\nKoren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "havealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\nMnih2008,).\nHowever,thereisabasiclimitationofcollaborativeﬁlteringsystems:whena\nnewitemoranewuserisintroduced,itslackofratinghistorymeansthatthere\nisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or\nthedegreeofassociationbetween,say,thatnewuserandexistingitems.This\niscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving\nthecold-startrecommendation problemistointroduceextrainformationabout",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "theindividualusersanditems.Forexample,thisextrainformationcouldbeuser\nproﬁleinformationorfeaturesofeachitem. Systems thatusesuchinformation\narecalledcontent-basedrecommendersystems.Themappingfromarich\nsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha\ndeeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,).\nSpecializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso\nbeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "audiotracks,formusicrecommendation (vandenOörd2013 e t a l .,).Inthatwork,\ntheconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding\nfortheassociatedsong.Thedotproductbetweenthissongembeddingandthe\nembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.\n12.5.1.1ExplorationVersusExploitation\nWhenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary\nsupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "mendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\nbandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010\nusetherecommendation systemtocollectdata,wegetabiasedandincomplete\nviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems\ntheywererecommendedandnottotheotheritems. Inaddition,insomecases\nwemaynotgetanyinformationonusersforwhomnorecommendation hasbeen\nmade(forexample,withadauctions,itmaybethatthepriceproposedforan\n4 8 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe\nadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat\noutcomewouldhaveresultedfromrecommendinganyoftheotheritems.This\nwouldbeliketrainingaclassiﬁerbypickingoneclassˆ yforeachtrainingexample\nx(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and\nthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,\neachexampleconveyslessinformationthaninthesupervisedcasewherethetrue",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "label yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot\ncareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions\nevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada\nverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn\naboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning\nwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "learningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\nscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\nasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\nsensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\nthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\nhavebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\ncontextualbanditsreferstothecasewheretheactionistakeninthecontextof",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "someinputvariablethatcaninformthedecision.Forexample,weatleastknow\ntheuseridentity,andwewanttopickanitem.Themappingfromcontextto\nactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata\ndistribution(whichnowdependsontheactionsofthelearner)isacentralresearch\nissueinthereinforcementlearningandbanditsliterature.\nReinforcementlearningrequireschoosingatradeoﬀbetweenexplorationand\nexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "bestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward.\nExplorationreferstotakingactionsspeciﬁcallyinordertoobtainmoretraining\ndata.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot\nknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\npolicyandcontinuetakingaction ainordertoberelativelysureofobtaininga\nrewardof1.However,wemayalsowanttoexplorebytryingaction a.Wedonot\nknowwhatwillhappenifwetryaction a.Wehopetogetarewardof,butwe 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\nExplorationcanbeimplementedinmanyways,rangingfromoccasionally\ntakingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\nmodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\nrewardandthemodel’samountofuncertaintyaboutthatreward.\n4 8 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.\nOneofthemostprominentfactorsisthetimescaleweareinterestedin. Ifthe\nagenthasonlyashortamountoftimetoaccruereward,thenweprefermore\nexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\nmoreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmore\nknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\nmoreexploitation.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "moreexploitation.\nSupervised learninghas notradeoﬀ between explorationand exploitation\nbecausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeach\ninput.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetter\nthanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput.\nAnotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidesthe\nexploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparing",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\nandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\nevaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.The\npolicyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011\ntechniquesforevaluatingcontextualbandits.\n12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-\nswering\nDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "translationandnaturallanguageprocessingduetotheuseofembeddingsfor\nsymbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,\n2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\nandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\nrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\nthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced\nrepresentations.\n12.5.2.1Knowledge,RelationsandQuestionAnswering",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "12.5.2.1Knowledge,RelationsandQuestionAnswering\nOneinterestingresearchdirectionisdetermininghowdistributedrepresentations\ncanbetrainedtocapturetherelationsbetweentwoentities.Theserelations\nallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.\nInmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs\nthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset\n4 8 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ndonot.Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities\n{1 ,2 ,3}bydeﬁningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis\nrelationisdeﬁned,wecanuseitlikeaverb.Because(1 ,2)∈ S,wesaythat1is\nlessthan2.Because(2 ,1)∈ S,wecannotsaythat2islessthan1.Ofcourse,the\nentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnea\nrelation containingtupleslike(,). is_a_type_of dogmammal\nInthecontextofAI,wethinkofarelationasasentenceinasyntactically",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "simpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\nwhiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\nsentencestaketheformofatripletoftokens\n(subjectverbobject) , , (12.21)\nwithvalues\n(entityi ,relation j ,entityk) . (12.22)\nWecanalsodeﬁneanattribute,aconceptanalogoustoarelation,buttaking\nonlyoneargument:\n(entity i ,attribute j) . (12.23)\nForexample,wecoulddeﬁnethehas_furattribute,andapplyittoentitieslike\ndog.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "dog.\nManyapplicationsrequirerepresentingrelationsandreasoningaboutthem.\nHowshouldwebestdothiswithinthecontextofneuralnetworks?\nMachinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.\nTherearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\nstructureforthesedatabasesistherelationaldatabase,whichstoresthissame\nkindofinformation, alb eit notformattedasthreetokensentences.Whena",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "databaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor\nexpertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,\nwecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral\noneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized\nknowledgebases,likeGeneOntology.2Representationsforentitiesandrelations\ncanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\nandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).2013a\n1R e s p e c t i v e l y a v a i l a b l e   f ro m t h e s e   w e b   s i t e s : f r e e b a s e . c o m , c y c . c o m / o p e n c y c , w o r d n e t .\np r i n c e t o n . e d u w i k i b a . s e ,\n2g e n e o n t o l o g y . o r g\n4 8 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nInadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain.\nAcommonapproachistoextendneurallanguagemodelstomodelentitiesand\nrelations.Neurallanguagemodelslearnavectorthatprovidesadistributed\nrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\nsuchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\nofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\nanembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "languageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\nhavetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases\nnaturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or\ncombiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b\npossibilitiesexistfortheparticularparametrization associatedwithsuchamodel.\nEarlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),\noftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities.\nForexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor\nentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\nonentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\ne t a l .,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis 2012",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "putinthemachinerythatcombinestheminordertomodeltheirjointdistribution.\nApracticalshort-termapplicationofsuchmodelsislinkprediction:predict-\ningmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew\nfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave\nbeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\nthemajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l .\n(),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "application.\nEvaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcult\nbecausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\nbetrue). Ifthemodelproposesafactthatisnotinthedataset,weareunsure\nwhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\nfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\nmodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts\nthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "thatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\nfactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity\nintherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat\n10%metriccountshowmanytimesthemodelranksa“correct”factamongthe\ntop10%ofallcorruptedversionsofthatfact.\n4 8 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nAnotherapplicationofknowledgebasesanddistributedrepresentationsfor\nthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,\n2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate\none,insomecontext.\nEventually,knowledgeofrelationscombinedwithareasoningprocessand\nunderstandingofnaturallanguagecouldallowustobuildageneralquestion\nansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocess",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "inputinformationandrememberimportantfacts,organizedinawaythatenables\nittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblem\nwhichcanonlybesolvedinrestricted“toy”environments.Currently,thebest\napproachtorememberingandretrievingspeciﬁcdeclarativefactsistousean\nexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12\nﬁrstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,).\ne t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "theinputintothememoryandtoproducetheanswergiventhecontentsofthe\nmemory.\nDeeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\ndescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\nbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\nofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\nasofthiswriting.\nThisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "networks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these\nmethodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofa\nmodelthatapproximates somedesiredfunction.Withenoughtrainingdata,this\napproachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\nterritoryofresearch—methodsthataredesignedtoworkwithlesstrainingdata\nortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcult\nandnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\n4 8 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "P a rt I\nAppliedMathandMachine\nLearningBasics\n29",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o\nunders t an d deep learning. W e b e gin with general ideas f r om applied math t hat\nallo w us t o deﬁne f unctions of many v ariables , ﬁ nd t he highes t and low e s t p oints\non t hes e f unctions and q uantify degrees of b e lief.\nN e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "t o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,\ndes igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with\nr e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction.\nThis e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\nalgorithms , including approac hes t o machine learning t hat are not deep. In t he",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "s ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his\nf r amew ork.\n3 0",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 4\nA u t o e n co d e rs\nAn aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput\ntoitsoutput. Internally ,ithasahiddenlayer hthatdescribesa c o deusedto\nrepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an\nencoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h).\nThisarchitectureispresentedinﬁgure.Ifanautoencodersucceedsinsimply 14.1\nlearningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead,",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare\nrestrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly\ninputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize\nwhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe\ndata.\nModern autoencoders havegeneralized the idea of anencoder and ade-\ncoderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and\npdecoder( ) x h|.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "pdecoder( ) x h|.\nTheideaofautoencodershasbeenpartofthehistoricallandscapeofneural\nnetworksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel\n1994).Traditionally, autoencoderswereused fordimensionalityreductionor\nfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersand\nlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative\nmodeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20\naspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "techniques,typicallyminibatchgradientdescentfollowinggradientscomputed\nbyback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay\nalsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning\nalgorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput\n502",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\ntotheactivationsonthereconstructedinput.Recirculationisregardedasmore\nbiologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning\napplications.\nxx rrh h\nf g\nFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x\n(calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder\nhastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto\nr).\n14.1UndercompleteAutoencoders",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "r).\n14.1UndercompleteAutoencoders\nCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynot\ninterestedintheoutputofthe decoder. Instead, wehope thattrainingthe\nautoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful\nproperties.\nOnewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto\nhavesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless\nthantheinputdimensioniscalled under c o m p l e t e.Learninganundercomplete",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "representationforcestheautoencodertocapturethemostsalientfeaturesofthe\ntrainingdata.\nThelearningprocessisdescribedsimplyasminimizingalossfunction\nL , g f ( x(())) x (14.1)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthemeansquarederror.\nWhenthedecoderislinearand Listhemeansquarederror,anundercomplete\nautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder\ntrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe\ntrainingdataasaside-eﬀect.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "trainingdataasaside-eﬀect.\nAutoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc-\ntions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu-\n5 0 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder\ncanlearntoperformthecopyingtaskwithoutextractingusefulinformationabout\nthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder\nwithaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto\nrepresenteachtrainingexample x() iwiththecode i.Thedecodercouldlearnto\nmaptheseintegerindicesbacktothevaluesofspeciﬁctrainingexamples.This",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "speciﬁcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-\ncodertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout\nthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.\n14.2RegularizedAutoencoders\nUndercomplete autoencoders,withcodedimensionlessthantheinputdimension,\ncanlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat\ntheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare\ngiventoomuchcapacity.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "giventoomuchcapacity.\nAsimilarproblemoccursifthehiddencodeisallowedtohavedimension\nequaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas\ndimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear\ndecodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful\naboutthedatadistribution.\nIdeally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing\nthecodedimensionandthecapacityoftheencoderanddecoderbasedonthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "complexityofdistributiontobemodeled.Regularizedautoencodersprovidethe\nabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder\nanddecodershallowandthecodesizesmall,regularizedautoencodersusealoss\nfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheability\ntocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe\nrepresentation,smallnessofthederivativeoftherepresentation,androbustness\ntonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "overcompletebutstilllearnsomethingusefulaboutthedatadistributionevenif\nthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.\nInadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted\nasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables\nandequippedwithaninferenceprocedure(forcomputinglatentrepresentations\ngiveninput)maybeviewedasaparticularformofautoencoder.Twogenerative\nmodelingapproachesthatemphasizethisconnectionwithautoencodersarethe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "descendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b\n5 0 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nautoencoder(section)andthegenerativestochasticnetworks(section). 20.10.3 20.12\nThesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput\nanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings\narenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize\ntheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.\n1 4 . 2 . 1 S p a rse A u t o en co d ers\nAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "sparsitypenaltyΩ( h)onthecodelayer h,inadditiontothereconstructionerror:\nL , g f ( x(()))+Ω() x h (14.2)\nwhere g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder\noutput.\nSparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch\nasclassiﬁcation.Anautoencoderthathasbeenregularizedtobesparsemust\nrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather\nthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "copyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful\nfeaturesasabyproduct.\nWecanthink ofthepenalty Ω( h)simplyasaregularizertermaddedto\nafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput\n(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask\n(with asupervised learning ob jective) thatdepends on thesesparsefeatures.\nUnlikeotherregularizerssuchasweightdecay,thereisnotastraightforward",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "Bayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1\nwithweightdecayandotherregularizationpenaltiescanbeinterpretedasa\nMAPapproximationtoBayesianinference,withtheaddedregularizingpenalty\ncorrespondingtoapriorprobabilitydistributionoverthemodelparameters.In\nthisview,regularizedmaximumlikelihoodcorrespondstomaximizing p( θ x|),\nwhichisequivalenttomaximizing log p( x θ|)+log p( θ). The log p( x θ|)term\nistheusualdatalog-likelihoodtermandthelog p( θ)term,thelog-priorover",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "parameters,incorporatesthepreferenceoverparticularvaluesof θ.Thisviewwas\ndescribedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6\nbecausetheregularizerdependsonthedataandisthereforebydeﬁnitionnota\npriorintheformalsenseoftheword.Wecanstillthinkoftheseregularization\ntermsasimplicitlyexpressingapreferenceoverfunctions.\nRatherthanthinkingofthesparsitypenaltyasaregularizerforthecopying\ntask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating\n5 0 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nmaximumlikelihood trainingofagenerativemodel thathaslatentvariables.\nSupposewehaveamodelwithvisiblevariables xandlatentvariables h,with\nanexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto\npmodel( h)asthemodel’spriordistributionoverthelatentvariables,representing\nthemodel’sbeliefspriortoseeing x.Thisisdiﬀerentfromthewaywehave\npreviouslyusedtheword“prior,”torefertothedistribution p( θ)encodingour",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "beliefsaboutthemodel’sparametersbeforewehaveseenthetrainingdata.The\nlog-likelihoodcanbedecomposedas\nlog pmodel() = log x\nhpmodel( ) h x , . (14.3)\nWecanthinkoftheautoencoderasapproximatingthissumwithapointestimate\nforjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative\nmodel(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather\nthantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof\nview,withthischosen,wearemaximizing h",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "view,withthischosen,wearemaximizing h\nlog pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4)\nThelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior,\npmodel( h i) =λ\n2e−| λ h i|, (14.5)\ncorrespondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan\nabsolutevaluepenalty,weobtain\nΩ() = h λ\ni| h i| (14.6)\n−log pmodel() = h\ni\nλ h| i|−logλ\n2\n= Ω()+const h (14.7)\nwheretheconstanttermdependsonlyon λandnot h.Wetypicallytreat λasa",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "hyperparameteranddiscardtheconstanttermsinceitdoesnotaﬀecttheparameter\nlearning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From\nthispointofviewofsparsityasresultingfromtheeﬀectof pmodel( h)onapproximate\nmaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat\nall. Itisjustaconsequenceofthemodel’sdistributionoveritslatentvariables.\nThisviewprovidesadiﬀerentmotivationfortraininganautoencoder:itisaway",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "ofapproximately trainingagenerativemodel.Italsoprovidesadiﬀerentreasonfor\n5 0 6",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent\nvariablesthatexplaintheinput.\nEarlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008\nvariousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty\nandthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected\nprobabilisticmodel p( x) =1\nZ˜ p( x).Theideaisthatminimizing log Zpreventsa\nprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "on anautoencoder preventstheautoencoderfrom having lowreconstruction\nerroreverywhere.Inthiscase, theconnectionisonthelevelofanintuitive\nunderstandingofageneralmechanismratherthanamathematical correspondence.\nTheinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina\ndirectedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward.\nOnewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "wasintroducedin ().Theideaistouserectiﬁedlinearunitsto Glorot e t a l .2011b\nproducethecodelayer.Withapriorthatactuallypushestherepresentationsto\nzero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage\nnumberofzerosintherepresentation.\n1 4 . 2 . 2 D en o i s i n g A u t o en co d ers\nRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder Ω \nthatlearnssomethingusefulbychangingthereconstructionerrortermofthecost\nfunction.\nTraditionally,autoencodersminimizesomefunction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Traditionally,autoencodersminimizesomefunction\nL , g f ( x(())) x (14.8)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthe L2normoftheirdiﬀerence. This encourages g f◦tolearntobemerelyan\nidentityfunctioniftheyhavethecapacitytodoso.\nA orDAEinsteadminimizes denoising aut o e nc o der\nL , g f ( x((˜ x))) , (14.9)\nwhere ˜ xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising\nautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir\ninput.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "input.\nDenoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x),\nasshown by  () and ().Denoising AlainandBengio2013Bengio  e t a l .2013c\n5 0 7",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge\nasabyproductofminimizingreconstructionerror.Theyarealsoanexampleof\nhowovercomplete,high-capacity modelsmaybeusedasautoencoderssolong\nascareistakentopreventthemfromlearningtheidentityfunction. Denoising\nautoencodersarepresentedinmoredetailinsection.14.5\n1 4 . 2 . 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es\nAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse Ω",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "autoencoders,\nL , g f , , ( x(()))+Ω( x h x) (14.10)\nbutwithadiﬀerentformof:Ω\nΩ( ) = h x , λ\ni||∇ x h i||2. (14.11)\nThisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x\nchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces\ntheautoencodertolearnfeaturesthatcaptureinformationaboutthetraining\ndistribution.\nAnautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der\norCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "manifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail\ninsection.14.7\n14.3RepresentationalPower,LayerSizeandDepth\nAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer\ndecoder.However,thisisnotarequirement.Infact,usingdeepencodersand\ndecodersoﬀersmanyadvantages.\nRecallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1\nwardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "alsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork\nasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually\nbeneﬁtfromdepth.\nOnemajoradvantageofnon-trivialdepthisthattheuniversalapproximator\ntheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden\nlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan\n5 0 8",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\narbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans\nthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity\nfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrom\ninputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary\nconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat\nleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any\nmappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "Depthcanexponentiallyreducethecomputational costofrepresentingsome\nfunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata\nneededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1\ndepthinfeedforwardnetworks.\nExperimentally,deepautoencodersyieldmuchbettercompressionthancorre-\nspondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).\nAcommonstrategyfortrainingadeepautoencoderistogreedilypretrain\nthedeeparchitecturebytrainingastackofshallowautoencoders,soweoften",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "encountershallowautoencoders,evenwhentheultimategoalistotrainadeep\nautoencoder.\n14.4StochasticEncodersandDecoders\nAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput\nunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor\nautoencoders.\nAsdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4\nandthelossfunctionofafeedforwardnetworkistodeﬁneanoutputdistribution\np( y x|)andminimizethenegativelog-likelihood−log p( y x|).Inthatsetting, y",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "wasavectoroftargets,suchasclasslabels.\nInthecaseofanautoencoder, xisnowthetargetaswellastheinput.However,\nwecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay\nthinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|). We\nmaythentraintheautoencoderbyminimizing −log pdecoder( ) x h|.Theexact\nformofthislossfunctionwillchangedependingontheformof pdecoder.Aswith\ntraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "themeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative\nlog-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues\ncorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid\noutputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon.\n5 0 9",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nTypically,theoutputvariablesaretreatedasbeingconditionallyindependent\ngiven hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome\ntechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs\nwithcorrelations.\nxx rrh h\np e n c o d e r ( ) h x| p d e c o d e r ( ) x h|\nFigure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe\ndecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "theiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder\nand pdecoder( ) x h|forthedecoder.\nTomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen\npreviously,wecanalsogeneralizethenotionofan e nc o di ng f unc t i o n f( x)to\nan e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedinﬁgure.14.2\nAnylatentvariablemodel pmodel( ) h x ,deﬁnesastochasticencoder\npencoder( ) = h x| pmodel( ) h x| (14.12)\nandastochasticdecoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "andastochasticdecoder\npdecoder( ) = x h| pmodel( ) x h| . (14.13)\nIngeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional\ndistributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l .\n()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015\nwilltendtomakethemcompatibleasymptotically(withenoughcapacityand\nexamples).\n14.5DenoisingAutoencoders\nThe denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "datapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint\nasitsoutput.\nTheDAEtrainingprocedureisillustratedinﬁgure.Weintroducea 14.3\ncorruptionprocess C(˜x x|)whichrepresentsaconditional distrib utionover\n5 1 0",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n˜ x ˜ x L Lh h\nfg\nxxC ( ˜ x x| )\nFigure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,\nwhichistrainedtoreconstructthecleandatapoint xfromitscorruptedversion˜ x.\nThisisaccomplishedbyminimizingtheloss L=−log pdecoder( x h|= f(˜ x)),where\n˜ xisacorruptedversionofthedataexample x,obtainedthroughagivencorruption\nprocess C(˜ x x|).Typicallythedistribution pdecoderisafactorialdistributionwhosemean\nparametersareemittedbyafeedforwardnetwork. g",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "parametersareemittedbyafeedforwardnetwork. g\ncorruptedsamples ˜ x,givenadatasample x.Theautoencoderthenlearnsa\nr e c o nst r u c t i o n di st r i but i o n preconstruct( x|˜ x)estimatedfromtrainingpairs\n( x ,˜ x),asfollows:\n1. Sampleatrainingexamplefromthetrainingdata. x\n2. Sampleacorruptedversion˜ xfrom C(˜ x x|= ) x.\n3.Use( x ,˜ x)asatrainingexampleforestimatingtheautoencoderreconstruction\ndistribution preconstruct( x|˜x) = pdecoder( x h|)with htheoutputofencoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "f(˜ x)and pdecodertypicallydeﬁnedbyadecoder. g() h\nTypicallywecansimplyperformgradient-basedapproximate minimization (such\nasminibatchgradientdescent)onthenegativelog-likelihood−log pdecoder( x h|).\nSolongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward\nnetwork andmay be trainedwith exactlythesame techniques as anyother\nfeedforwardnetwork.\nWecanthereforeviewtheDAEasperformingstochasticgradientdescenton\nthefollowingexpectation:",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "thefollowingexpectation:\n− E x∼ˆ p d a t a() x E˜ x∼ C(˜x| x)log pdecoder( = ( x h| f˜ x))(14.14)\nwhere ˆ pdata() xisthetrainingdistribution.\n5 1 1",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nx˜ x\ng f◦\n˜ x\nC ( ˜ x x| )\nx\nFigure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint˜xbackto\ntheoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara\nlow-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption\nprocess C(˜x x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates\nhowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors\n|| g( f(˜ x))−|| x2,thereconstruction g( f(˜ x)) estimates E x ,˜ x∼ p dat a()( x C˜x x|)[ x|˜ x].Thevector\ng( f(˜x))−˜ xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(˜x))\nestimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenriseto˜ x.The\nautoencoderthuslearnsavectorﬁeld g( f( x))− xindicatedbythegreenarrows.This\nvectorﬁeldestimatesthescore∇ xlog pdata( x)uptoamultiplicativefactorthatisthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "averagerootmeansquarereconstructionerror.\n5 1 2",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n1 4 . 5 . 1 E s t i m a t i n g t h e S co re\nScorematching(,)isanalternativetomaximumlikelihood.It Hyvärinen2005\nprovidesaconsistentestimatorofprobabilitydistributionsbasedonencouraging\nthemodeltohavethesame sc o r easthedatadistributionateverytrainingpoint\nx.Inthiscontext,thescoreisaparticulargradientﬁeld:\n∇ xlog() p x . (14.15)\nScorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4\nregardingautoencoders,itissuﬃcienttounderstandthatlearningthegradient",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "ﬁeldoflog pdataisonewaytolearnthestructureof pdataitself.\nAveryimportantpropertyofDAEsisthat theirtrainingcriterion(with\nconditionallyGaussian p( x h|))makes theautoencoder learnavectorﬁeld\n( g( f( x))− x)thatestimatesthescoreofthedatadistribution.Thisisillustrated\ninﬁgure.14.4\nDenoisingtrainingofaspeciﬁckindofautoencoder(sigmoidalhiddenunits,\nlinear reconstr uction units) usingGaussiannoiseand meansquared erroras\nthereconstructioncostisequivalent(,)totrainingaspeciﬁckind Vincent2011",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "ofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.\nThiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1\ndiscussionitsuﬃcestoknowthatitisamodelthatprovidesanexplicit pmodel( x; θ).\nWhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun\n2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding\nautoencoder.Withaﬁxednoiselevel,regularizedscorematchingisnotaconsistent",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "estimator;itinsteadrecoversablurredversionofthedistribution.However,if\nthenoiselevelischosentoapproach0whenthenumberofexamplesapproaches\ninﬁnity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin\nmoredetailinsection.18.5\nOtherconnectionsbetweenautoencodersandRBMsexist.Scorematching\nappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror\ncombinedwitharegularizationtermsimilartothecontractivepenaltyofthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "CAE(Swersky2011BengioandDelalleau2009 e t a l .,). ()showedthatanautoen-\ncodergradientprovidesanapproximationtocontrastivedivergencetrainingof\nRBMs.\nForcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand\nreconstructiondistributionyieldsanestimatorofthescorethatisapplicableto\ngeneralencoderanddecoderparametrizations ( ,).This AlainandBengio2013\nmeansagenericencoder-decoderarchitecturemaybemadetoestimatethescore\n5 1 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nbytrainingwiththesquarederrorcriterion\n|| g f((˜ x x))−||2(14.16)\nandcorruption\nC(˜ x=˜x x|) = (N˜ x x ;= µ , σΣ = 2I) (14.17)\nwithnoisevariance σ2.Seeﬁgureforanillustrationofhowthisworks. 14.5\nFigure14.5:Vectorﬁeldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold\nnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe\nreconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "accordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorﬁeldhaszeros\natbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima\nofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof\nlocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof\nthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength\nofthearrows)islarge,itmeansthatprobabilitycanbesigniﬁcantlyincreasedbymoving",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "inthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.\nTheautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.\nWhereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore\naccurate.Figurereproducedwithpermissionfrom (). AlainandBengio2013\nIngeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe\ninput xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis\n5 1 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nwhytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011\nwhere g( f( x))− xmaybeobtainedbytakingthederivativeofanotherfunction.\nKamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by\nidentifyingafamilyofshallowautoencoderssuchthat g( f( x))− xcorrespondsto\nascoreforallmembersofthefamily.\nSofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent\naprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "agenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed\nlater,insection.20.11\n1 4 . 5 . 1 . 1 Hi st o r i c a l P e r spec t i v e\nTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987\nand ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001\nimages.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.\nHowever,thename“denoisingautoencoder”referstoamodelthatisintendednot\nmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "as asideeﬀect oflearningto denoise.This ideacame muchlater (Vincent\ne t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010\ndeeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,\nsparsecoding,contractiveautoencodersandotherregularizedautoencoders,the\nmotivationforDAEswastoallowthelearningofaveryhigh-capacity encoder\nwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.\nPriortotheintroduction ofthemodernDAE,InayoshiandKurita2005()",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "exploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach\nminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting\nnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove\ngeneralization byintroducing the reconstructionerror andtheinjectednoise.\nHowever,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction\nfamiliesaspowerfulascanthemodernDAE.\n14.6LearningManifoldswithAutoencoders\nLike many other machine learning algorithms, auto encoders exploittheidea",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "thatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch\nmanifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3\nthisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe\nmanifoldbutmayhaveunusualbehaviorifgivenaninputthatisoﬀthemanifold.\n5 1 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.\nTounderstandhowautoencodersdothis,wemustpresentsomeimportant\ncharacteristicsofmanifolds.\nAnimportantcharacterization ofamanifoldisthesetofits t angen t pl anes.\nAtapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis\nvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As\nillustratedinﬁgure,theselocaldirectionsspecifyhowonecanchange 14.6 x",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "inﬁnitesimallywhilestayingonthemanifold.\nAllautoencodertrainingproceduresinvolveacompromisebetweentwoforces:\n1.Learningarepresentation hofatrainingexample xsuchthat xcanbe\napproximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn\nfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed\nnotsuccessfullyreconstructinputsthatarenotprobableunderthedata\ngeneratingdistribution.\n2. Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe\naregularizationtermaddedtothereconstructioncost.Thesetechniques\ngenerallyprefersolutionsthatarelesssensitivetotheinput.\nClearly,neitherforcealonewouldbeuseful—copyingtheinputtotheoutput\nisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether\nareusefulbecausetheyforcethehiddenrepresentationtocaptureinformation\naboutthestructureofthedatageneratingdistribution.Theimportantprinciple",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "isthattheautoencodercanaﬀordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d\nt o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates\nnearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture\nalocalcoordinatesystemforthismanifold:onlythevariationstangenttothe\nmanifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder\nlearnsamappingfromtheinputspace xtoarepresentationspace,amappingthat",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "isonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto\nchangesorthogonaltothemanifold.\nAone-dimensional exampleisillustratedinﬁgure,showingthat,bymaking 14.7\nthereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe\ndatapoints,wecausetheautoencodertorecoverthemanifoldstructure.\nTounderstandwhyautoencodersareusefulformanifoldlearning,itisin-\nstructivetocomparethemtootherapproaches.Whatismostcommonlylearned",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "tocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear)\n5 1 6",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.6: Anillustrationoftheconceptofatangenthyperplane.Herewecreatea\none-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784\npixelsandtransformitbytranslatingitvertically. Theamountofverticaltranslation\ndeﬁnesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath\nthroughimagespace.Thisplotshowsafewpointsalongthismanifold. Forvisualization,\nwehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "manifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches\nthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.\nItdeﬁnesthespaceofdirectionsinwhichitispossibletomovewhileremainingon\nthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean\nexampletangentlineatonepoint,withanimageshowinghowthistangentdirection\nappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "thetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels\nthatdarken.\n5 1 7",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nx 0 x 1 x 2\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y\nO p t i m a l r e c o n s t r u c t i o n\nFigure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall\nperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here\nthemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0\nlineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "functioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal\narrowsatthebottomoftheplotindicatethe r( x)− xreconstructiondirectionvector\natthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest“manifold”\n(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake\nthederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The\ncontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "askedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The\nspacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where\nthereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints\nbackontothemanifold.\nthemanifold.Sucharepresentationforaparticularexampleisalsocalledits\nembedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions\nthanthe“ambient”spaceofwhichthemanifoldisalow-dimensionalsubset.Some",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "algorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly\nlearnanembeddingforeachtrainingexample,whileotherslearnamoregeneral\nmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany\npointintheambientspace(theinputspace)toitsembedding.\nManifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat\nattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch\nonlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "onthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample\nandedgesconnectingnearneighborstoeachother.Thesemethods(Schölkopf\ne t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,;\n5 1 8",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph\ninwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor\nrelationships. Variousprocedurescanthusobtainthetangentplaneassociatedwitha\nneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining\nexamplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize\nsucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "ofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these\napproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l .\n2000).\nandNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;\nandRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha\ntangentplanethatspansthedirectionsofvariationsassociatedwiththediﬀerence\nvectorsbetweentheexampleanditsneighbors,asillustratedinﬁgure.14.8\nAglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "solvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9\nlargenumberoflocallylinearGaussian-likepatches(or“pancakes,”becausethe\nGaussiansareﬂatinthetangentdirections).\nHowever,thereisafundamentaldiﬃcultywithsuchlocalnon-parametric\napproachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005\nmanifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),\nonemayneedaverylargenumberoftrainingexamplestocovereachoneof\n5 1 9",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.9:Ifthetangentplanes(seeﬁgure)ateachlocationareknown,thenthey 14.6\ncanbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch\ncanbethoughtofasalocalEuclideancoordinatesystemorasalocallyﬂatGaussian,or\n“pancake,”withaverysmallvarianceinthedirectionsorthogonaltothepancakeanda\nverylargevarianceinthedirectionsdeﬁningthecoordinatesystemonthepancake.A\nmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "Parzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003\nvariant( ,). Bengio e t a l .2006c\nthesevariations,withnochancetogeneralizetounseenvariations.Indeed,these\nmethodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between\nneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan\nhaveverycomplicatedstructurethatcanbediﬃculttocapturefromonlylocal\ninterpolation.Considerforexamplethemanifoldresultingfromtranslationshown",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "inﬁgure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe\nimageistranslated,wewillobservethatonecoordinateencountersapeakora\ntroughinitsvalueonceforeverypeakortroughinbrightnessintheimage. In\notherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage\ntemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming\nsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentations\nanddeeplearningforcapturingmanifoldstructure.\n5 2 0",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n14.7ContractiveAutoencoders\nThecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab\nonthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible:\nΩ() = h λ∂ f() x\n∂ x2\nF. (14.18)\nThepenaltyΩ( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe\nJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.\nThereisaconnectionbetweenthedenoisingautoencoderandthecontractive",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "autoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013\ninput noise, the denoising reconstruction erroris equivalent toacontractive\npenaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother\nwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbut\nﬁnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe\nfeatureextractionfunctionresistinﬁnitesimalperturbationsoftheinput.When\nusingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "withaclassiﬁer,thebestclassiﬁcationaccuracyusuallyresultsfromapplyingthe\ncontractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x)\nalsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1\nThename c o n t r ac t i v earisesfromthewaythattheCAEwarpsspace.Speciﬁ-\ncally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged\ntomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.\nWecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "neighborhood.\nToclarify,theCAEiscontractiveonlylocally—allperturbationsofatraining\npoint xaremappednearto f( x).Globally,twodiﬀerentpoints xand xmaybe\nmappedto f( x)and f( x)pointsthatarefartherapartthantheoriginalpoints.\nItisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see\nforexamplewhathappensinthe1-Dtoyexampleofﬁgure).Whenthe 14.7 Ω( h)\npenaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto\nmakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa\nbinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout\nmostofthehypercubethatitssigmoidalhiddenunitscanspan.\nWecanthinkoftheJacobianmatrix Jatapoint xasapproximating the\nnonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword\n“contractive”moreformally. Inthetheoryoflinearoperators,alinearoperator\n5 2 1",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nissaidtobecontractiveifthenormof J xremainslessthanorequaltofor1\nallunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere.\nWecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear\napproximationof f( x)ateverytrainingpoint xinordertoencourageeachof\ntheselocallinearoperatortobecomeacontraction.\nAsdescribed insection, regularized autoencoderslearnmanifoldsby 14.6\nbalancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "reconstructionerrorandthecontractivepenaltyΩ( h).Reconstructionerroralone\nwouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty\nalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x.\nThecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives\n∂ f() x\n∂ xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa\nsmallnumberofdirectionsintheinput,mayhavesigniﬁcantderivatives.\nThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "xwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich\napproximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a\nand ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b\nof Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1\nsomesingularvaluesremainabove,becausethereconstructionerrorpenalty 1\nencouragestheCAEtoencodethedirectionswiththemostlocalvariance.The",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "directionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent\ndirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent\ndirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE\nappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas\nobjectsintheimagegraduallychangepose,asshowninﬁgure.Visualizations 14.6\noftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful\ntransformationsoftheinputimage,asshowninﬁgure.14.10",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit\nischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes\nmuchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby\nRifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each\ntrainedtoreconstructthepreviousautoencoder’shiddenlayer.Thecomposition\noftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas\nseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "aswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining\ntheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit\ncapturesmanyofthedesirablequalitativecharacteristics.\nAnotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults\n5 2 2",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nInput\npointTangentvectors\nLocalPCA(nosharingacrossregions)\nContractiveautoencoder\nFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA\nandbyacontractiveautoencoder.Thelocationonthemanifoldisdeﬁnedbytheinput\nimageofadogdrawnfromtheCIFAR-10dataset. Thetangentvectorsareestimated\nbytheleadingsingularvectorsoftheJacobianmatrix∂ h\n∂ xoftheinput-to-codemapping.\nAlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "formmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter\nsharingacrossdiﬀerentlocationsthatshareasubsetofactivehiddenunits. TheCAE\ntangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas\ntheheadorlegs).Imagesreproducedwithpermissionfrom (). Rifai e t a l .2011c\nifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder\ncouldconsistofmultiplyingtheinputbyasmallconstant andthedecoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "couldconsistofdividingthecodeby .As approaches,theencoderdrivesthe 0\ncontractivepenaltyΩ( h)toapproachwithouthavinglearnedanythingaboutthe 0\ndistribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai\ne t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare\nstandardneuralnetworklayersconsistingofanaﬃnetransformationfollowedby\nanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g\ntobethetransposeoftheweightmatrixof. f",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "tobethetransposeoftheweightmatrixof. f\n14.8PredictiveSparseDecomposition\nP r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse\ncodingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric\nencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen\nappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo\n(Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "asforaudio( ,).Themodelconsistsofanencoder Henaﬀ e t a l .2011 f( x)anda\ndecoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe\n5 2 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\noptimization algorithm.Trainingproceedsbyminimizing\n||− || x g() h2+ λ|| h1+ () γ f ||− h x||2. (14.19)\nLikeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with\nrespectto handminimization withrespecttothemodelparameters.Minimization\nwithrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe\ncostfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent\ncanobtainreasonablevaluesofinasfewastensteps. h",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "canobtainreasonablevaluesofinasfewastensteps. h\nThetrainingprocedureusedbyPSDisdiﬀerentfromﬁrsttrainingasparse\ncodingmodelandthentraining f( x)topredictthevaluesofthesparsecoding\nfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparameters\nforwhichcaninfergoodcodevalues. f() x\nPredictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e.\nInsection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding\nprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe\nmodel.\nInpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring\ntraining.Theparametricencoder fisusedtocomputethelearnedfeatureswhen\nthemodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto\ninferring hviagradientdescent.Because fisadiﬀerentiableparametricfunction,\nPSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "withanothercriterion.\n14.9ApplicationsofAutoencoders\nAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-\nmationretrievaltasks.Dimensionalityreductionwasoneoftheﬁrstapplications\nofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations\nforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained\nastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder\nwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "resultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand\nthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe\nunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.\nLower-dimensionalrepresentationscanimproveperformanceonmanytasks,\nsuchasclassiﬁcation.Modelsofsmallerspacesconsumelessmemoryandruntime.\n5 2 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nManyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear\neachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l .\n().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008\ngeneralization.\nOnetaskthatbeneﬁtsevenmorethanusualfromdimensionalityreductionis\ni nf o r m at i o n r e t r i e v al,thetaskofﬁndingentriesinadatabasethatresemblea\nqueryentry. Thistaskderivestheusualbeneﬁtsfromdimensionalityreduction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "thatothertasksdo,butalsoderivestheadditionalbeneﬁtthatsearchcanbecome\nextremelyeﬃcientincertainkindsoflowdimensionalspaces.Speciﬁcally, if\nwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-\ndimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary\nmappingbinarycodevectorstoentries.Thishashtableallowsustoperform\ninformationretrievalbyreturningalldatabaseentriesthathavethesamebinary\ncodeasthe query.Wecanalso search overslightlylesssimilar entries very",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "eﬃciently,justbyﬂippingindividualbitsfromtheencodingofthequery. This\napproachtoinformationretrievalviadimensionalityreductionandbinarization\niscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas\nbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and\nimages(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,).\nToproducebinarycodesforsemantichashing,onetypicallyusesanencoding\nfunctionwithsigmoidsontheﬁnallayer.Thesigmoidunitsmustbetrainedtobe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "saturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish\nthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring\ntraining.Themagnitudeofthenoiseshouldincreaseovertime.Toﬁghtthat\nnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe\nmagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.\nTheideaoflearningahashingfunctionhasbeenfurtherexploredinseveral\ndirections,includingtheideaoftrainingtherepresentationssoastooptimize",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "alossmoredirectlylinkedtothetaskofﬁndingnearbyexamplesinthehash\ntable( ,). NorouziandFleet2011\n5 2 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "N ot at i o n\nThissectionprovidesaconcisereferencedescribingthenotationusedthroughout\nthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\nconcepts,wedescribemostoftheseideasinchapters2–4.\nNum b e r s and Ar r a y s\naAscalar(integerorreal)\naAvector\nAAmatrix\nAAtensor\nI nIdentitymatrixwithrowsandcolumns n n\nIIdentitymatrixwithdimensionalityimpliedby\ncontext\ne( ) iStandardbasisvector[0 , . . . ,0 ,1 ,0 , . . . ,0]witha\n1atposition i\ndiag()aAsquare,diagonalmatrixwithdiagonalentries",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "diag()aAsquare,diagonalmatrixwithdiagonalentries\ngivenbya\naAscalarrandomvariable\naAvector-valuedrandomvariable\nAAmatrix-valuedrandomvariable\nxi",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nSet s and G r aphs\nAAset\nRThesetofrealnumbers\n{}01 ,Thesetcontaining0and1\n{ } 01 , , . . . , nThesetofallintegersbetweenand0 n\n[] a , bTherealintervalincludingand a b\n(] a , bTherealintervalexcludingbutincluding a b\nA B\\Setsubtraction,i.e., thesetcontainingtheele-\nmentsofthatarenotin A B\nGAgraph\nP a G(x i)Theparentsofx iinG\nI ndexing\na iElement iofvectora,withindexingstartingat1\na − iAllelementsofvectorexceptforelementa i\nA i , jElementofmatrix i , jA\nA i , :Rowofmatrix iA",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "A i , :Rowofmatrix iA\nA : , iColumnofmatrix iA\nA i , j , kElementofa3-Dtensor ( ) i , j , k A\nA : : , , i2-Dsliceofa3-Dtensor\na iElementoftherandomvector i a\nL i near Al g e br a O p e r at i o ns\nATransposeofmatrixA\nA+Moore-PenrosepseudoinverseofA\nABElement-wise(Hadamard)productofandAB\ndet()ADeterminantofA\nx i i",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nCal c ul usd y\nd xDerivativeofwithrespectto y x\n∂ y\n∂ xPartialderivativeofwithrespectto y x\n∇ x yGradientofwithrespectto y x\n∇ X yMatrixderivativesofwithrespectto y X\n∇ X yTensorcontainingderivativesof ywithrespectto\nX\n∂ f\n∂xJacobianmatrixJ∈ Rm n ×of f: Rn→ Rm\n∇2\nx f f f () (xorH)()xTheHessianmatrixofatinputpointx\nf d()xxDeﬁniteintegralovertheentiredomainofx\n\nSf d()xx x Deﬁniteintegralwithrespecttoovertheset S\nP r o babil i t y and I nf o r m at i o n T heor y",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "abTherandomvariablesaandbareindependent ⊥\nabcTheyareconditionallyindependentgivenc ⊥|\nP()aAprobabilitydistributionoveradiscretevariable\np()aAprobabilitydistributionoveracontinuousvari-\nable,oroveravariablewhosetypehasnotbeen\nspeciﬁed\na Randomvariableahasdistribution ∼ P P\nE x ∼ P[()] () () () f xor E f xExpectationof f xwithrespectto Px\nVar(()) f xVarianceofunderx f x() P()\nCov(()()) f x , g xCovarianceofandunderx f x() g x() P()\nH()xShannonentropyoftherandomvariablex",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "H()xShannonentropyoftherandomvariablex\nD K L( ) P QKullback-LeiblerdivergenceofPandQ\nN(; )xµ ,ΣGaussiandistributionoverxwithmeanµand\ncovarianceΣ\nx i i i",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nF unc t i o ns\nf f : A B→Thefunctionwithdomainandrange A B\nf g f g ◦Compositionofthefunctionsand\nf(;)xθAfunctionofxparametrized byθ. (Sometimes\nwewrite f(x)andomittheargumentθtolighten\nnotation)\nlog x x Naturallogarithmof\nσ x()Logisticsigmoid,1\n1+exp()− x\nζ x x () log(1+exp( Softplus, ))\n||||x p Lpnormofx\n||||x L2normofx\nx+Positivepartof,i.e., x max(0) , x\n1 c o ndi t i o nis1iftheconditionistrue,0otherwise\nSometimesweuseafunction fwhoseargumentisascalarbutapplyittoa",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "vector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f\ntothearrayelement-wise. Forexample,if C= σ( X),then C i , j , k= σ( X i , j , k)forall\nvalidvaluesof,and. i j k\nD at aset s and D i st r i but i o n s\np da t aThedatageneratingdistribution\nˆ p da t aTheempiricaldistributiondeﬁnedbythetraining\nset\nXAsetoftrainingexamples\nx( ) iThe-thexample(input)fromadataset i\ny( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-\ning\nXThe m n×matrixwithinputexamplex( ) iinrow",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "ing\nXThe m n×matrixwithinputexamplex( ) iinrow\nX i , :\nx i v",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 5\nRepresen t at i on L e ar n i n g\nInthischapter,weﬁrstdiscusswhatitmeanstolearnrepresentationsandhow\nthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss\nhowlearningalgorithmssharestatisticalstrengthacrossdiﬀerenttasks,including\nusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared\nrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransfer\nlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "representationexists.Finally,westepbackandargueaboutthereasonsforthe\nsuccessofrepresentationlearning,startingwiththetheoreticaladvantagesof\ndistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand\nendingwiththemoregeneralideaofunderlyingassumptionsaboutthedata\ngeneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.\nManyinformationprocessingtaskscanbeveryeasyorverydiﬃcultdepending\nonhowtheinformationisrepresented.Thisisageneralprincipleapplicableto",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "dailylife,computerscienceingeneral,andtomachinelearning.Forexample,it\nisstraightforwardforapersontodivide210by6usinglongdivision. Thetask\nbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman\nnumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX\nbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,\npermittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More\nconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "appropriateorinappropriate representations.Forexample,insertinganumber\nintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe\nlistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa\nred-blacktree.\nInthecontextofmachinelearning,whatmakesonerepresentationbetterthan\n526",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent\nlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice\nofthesubsequentlearningtask.\nWecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-\nformingakindofrepresentationlearning.Speciﬁcally,thelastlayerofthenetwork\nistypicallyalinearclassiﬁer,suchasasoftmaxregressionclassiﬁer.Therestof\nthenetworklearnstoprovidearepresentationtothisclassiﬁer.Trainingwitha",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "supervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but\nmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiﬁcation\ntaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput\nfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the\nlastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiﬁer\n(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould\nlearndiﬀerentpropertiesdependingonthetypeofthelastlayer.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing\nanyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation\nlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin\nsomeparticularway.Forexample,supposewewanttolearnarepresentationthat\nmakesdensityestimationeasier.Distributionswithmoreindependencesareeasier\ntomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements\noftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso\nlearnarepresentationasasideeﬀect.Regardlessofhowarepresentationwas\nobtained,itcanbeusedforanothertask.Alternatively,multipletasks(some\nsupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal\nrepresentation.\nMostrepresentationlearningproblemsfaceatradeoﬀbetweenpreservingas\nmuchinformationabouttheinputaspossibleandattainingniceproperties(such\nasindependence).",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "asindependence).\nRepresentationlearningisparticularlyinterestingbecauseitprovidesone\nwaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery\nlargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining\ndata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften\nresultsinsevereoverﬁtting.Semi-supervisedlearningoﬀersthechancetoresolve\nthisoverﬁttingproblembyalsolearningfromtheunlabeleddata.Speciﬁcally,\nwecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "representationstosolvethesupervisedlearningtask.\nHumansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo\n5 2 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman\nperformance—forexample,thebrainmayuseverylargeensemblesofclassiﬁers\norBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis\nabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways\ntoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe\nunlabeleddatacanbeusedtolearnagoodrepresentation.\n15. 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "Unsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural\nnetworks,enablingresearchersfortheﬁrsttimetotrainadeepsupervisednetwork\nwithoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We\ncallthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r -\nwi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow\narepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "theshapeoftheinputdistribution)cansometimesbeusefulforanothertask\n(supervisedlearningwiththesameinputdomain).\nGreedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-\ntationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse\ncodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris\npretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer\nandproducingasoutputanewrepresentationofthedata,whosedistribution(or",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "itsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.\nSeealgorithm foraformaldescription. 15.1\nGreedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong\nbeenusedtosidestepthediﬃcultyofjointlytrainingthelayersofadeepneuralnet\nforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron\n(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery\nthatthisgreedylearningprocedurecouldbeusedtoﬁndagoodinitialization for",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "ajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused\ntosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,;\nandSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,).\nPriortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth\nresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow\nthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "architectures,buttheunsupervisedpretrainingapproachwastheﬁrstmethodto\nsucceed.\nGreedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o -\n5 2 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nr i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one\npieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se\nbecausetheseindependentpiecesarethelayersofthenetwork.Speciﬁcally,greedy\nlayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile\nkeepingthepreviousonesﬁxed.Inparticular,thelowerlayers(whicharetrained\nﬁrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "v i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning\nalgorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe\nonlyaﬁrststepbeforeajointtrainingalgorithmisappliedto ﬁne-t uneallthe\nlayerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed\nasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout\ndecreasingtrainingerror)andaformofparameterinitialization.\nItiscommontousetheword“pretraining”torefernotonlytothepretraining",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "stageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining\nphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve\ntrainingasimpleclassiﬁerontopofthefeatureslearnedinthepretrainingphase,\noritmayinvolvesupervisedﬁne-tuningoftheentirenetworklearnedinthe\npretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor\nwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining\nschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "willobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining\nfollowthisbasicprotocol.\nGreedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization\nforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton\nandSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent\nvariables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006\nBoltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative\nmodelswillbedescribedinchapter.20",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "modelswillbedescribedinchapter.20\nAsdiscussedinsection, itisalsopossibletohavegreedylayer-wise 8.7.4\nsupervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork\niseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral\ncontexts(,). Erhanetal.2010\n1 5 . 1 . 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk?\nOnmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial\nimprovementsintesterrorforclassiﬁcationtasks.Thisobservationwasresponsible",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "fortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,\n5 2 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nAl g o r i t hm 1 5 . 1Greedylayer-wiseunsupervisedpretrainingprotocol.\nGiventhefollowing: Unsupervisedfeaturelearningalgorithm L,whichtakesa\ntrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw\ninputdataisX,withonerowperexampleandf( 1 )(X)istheoutputoftheﬁrst\nstageencoderonX.Inthecasewhereﬁne-tuningisperformed,weusealearner\nTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "ﬁne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber\nofstagesis.m\nf←Identityfunction\n˜XX= \nf o r dok,...,m = 1\nf( ) k= (L˜X)\nff←( ) k◦f\n˜X←f( ) k(˜X)\ne nd f o r\ni fﬁne-tuning t he n\nff,, ←T(XY)\ne nd i f\nRet ur nf\n2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however,\nunsupervisedpretrainingeitherdoesnotconferabeneﬁtorevencausesnoticeable\nharm. ()studiedtheeﬀectofpretrainingonmachinelearning Maetal.2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas\nslightlyharmful,butformanytaskswassigniﬁcantlyhelpful.Becauseunsupervised\npretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand\nwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular\ntask.\nAttheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted\ntogreedyunsupervisedpretraininginparticular.Thereareother,completely",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,\nsuchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13\ntrainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.\nExamplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle\nandBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015\nobjectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly\nusingtheinput).",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "usingtheinput).\nUnsupervisedpretrainingcombinestwodiﬀerentideas.First,itmakesuseof\n5 3 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ntheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave\nasigniﬁcantregularizingeﬀectonthemodel(and,toalesserextent,thatitcan\nimproveoptimization). Second,itmakesuseofthemoregeneralideathatlearning\nabouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto\noutputs.\nBothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral\npartsofthemachinelearningalgorithmthatarenotentirelyunderstood.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "Theﬁrstidea,thatthechoiceofinitialparametersforadeepneuralnetwork\ncanhaveastrongregularizingeﬀectonitsperformance, istheleastwellunderstood.\nAtthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe\nmodelinalocationthatwouldcauseittoapproachonelocalminimumratherthan\nanother. Today,localminimaarenolongerconsideredtobeaseriousproblem\nforneuralnetworkoptimization. Wenowknowthatourstandardneuralnetwork\ntrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "possiblethatpretraininginitializesthemodelinalocationthatwouldotherwise\nbeinaccessible—forexample,aregionthatissurroundedbyareaswherethecost\nfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly\naverynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe\nHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse\nverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe\npretrainedparametersareretainedduringthesupervisedtrainingstageislimited.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised\nlearningandsupervisedlearningratherthantwosequentialstages.Onemay\nalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe\nsupervisedlearningstagepreservesinformationfromtheunsupervisedlearning\nstagebysimplyfreezingthe parameters for thefeature extractorsand using\nsupervisedlearningonlytoaddaclassiﬁerontopofthelearnedfeatures.\nTheotheridea,thatalearningalgorithmcanuseinformationlearnedinthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter\nunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\ntaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain\nagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout\nwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,\ntherepresentationofthewheelswilltakeonaformthatiseasyforthesupervised\nlearnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "soitisnotalwayspossibletopredictwhichtaskswillbeneﬁtfromunsupervised\nlearninginthisway.Manyaspectsofthisapproacharehighlydependenton\nthespeciﬁcmodelsused.Forexample,ifwewishtoaddalinearclassiﬁeron\n5 3 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ntopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly\nseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This\nisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe\npreferable—theconstraintsimposedbytheoutputlayerarenaturallyincluded\nfromthestart.\nFromthepointofviewofunsupervisedpretrainingaslearningarepresentation,\nwecanexpectunsupervisedpretrainingtobemoreeﬀectivewhentheinitial",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "representationispoor. Onekeyexampleofthisistheuseofwordembeddings.\nWordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo\ndistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance\nof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2\ndistancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially\nusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps\nbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealow",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "qualitysimilaritymetric.\nFromthepointofviewofunsupervisedpretrainingasaregularizer,wecan\nexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled\nexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervised\npretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining\ntoperformbest whenthe number ofunlabeled examples is very large.The\nadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany\nunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "2011withunsupervisedpretrainingwinningtwointernationaltransferlearning\ncompetitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011\nnumberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens\nofexamplesperclass).Theseeﬀectswerealsodocumentedincarefullycontrolled\nexperimentsbyPaine2014etal.().\nOtherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining\nislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "Unsupervisedlearningdiﬀersfromregularizerslikeweightdecaybecauseitdoesnot\nbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering\nfeaturefunctionsthatareusefulfortheunsupervisedlearningtask. Ifthetrue\nunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput\ndistribution,unsupervisedlearningcanbeamoreappropriateregularizer.\nThesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised\npretrainingisknowntocauseanimprovement,andexplainwhatisknownabout",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "whythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused\ntoimproveclassiﬁers,andisusuallymostinterestingfromthepointofviewof\n5 3 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n\u0000    \u0000    \u0000    \u0000                    \u0000   \u0000   \u0000          \n               \n                  \nFigure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiﬀerent\nneuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one\nmappingsfromparametervectorstofunctions),withdiﬀerentrandominitializations",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "andwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiﬀerent\nneuralnetwork,ataparticulartimeduringitstrainingprocess.Thisﬁgureisadapted\nwithpermissionfrom ().Acoordinateinfunctionspaceisaninﬁnite- Erhan e t a l .2010\ndimensionalvectorassociatingeveryinputxwithanoutputy. ()made Erhan e t a l .2010\nalinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciﬁcx\npoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000\n(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions\novertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to\npointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen\nusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.\nIsomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "correspondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator\nhasreducedvariance.\n5 3 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nreducingtestseterror.However,unsupervisedpretrainingcanhelptasksother\nthanclassiﬁcation,andcanacttoimproveoptimization ratherthanbeingmerely\naregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror\nfordeepautoencoders(HintonandSalakhutdinov2006,).\nErhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof\nunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "totesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe\nparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork\ntrainingisnon-determinis tic,andconvergestoadiﬀerentfunctioneverytimeit\nisrun. Trainingmayhaltatapointwherethegradientbecomessmall,apoint\nwhereearlystoppingendstrainingtopreventoverﬁtting,oratapointwherethe\ngradientislargebutitisdiﬃculttoﬁndadownhillstepduetoproblemssuchas\nstochasticityorpoorconditioningoftheHessian. Neuralnetworksthatreceive",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,\nwhileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See\nﬁgureforavisualizationofthisphenomenon. Theregionwherepretrained 15.1\nnetworksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe\nestimationprocess,whichcaninturnreducetheriskofsevereover-ﬁtting.In\notherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto\naregionthattheydonotescape,andtheresultsfollowingthisinitialization are",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "moreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.\nErhan2010etal.()alsoprovidesomeanswersastopretrainingworks when\nbest—themeanandvarianceofthetesterrorweremostreducedbypretrainingfor\ndeepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe\ninventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks\n(rectiﬁedlinearunits,dropoutandbatchnormalization) solessisknownaboutthe\neﬀectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Animportantquestionishowunsupervisedpretrainingcanactasaregularizer.\nOnehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover\nfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.\nThisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised\npretraining,andisdescribedfurtherinsection.15.3\nComparedtootherformsofunsupervisedlearning,unsupervisedpretraining\nhasthedisadvantagethatitoperateswithtwoseparatetrainingphases. Many",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "regularizationstrategieshavetheadvantageofallowingtheusertocontrolthe\nstrengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.\nUnsupervisedpretrainingdoesnotoﬀeraclearwaytoadjustthethestrength\noftheregularization arisi ngfromtheunsupervised stage.Instead, thereare\n5 3 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nverymanyhyperparameters ,whoseeﬀectmaybemeasuredafterthefactbut\nisoftendiﬃculttopredictaheadoftime.Whenweperformunsupervisedand\nsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there\nisasinglehyperparameter,usuallyacoeﬃcientattachedtotheunsupervised\ncost, thatdetermineshowstronglytheunsupervisedobjectivewillregularize\nthesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "decreasingthiscoeﬃcient.Inthecaseofunsupervisedpretraining,thereisnota\nwayofﬂexiblyadaptingthestrengthoftheregularization—either thesupervised\nmodelisinitializedtopretrainedparameters,oritisnot.\nAnotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase\nhasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot\nbepredictedduringtheﬁrstphase,sothereisalongdelaybetweenproposing\nhyperparametersfortheﬁrstphaseandbeingabletoupdatethemusingfeedback",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "fromthesecondphase.Themostprincipledapproachistousevalidationseterror\ninthesupervisedphaseinordertoselectthehyperparameters ofthepretraining\nphase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009\nlikethenumberofpretrainingiterations,aremoreconvenientlysetduringthe\npretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis\nnotidealbutcomputationally muchcheaperthanusingthesupervisedobjective.\nToday,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "ﬁeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas\none-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled\nsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain\nonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof\nwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and\nthenusethisrepresentationorﬁne-tuneitforasupervisedtaskforwhichthe\ntrainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "bybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal.\n()andremainsincommonusetoday. 2011a\nDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout\norbatchnormalization, areabletoachievehuman-levelperformanceonverymany\ntasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-\nperformunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and\nMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "datasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform\nmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,\nthepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised\npretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch\n5 3 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nandcontinuestoinﬂuencecontemporaryapproaches.Theideaofpretraininghas\nbeengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4\ncommonapproachfortransferlearning.Supervisedpretrainingfortransferlearning\nispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional\nnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparameters\nofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "publishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,).\n15. 2 T ransfer L earni n g an d D om ai n A d ap t at i o n\nTransferlearninganddomainadaptationrefertothesituationwherewhathasbeen\nlearnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization\ninanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe\nprevioussection,wherewetransferredrepresentationsbetweenanunsupervised\nlearningtaskandasupervisedlearningtask.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "learningtaskandasupervisedlearningtask.\nIn t r ansf e r l e ar ni ng,thelearnermustperformtwoormorediﬀerenttasks,\nbutweassumethatmanyofthefactorsthatexplainthevariationsinP 1are\nrelevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically\nunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthe\ntargetmaybeofadiﬀerentnature.Forexample,wemaylearnaboutonesetof\nvisualcategories,suchascatsanddogs,intheﬁrstsetting,thenlearnabouta",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If\nthereissigniﬁcantlymoredataintheﬁrstsetting(sampledfromP 1),thenthat\nmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly\nveryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions\nofedgesandvisualshapes,theeﬀectsofgeometricchanges,changesinlighting,\netc. Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7\nadaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thatareusefulforthediﬀerentsettingsortasks,correspondingtounderlying\nfactorsthatappearinmorethanonesetting.Thisisillustratedinﬁgure,with7.2\nsharedlowerlayersandtask-dependentupperlayers.\nHowever, sometimes, whatisshared amongthe diﬀerent tasksisnotthe\nsemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech\nrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,but\ntheearlierlayersneartheinputmayneedtorecognizeverydiﬀerentversionsof",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thesamephonemesorsub-phonemicvocalizationsdependingonwhichperson\nisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers\n(neartheoutput)oftheneuralnetwork,andhaveatask-speciﬁcpreprocessing,as\n5 3 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nillustratedinﬁgure.15.2\nSe l e c t i on sw i t c h\nh(1)h(1)h(2)h(2)h(3)h(3)yy\nh(shared)h(shared)\nx(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 )\nFigure15.2: Example architectureformulti-taskortransferlearningwhentheoutput\nvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiﬀerent y x \nmeaning(andpossiblyevenadiﬀerentdimension)foreachtask(or,forexample,each\nuser),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "switch)aretask-speciﬁc,whiletheupperlevelsareshared.Thelowerlevelslearnto\ntranslatetheirtask-speciﬁcinputintoagenericsetoffeatures.\nIntherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to-\noutputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution\nisslightlydiﬀerent.Forexample,considerthetaskofsentimentanalysis,which\nconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.\nCommentspostedonthewebcomefrommanycategories.Adomainadaptation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "scenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof\nmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments\naboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine\nthatthereisanunderlyingfunctionthattellswhetheranystatementispositive,\nneutralornegative,butofcoursethevocabularyandstylemayvaryfromone\ndomaintoanother,makingitmorediﬃculttogeneralizeacrossdomains.Simple\nunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "successfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b\nArelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform\noftransferlearningduetogradualchangesinthedatadistributionovertime.\nBothconceptdriftandtransferlearningcanbeviewedasparticularformsof\n5 3 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nmulti-tasklearning.Whilethephrase“multi-tasklearning” typicallyrefersto\nsupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable\ntounsupervisedlearningandreinforcementlearningaswell.\nInallofthesecases,theobjectiveistotakeadvantageofdatafromtheﬁrst\nsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhen\ndirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "learningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe\nsamerepresentationinbothsettingsallowstherepresentationtobeneﬁtfromthe\ntrainingdatathatisavailableforbothtasks.\nAsmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound\nsuccessinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow\netal.,).Intheﬁrstofthesecompetitions,theexperimentalsetupisthe 2011\nfollowing.Eachparticipantisﬁrstgivenadatasetfromtheﬁrstsetting(from",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "distributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants\nmustusethistolearnagoodfeaturespace(mappingtherawinputtosome\nrepresentation),suchthatwhenweapplythislearnedtransformationtoinputs\nfromthetransfersetting(distributionP 2),alinearclassiﬁercanbetrainedand\ngeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults\nfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand\ndeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "intheﬁrstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond\n(transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled\nexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic\ngeneralization performance.\nTwoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r o - sho t\nl e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample\nofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "givenatallforthezero-shotlearningtask.\nOne-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation\nlearnstocleanlyseparatetheunderlyingclassesduringtheﬁrststage.Duringthe\ntransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany\npossibletestexamplesthatallclusteraroundthesamepointinrepresentation\nspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingto\ntheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "representationspace,andwehavesomehowlearnedwhichfactorsdoanddonot\nmatterwhendiscriminatingobjectsofcertaincategories.\nAsanexampleofazero-shotlearningsetting,considertheproblemofhaving\nalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.\n5 3 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nItmaybepossibletorecognizeaspeciﬁcobjectclassevenwithouthavingseenan\nimageofthatobject,ifthetextdescribestheobjectwellenough. Forexample,\nhavingreadthatacathasfourlegsandpointyears,thelearnermightbeableto\nguessthatanimageisacat,withouthavingseenacatbefore.\nZero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(\netal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "hasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario\nasincludingthreerandomvariables:thetraditionalinputsx,thetraditional\noutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.\nThemodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where\nTisadescriptionofthetaskwewishthemodeltoperform. Inourexampleof\nrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley\nwithy= 1indicating“yes”andy= 0indicating“no.”ThetaskvariableTthen",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "representsquestionstobeansweredsuchas“Isthereacatinthisimage?”Ifwe\nhaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe\nsamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.\nInourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis\nimportantthatwehavehadunlabeledtextdatacontainingsentencessuchas“cats\nhavefourlegs”or“catshavepointyears.”\nZero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "ofgeneralization. Forexample,Tcannotbejustaone-hotcodeindicatingan\nobjectcategory. ()provideinsteadadistributedrepresentation Socheretal.2013b\nofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated\nwitheachcategory.\nAsimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,;\nMikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and\ntherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "otherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith\nwordsintheother.Eventhoughwemaynothavelabeledexamplestranslating\nwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa\ntranslationforwordAbecausewehavelearnedadistributedrepresentationfor\nwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and\ncreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples\nconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "mostsuccessfulifallthreeingredients(thetworepresentationsandtherelations\nbetweenthem)arelearnedjointly.\nZero-shotlearningisaparticularformoftransferlearning.Thesameprinciple\nexplainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation\n5 3 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nh x = f x ( ) x\nx t e s t\ny t e s th y = f y ( ) y\ny − s pa ce\nR e l at i onshi p  b e t w e e n   e m be dde d  p oi n t s   w i t hi n  one   o f   t h e   d o m a i n s\nMaps be t w e e n   r e p r e s e n t at i on spac e s  f x\nf y\nx − s pa ce\n( ) pa i r s i n t he t r a i ni ng s et x y ,\nf x : enco der f unctio n f o r x\nf y : enco der f unctio n f o r y\nFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand\nsimilarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions\nappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis\napplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints\ninxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance\ninh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "ofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled\nexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way\nortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe\nrepresentationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning\nisthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno\nimageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "andimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween\nrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwerenever\npaired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach\nother.FigureinspiredfromsuggestionbyHrantKhachatrian.\n5 4 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ninonemodality,arepresentationintheother,andtherelationship(ingeneralajoint\ndistribution)betweenpairs (xy,)consistingofoneobservationxinonemodality\nandanotherobservationyintheothermodality(SrivastavaandSalakhutdino v,\n2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,from\nytoitsrepresentation,andtherelationshipbetweenthetworepresentations),\nconceptsinonerepresentationareanchoredintheother,andvice-versa,allowing",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "onetomeaningfully generalizeto newpairs.Theprocedureis illustratedin\nﬁgure.15.3\n15. 3 S em i - S u p ervi s ed D i s en t a n g l i n g of C au s al F ac t ors\nAnimportantquestionaboutrepresentationlearningis“whatmakesonerepre-\nsentationbetterthananother?”Onehypothesisisthatanidealrepresentation\nisoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-\nlyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "spacecorrespondingtodiﬀerentcauses,sothattherepresentationdisentanglesthe\ncausesfromoneanother.Thishypothesismotivatesapproachesinwhichweﬁrst\nseekagoodrepresentationforp(x). Sucharepresentationmayalsobeagood\nrepresentationforcomputingp(yx|)ifyisamongthemostsalientcausesof\nx. Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast\nthe1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,),inmoredetail.\nForotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "supervisedlearning,wereferthereadertosection1.2of (). Chapelleetal.2006\nInotherapproachestorepresentationlearning,wehaveoftenbeenconcerned\nwitharepresentationthatiseasytomodel—forexample,onewhoseentriesare\nsparse,orindependentfromeachother.Arepresentationthatcleanlyseparates\ntheunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.\nHowever,afurtherpartofthehypothesismotivatingsemi-supervisedlearning\nviaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "propertiescoincide: once weareabletoobtaintheunderlyingexplanationsfor\nwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom\ntheothers.Speciﬁcally,ifarepresentationhrepresentsmanyoftheunderlying\ncausesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,\nthenitiseasytopredictfrom.yh\nFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised\nlearningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecase",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "wherep(x)isuniformlydistributedandwewanttolearnf(x) = E[y|x].Clearly,\nobservingatrainingsetofvaluesalonegivesusnoinformationabout. x p( )y x|\n5 4 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nxp x ( )y = 1 y = 2 y = 3\nFigure15.4:Exampleofadensityoverxthatisamixtureoverthree components.\nThecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture\ncomponents(e.g., naturalobjectclassesinimagedata)arestatisticallysalient,just\nmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor\ny.\nNext,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.\nConsiderthesituationwhere xarisesfromamixture,withonemixturecomponent",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "pervalueofy,asillustratedinﬁgure. Ifthemixturecomponentsarewell- 15.4\nseparated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda\nsinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).\nButmoregenerally,whatcouldmakeandbetiedtogether? p( )y x|p()x\nIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and\np(yx|)will bestronglytied, andunsupervisedrepresentationlearningthat\ntriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "semi-supervisedlearningstrategy.\nConsidertheassumptionthatyisoneofthecausalfactorsofx,andlet\nhrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas\nstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x\np,pp. (hx) = ( )xh|()h (15.1)\nAsaconsequence,thedatahasmarginalprobability\np() = x E hp. ( )xh| (15.2)\nFromthisstraightforwardobservation,weconcludethatthebestpossiblemodel\nofx(fromageneralization pointofview)istheonethatuncoverstheabove“true”\n5 4 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.\nThe“ideal”representationlearningdiscussedaboveshouldthusrecovertheselatent\nfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe\nveryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe\nconditionaldistributionofygivenxistiedbyBayes’ruletothecomponentsin\ntheaboveequation:\np( ) = yx|pp ( )xy|()y\np()x. (15.3)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "p( ) = yx|pp ( )xy|()y\np()x. (15.3)\nThusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge\nofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in\nsituationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove\nperformance.\nAnimportantresearchproblemregardsthefactthatmostobservationsare\nformedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but\ntheunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "anunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all\nsalientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking\niteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y\nInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible\ntocaptureallormostofthefactorsofvariationthatinﬂuenceanobservation.\nForexample,inavisualscene,shouldtherepresentationalwaysencodeallof\nthesmallestobjectsinthebackground? Itisawell-documented psychological",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "phenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat\narenotimmediately relevanttothetasktheyareperforming—see,e.g.,Simons\nandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis\ndetermining toencodeineachsituation.Currently,twoofthemainstrategies what\nfordealingwithalargenumberofunderlyingcausesaretouseasupervised\nlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthe\nmodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "largerrepresentationsifusingpurelyunsupervisedlearning.\nAnemergingstrategyforunsupervisedlearningistomodifythedeﬁnitionof\nwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative\nmodelshavebeentrainedtooptimizeaﬁxedcriterion,oftensimilartomean\nsquarederror.Theseﬁxedcriteriadeterminewhichcausesareconsideredsalient.\nForexample,meansquarederrorappliedtothepixelsofanimageimplicitly\nspeciﬁesthatanunderlyingcauseisonlysalientifitsigniﬁcantlychangesthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "brightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish\ntosolveinvolvesinteractingwithsmallobjects.Seeﬁgureforanexample 15.5\n5 4 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nInput Reconstruction\nFigure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas\nfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits\nspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand\narerelevanttotheroboticstask. Unfortunately,theautoencoderhaslimitedcapacity,\nandthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing\nsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall\npingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger\nobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.\nOtherdeﬁnitionsofsaliencearepossible.Forexample,ifagroupofpixels\nfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme\nbrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.\nOnewaytoimplementsuchadeﬁnitionofsalienceistousearecentlydeveloped",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "approachcalled g e ner at i v e adv e r sar i al net w o r k s( ,). Goodfellow etal.2014c\nInthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiﬁer.\nThefeedforwardclassiﬁerattemptstorecognizeallsamplesfromthegenerative\nmodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis\nframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis\nhighlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "insection.Forthepurposesofthepresentdiscussion,itissuﬃcientto 20.10.4\nunderstandthattheylearnhowtodeterminewhatissalient. () Lotteretal.2015\nshowedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect\ntogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully\ngeneratetheearswhentrainedwiththeadversarialframework.Becausethe\nearsarenotextremelybrightordarkcomparedtothesurroundingskin,they\narenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly\n5 4 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nGroundTruth MSE Adversarial\nFigure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof\nlearningwhichfeaturesaresalient. Inthisexample,thepredictivegenerativenetwork\nhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciﬁc\nviewingangle. ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould\nemit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r )",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "squarederroralone.Becausetheearsdonotcauseanextremediﬀerenceinbrightness\ncomparedtotheneighboringskin,theywerenotsuﬃcientlysalientforthemodeltolearn\ntorepresentthem. ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof\nmeansquarederrorandadversarialloss. Usingthislearnedcostfunction,theearsare\nsalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare\nimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures\ngraciouslyprovidedby (). Lotter e t a l .2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "graciouslyprovidedby (). Lotter e t a l .2015\nrecognizableshapeandconsistentpositionmeansthatafeedforwardnetwork\ncaneasilylearntodetectthem,makingthemhighlysalientunderthegenerative\nadversarialframework.Seeﬁgureforexampleimages.Generativeadversarial 15.6\nnetworksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.\nWeexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich\nfactorstorepresent,anddevelopmechanismsforrepresentingdiﬀerentfactors\ndependingonthetask.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "dependingonthetask.\nAbeneﬁtoflearningtheunderlyingcausalfactors,aspointedoutbySchölkopf\netal.(),isthatifthetruegenerativeprocesshas 2012 xasaneﬀectandyas\nacause,thenmodelingp(x y|)isrobusttochangesinp(y). Ifthecause-eﬀect\nrelationshipwasreversed,thiswouldnotbetrue,sincebyBayes’rule,p(x y|)\nwouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin\ndistributionduetodiﬀerentdomains,temporalnon-stationarity,orchangesin\nthenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "universeareconstant)whilethemarginaldistributionovertheunderlyingcauses\ncanchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan\n5 4 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nbeexpectedvialearningagenerativemodelthatattemptstorecoverthecausal\nfactorsand. h p( )xh|\n15. 4 D i s t ri b u t ed R ep res en t at i on\nDistributedrepresentationsofconcepts—representationscomposedofmanyele-\nmentsthatcanbesetseparatelyfromeachother—areoneofthemostimportant\ntoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause\ntheycanusenfeatureswithkvaluestodescribekndiﬀerentconcepts.Aswe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "haveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits\nandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof\ndistributedrepresentation. Wenowintroduceanadditionalobservation. Many\ndeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits\ncanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as\ndiscussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3\nbecauseeachdirectioninrepresentationspacecancorrespondtothevalueofa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentunderlyingconﬁgurationvariable.\nAnexampleofadistributedrepresentationisavectorofnbinaryfeatures,\nwhichcantake2nconﬁgurations, eachpotentiallycorrespondingtoadiﬀerent\nregionininputspace,asillustratedinﬁgure.Thiscanbecomparedwith 15.7\nasymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor\ncategory.Iftherearensymbolsinthedictionary,onecanimaginenfeature\ndetectors,eachcorrespondingtothedetectionofthepresenceoftheassociated",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "category.Inthatcaseonlyndiﬀerentconﬁgurations oftherepresentationspace\narepossible,carvingndiﬀerentregionsininputspace,asillustratedinﬁgure.15.8\nSuchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan\nbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone\nofthemcanbeactive).Asymbolicrepresentationisaspeciﬁcexampleofthe\nbroaderclassofnon-distributedrepresentations,whicharerepresentationsthat\nmaycontainmanyentriesbutwithoutsigniﬁcantmeaningfulseparatecontrolover",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "eachentry.\nExamplesoflearningalgorithms basedonnon-distributedrepresentations\ninclude:\n•Clusteringmethods,includingthek-meansalgorithm:eachinputpointis\nassignedtoexactlyonecluster.\n•k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples\nareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple\n5 4 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nh 1h 2 h 3\nh = [ 1 , , 1 1 ]\nh = [ 0 , , 1 1 ]h = [ 1 , , 0 1 ]h = [ 1 , , 1 0 ]\nh = [ 0 , , 1 0 ]h = [ 0 , , 0 1 ]h = [ 1 , , 0 0 ]\nFigure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation\nbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures\nh 1,h 2,andh 3. Eachfeatureisdeﬁnedbythresholdingtheoutputofalearned,linear\ntransformation.Eachfeaturedivides R2intotwohalf-planes.Leth+\nibethesetofinput",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "ibethesetofinput\npointsforwhichh i=1andh−\nibethesetofinputpointsforwhichh i=0.Inthis\nillustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding\narrowpointingtotheh+\nisideoftheboundary.Therepresentationasawholetakes\nonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the\nrepresentationvalue[1,1,1]correspondstotheregionh+\n1∩h+\n2∩h+\n3.Comparethistothe\nnon-distributedrepresentationsinﬁgure.Inthegeneralcaseof 15.8 dinputdimensions,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "adistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes.\nThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diﬀerent\nregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly\nnregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany\nmoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible\n(thereisnoh=0inthisexample)andthatalinearclassiﬁerontopofthedistributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "representationisnotabletoassigndiﬀerentclassidentitiestoeveryneighboringregion;\nevenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew\nisthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998\nlayerandaweakclassiﬁerlayercanbeastrongregularizer;aclassiﬁertryingtolearn\ntheconceptof“person”versus“notaperson”doesnotneedtoassignadiﬀerentclassto\naninputrepresentedas“womanwithglasses”thanitassignstoaninputrepresentedas",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "“manwithoutglasses.”Thiscapacityconstraintencourageseachclassiﬁertofocusonfew\nh iandencouragestolearntorepresenttheclassesinalinearlyseparableway. h\n5 4 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom\neachother,sothisdoesnotqualifyasatruedistributedrepresentation.\n•Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is\nactivatedwhenaninputisgiven.\n•Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or\nexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest\nneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "thosevaluescannotreadilybecontrolledseparatelyfromeachother.\n•KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):\nalthoughthedegreeofactivationofeach“supportvector”ortemplateexample\nisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.\n•Languageortranslationmodelsbasedonn-grams.Thesetofcontexts\n(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuﬃxes.\nAleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "Separateparametersareestimatedforeachleafofthetree(withsomesharing\nbeingpossible).\nForsomeofthesenon-distributedalgorithms,theoutputisnotconstantby\npartsbutinsteadinterpolatesbetweenneighboringregions.Therelationship\nbetweenthenumberofparameters(orexamples)andthenumberofregionsthey\ncandeﬁneremainslinear.\nAnimportantrelatedconceptthatdistinguishesadistributedrepresentation\nfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentconcepts.Aspuresymbols,“cat”and“dog”areasfarfromeachother\nasanyothertwosymbols.However,ifoneassociatesthemwithameaningful\ndistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats\ncangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation\nmaycontainentriessuchas“has_fur”or“number_of_legs”thathavethesame\nvaluefortheembeddingofboth“cat”and“dog.”Neurallanguagemodelsthat\noperateondistributedrepresentationsofwordsgeneralizemuchbetterthanother",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "modelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin\nsection.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich\nsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis\nabsentfrompurelysymbolicrepresentations.\nWhenandwhycantherebeastatisticaladvantagefromusingadistributed\nrepresentationaspartofalearningalgorithm? D istributedrepresentationscan\n5 4 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nFigure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintodiﬀerentregions.Thenearestneighboralgorithmprovidesanexampleofalearning\nalgorithmbasedonanon-distributedrepresentation.Diﬀerentnon-distributedalgorithms\nmayhavediﬀerentgeometry, but theytypicallybreaktheinput spaceintoregions,\nw i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "approachisthat,givenenoughparameters,itcanﬁtthetrainingsetwithoutsolvinga\ndiﬃcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiﬀerentoutput\ni n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels\ngeneralizeonlylocallyviathesmoothnessprior,makingitdiﬃculttolearnacomplicated\nfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast\nthiswithadistributedrepresentation,ﬁgure.15.7\n5 4 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe\ncompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-\ndistributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,\nwhichstatesthatifuv≈,thenthetargetfunctionftobelearnedhasthe\npropertythatf(u)≈f(v),ingeneral.Therearemanywaysofformalizingsuchan\nassumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "knowthatf(x)≈y,thenwechooseanestimator ˆfthatapproximatelysatisﬁes\ntheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby\ninputx+.Thisassumptionisclearlyveryuseful,butitsuﬀersfromthecurseof\ndimensionality: inordertolearnatargetfunctionthatincreasesanddecreases\nmanytimesinmanydiﬀerentregions,1wemayneedanumberofexamplesthatis\natleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof\ntheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "eachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol\ntovalue. However,thisdoesnotallowustogeneralizetonewsymbolsfornew\nregions.\nIfwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing\nsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean\nobjectregardlessofitslocationintheimage,eventhoughspatialtranslationof\ntheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.\nLetusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each\nbinaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces, as\nillustratedinﬁgure.Theexponentiallylargenumberofintersectionsof 15.7 n\nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed\nrepresentationlearnercandistinguish.Howmanyregionsaregeneratedbyan\narrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe\nintersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis\nd\nj = 0n\nj\n= (Ond). (15.4)\nTherefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin\nthenumberofhiddenunits.\n1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "re g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i ﬀ e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e\nm i g h t wa n t t o d i ﬀ e r i n f 2dd i ﬀ e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s .\n5 5 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nThisprovidesageometricargumenttoexplainthegeneralization powerof\ndistributedrepresentation:withO(nd)parameters(fornlinear-threshold features\nin Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade\nnoassumptionatallaboutthedata,andusedarepresentationwithoneunique\nsymbolforeachregion,andseparateparametersforeachsymboltorecognizeits\ncorrespondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "examples.Moregenerally,theargumentinfavorofthedistributedrepresentation\ncouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe\nusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin\nthedistributedrepresentation.Theargumentinthiscaseisthatifaparametric\ntransformationwithkparameterscanlearnaboutrregionsininputspace,with\nkr,andifobtainingsucharepresentationwasusefultothetaskofinterest,then\nwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "settingwherewewouldneedO(r)examplestoobtainthesamefeaturesand\nassociatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto\nrepresentthemodelmeansthatwehavefewerparameterstoﬁt,andthusrequire\nfarfewertrainingexamplestogeneralizewell.\nAfurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-\ntationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto\ndistinctlyencodesomanydiﬀerentregions.Forexample,theVCdimensionofa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber\nofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery\nmanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode\nspace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace\nhtotheoutputyusingalinearclassiﬁer.Theuseofadistributedrepresentation\ncombinedwithalinearclassiﬁerthusexpressesapriorbeliefthattheclassesto\nberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "capturedbyh. Wewilltypicallywanttolearncategoriessuchasthesetofall\nimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat\nrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition\nthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall\ngreencarsandredtrucksasanotherclass.\nTheideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally\nvalidated. ()ﬁndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "trainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery\nofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.\nInpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething\nthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe\ntoplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein\n5 5 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n-+ =\nFigure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles\ntheconceptofgenderfromtheconceptofwearingglasses. Ifwebeginwiththerepre-\nsentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe\nconceptofamanwithoutglasses,andﬁnallyaddthevectorrepresentingtheconcept\nofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman\nwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "imagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith\npermissionfrom (). Radford e t a l .2015\ncommonisthatonecouldimagine learningabouteachofthemwithouthavingto\nseealltheconﬁgurationsofalltheothers. ()demonstratedthat Radfordetal.2015\nagenerativemodelcanlearnarepresentationofimagesoffaces,withseparate\ndirectionsinrepresentationspacecapturingdiﬀerentunderlyingfactorsofvariation.\nFiguredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "towhetherthepersonismaleorfemale,whileanothercorrespondstowhether\nthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not\nﬁxedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiﬁers:\ngradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically\ninterestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout\nthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof\nglasses,withouthavingtocharacterizealloftheconﬁgurations ofthen−1other",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "featuresbyexamplescoveringallofthesecombinationsofvalues. Thisformof\nstatisticalseparabilityiswhatallowsonetogeneralizetonewconﬁgurations ofa\nperson’sfeaturesthathaveneverbeenseenduringtraining.\n5 5 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n15. 5 E x p on en t i al Gai n s f rom D ep t h\nWehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1\ntors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep\nnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto\nimprovedstatisticaleﬃciency.Inthissection,wedescribehowsimilarresultsapply\nmoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.\nInsection,wesawanexampleofagenerativemodelthatlearnedabout 15.4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "theexplanatoryfactorsunderlyingimagesoffaces,includingtheperson’sgender\nandwhethertheyarewearingglasses.Thegenerativemodelthataccomplished\nthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect\nashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship\nbetweentheseabstractexplanatoryfactorsandthepixelsintheimage. Inthis\nandotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom\neachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "high-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis\ndemands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas\nfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough\nthecompositionofmanynonlinearities.\nIthasbeenproveninmanydiﬀerentsettingsthatorganizingcomputation\nthroughthecompositionofmanynonlinearities andahierarchyofreusedfeatures\ncangiveanexponentialboosttostatisticaleﬃciency,ontopoftheexponential",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "boostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,\nwithsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with\nasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel\nfamilythatisauniversalapproximator canapproximatealargeclassoffunctions\n(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough\nhiddenunits. However,therequirednumberofhiddenunitsmaybeverylarge.\nTheoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "therearefamiliesoffunctionsthatcanberepresentedeﬃcientlybyanarchitecture\nofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect\ntotheinputsize)withinsuﬃcientdepth(depth2ordepth).k−1\nInsection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1\napproximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle\nhiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep\nbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "andBengio20082010MontúfarandAy2011Montúfar2014Krause ,,; ,;,; etal.,\n2013).\n5 5 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nInsection,wesawthatasuﬃcientlydeepfeedforwardnetworkcanhave 6.4.1\nanexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso\nbeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic\nmodelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These\nmodelsusepolynomialcircuitstocomputetheprobabilitydistributionovera\nsetofrandomvariables. ()showedthatthereexist DelalleauandBengio2011",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid\nneedinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014\nshowedthattherearesigniﬁcantdiﬀerencesbetweeneverytwoﬁnitedepthsof\nSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit\ntheirrepresentationalpower.\nAnotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive\npoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "exponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed\ntoonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal.\n2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe\ncasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.\n15. 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es\nToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone\nrepresentationbetterthananother?Oneanswer,ﬁrstintroducedinsection,is15.3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof\nvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour\napplications.Moststrategiesforrepresentationlearningarebasedonintroducing\ncluesthathelpthelearningtoﬁndtheseunderlyingfactorsofvariations.Theclues\ncanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised\nlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "speciﬁesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,\ntomakeuseofabundantunlabeleddata,representationlearningmakesuseof\nother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof\nimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein\nordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat\nregularizationstrategiesarenecessarytoobtaingoodgeneralization. Whileitis\nimpossibletoﬁndauniversallysuperiorregularizationstrategy,onegoalofdeep",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "learningistoﬁndasetoffairlygenericregularizationstrategiesthatareapplicable\ntoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable\ntosolve.\n5 5 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nWeprovideherealistofthesegenericregularizationstrategies.Thelistis\nclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning\nalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying\nfactors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d\nbeenpartiallyexpandedhere.\n•Smoothness:Thisistheassumptionthatf(x+d)≈f(x)forunitdand\nsmall.Thisassumptionallowsthelearnertogeneralizefromtraining",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "examplestonearbypointsininputspace.Manymachinelearningalgorithms\nleveragethisidea,butitisinsuﬃcienttoovercomethecurseofdimensionality.\n•Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome\nvariablesarelinear.Thisallowsthealgorithmtomakepredictionseven\nveryfarfromtheobserveddata,butcansometimesleadtooverlyextreme\npredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe\nsmoothnessassumptioninsteadmakethelinearityassumption.Theseare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "infactdiﬀerentassumptions—linearfunctionswithlargeweightsapplied\ntohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.\n()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b\n•Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare\nmotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying\nexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate\nofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "supervisedlearningviarepresentationlearning.Learningthestructureofp(x)\nrequireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|\nx)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4\ndescribeshowthisviewmotivatestheuseofdistributedrepresentations,with\nseparatedirectionsinrepresentationspacecorrespondingtoseparatefactors\nofvariation.\n•Causalfactors:themodelisconstructedinsuchawaythatittreatsthe\nfactorsofvariationdescribedbythelearnedrepresentationhasthecauses",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "oftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3\nisadvantageousforsemi-supervisedlearningandmakesthelearnedmodel\nmorerobustwhenthedistributionovertheunderlyingcauseschangesor\nwhenweusethemodelforanewtask.\n•Depthahierarchical organization ofexplanatory factors , or  :High-level,\nabstractconceptscanbedeﬁnedintermsofsimpleconcepts,forminga\nhierarchy.From another point of view, the us e ofa deeparchitecture\n5 5 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step\nprogram, with eachstep referringbacktothe outputoftheprocessing\naccomplishedviaprevioussteps.\n•Sharedfactors across tasks:In thecontextwherewehavemanytasks,\ncorrespondingtodiﬀerentyivariablessharingthesameinput xorwhere\neachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput\nx,theassumptionisthateachyiisassociatedwithadiﬀerentsubsetfroma",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "commonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning\nalltheP(yi|x)viaasharedintermediate representationP(h x|)allows\nsharingofstatisticalstrengthbetweenthetasks.\n•Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-\ncentratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous\ncase,theseregionscanbeapproximatedbylow-dimensional manifoldswith\namuchsmallerdimensionalitythantheoriginalspacewherethedatalives.\nManymachinelearningalgorithmsbehavesensiblyonlyonthismanifold",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b\nautoencoders,attempttoexplicitlylearnthestructureofthemanifold.\n•Naturalclustering:Manymachinelearningalgorithmsassumethateach\nconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.The\ndatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant\nwithineachoneofthese. Thisassumptionmotivatesavarietyoflearning\nalgorithms,includingtangentpropagation, doublebackprop,themanifold\ntangentclassiﬁerandadversarialtraining.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "tangentclassiﬁerandadversarialtraining.\n•Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms\nmaketheassumptionthatthemostimportantexplanatoryfactorschange\nslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying\nexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.\nSeesectionforfurtherdescriptionofthisapproach. 13.3\n•Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost\ninputs—thereisnoneedtouseafeaturethatdetectselephanttrunkswhen",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "representinganimageofacat.Itisthereforereasonabletoimposeaprior\nthatanyfeaturethatcanbeinterpretedas“present”or“absent”shouldbe\nabsentmostofthetime.\n•SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the\nfactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest\n5 5 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\npossibleismarginalindependence,P(h) =\niP(h i),butlineardependencies\northosecapturedbyashallowautoencoderarealsoreasonableassumptions.\nThiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga\nlinearpredictororafactorizedpriorontopofalearnedrepresentation.\nTheconceptofrepresentationlearningtiestogetherallofthemanyforms\nofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep\nprobabilisticmodelsalllearnandexploitrepresentations.Learning thebest",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "possiblerepresentationremainsanexcitingavenueofresearch.\n5 5 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 6\nS t ru ct u r e d Probabilis t i c Mo d e l s\nf or D e e p L e ar n i n g\nDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto\nguidetheirdesigneﬀortsanddescribetheiralgorithms.Oneoftheseformalisms\nistheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed\nstructuredprobabilisticmodelsbrieﬂyinsection.Thatbriefpresentationwas 3.14\nsuﬃcienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "describesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III\nmodelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep\nlearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes\nstructuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended\ntobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction\nbeforecontinuingwiththischapter.\nAstructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "usingagraphtodescribewhichrandomvariablesintheprobabilitydistribution\ninteractwitheachotherdirectly.Hereweuse“graph”inthegraphtheorysense—a\nsetofverticesconnectedtooneanotherbyasetofedges.Becausethestructure\nofthemodelisdeﬁnedbyagraph,thesemodelsareoftenalsoreferredtoas\ngraphicalmodels.\nThegraphicalmodelsresearchcommunityislargeandhasdevelopedmany\ndiﬀerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we\nprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning\nresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,\nyoumaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert\n558",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nmaybeneﬁtfromreadingtheﬁnalsectionofthischapter,section,inwhichwe 16.7\nhighlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning\nalgorithms.Deeplearningpractitioners tendtouseverydiﬀerentmodelstructures,\nlearningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest\nofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese\ndiﬀerencesinpreferencesandexplainthereasonsforthem.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Inthischapterweﬁrstdescribethechallengesofbuildinglarge-scaleproba-\nbilisticmodels. Next,wedescribehowtouseagraphtodescribethestructure\nofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany\nchallenges,itisnotwithoutitsowncomplications. Oneofthemajordiﬃcultiesin\ngraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract\ndirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem. We\noutlinetwoapproachestoresolvingthisdiﬃcultybylearningaboutthedependen-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "ciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5\ndeeplearningpractitioners placeonspeciﬁcapproachestographicalmodelingin\nsection.16.7\n16.1TheChallengeofUnstructuredModeling\nThegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges\nneededtosolveartiﬁcialintelligence.Thismeansbeingabletounderstandhigh-\ndimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto\nbeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "documentscontainingmultiplewordsandpunctuationcharacters.\nClassiﬁcationalgorithmscantakeaninputfromsucharichhigh-dimensional\ndistributionandsummarizeitwithacategoricallabel—whatobjectisinaphoto,\nwhatwordisspokeninarecording,whattopicadocumentisabout.Theprocess\nofclassiﬁcationdiscardsmostoftheinformationintheinputandproducesa\nsingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The\nclassiﬁerisalsooftenabletoignoremanypartsoftheinput.Forexample,when",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "recognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof\nthephoto.\nItispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare\noftenmoreexpensivethanclassiﬁcation.Someofthemrequireproducingmultiple\noutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof\n1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "e n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c .\n5 5 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:\n•Densityestimation:givenaninput x,themachinelearningsystemreturns\nanestimateofthetruedensity p( x)underthedatageneratingdistribution.\nThisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-\ningoftheentireinput.Ifevenoneelementofthevectorisunusual,the\nsystemmustassignitalowprobability.\n•Denoising:givenadamagedorincorrectlyobservedinput ˜ x,themachine",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "learningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample,\nthemachinelearningsystemmightbeaskedtoremovedustorscratches\nfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe\nestimatedcleanexample x)andanunderstandingoftheentireinput(since\nevenonedamagedareawillstillrevealtheﬁnalestimateasbeingdamaged).\n•Missingvalueimputation:giventheobservationsofsomeelementsof x,\nthemodelisaskedtoreturnestimatesoforaprobabilitydistributionover",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "someoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs.\nBecausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it\nmustunderstandtheentireinput.\n•Sampling:themodelgeneratesnewsamplesfromthedistribution p( x).\nApplicationsincludespeechsynthesis,i.e.producingnewwaveformsthat\nsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda\ngoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn\nfromthewrongdistribution,thenthesamplingprocessiswrong.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "Foranexampleofasamplingtaskusingsmallnaturalimages,seeﬁgure.16.1\nModelingarichdistributionoverthousandsormillionsofrandomvariablesisa\nchallengingtask,bothcomputationally andstatistically.Supposeweonlywanted\ntomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit\nseemsoverwhelming.Forasmall, 32×32 2 pixelcolor(RGB)image,thereare3 0 7 2\npossiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan\ntheestimatednumberofatomsintheuniverse.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "theestimatednumberofatomsintheuniverse.\nIngeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining\nndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof\nrepresenting P(x)bystoringalookuptablewithoneprobabilityvalueperpossible\noutcomerequires knparameters!\nThisisnotfeasibleforseveralreasons:\n5 6 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.1:Probabilisticmodelingofnaturalimages. ( T o p )Example32×32pixelcolor\nimagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom )\ndrawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears\natthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean\nspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen\nadjustedfordisplay.Figurereproducedwithpermissionfrom (). Courville e t a l .2011\n5 6 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n• M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues\nof nand k,representingthedistributionasatablewillrequiretoomany\nvaluestostore.\n• St a t i s t i c a l e ﬃ c i e nc y:Asthenumberofparametersinamodelincreases,\nsodoestheamountoftrainingdataneededtochoosethevaluesofthose\nparametersusingastatisticalestimator.Becausethetable-basedmodel",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "hasanastronomicalnumberofparameters,itwillrequireanastronomically\nlargetrainingsettoﬁtaccurately.Anysuchmodelwilloverﬁtthetraining\nsetverybadlyunlessadditionalassumptionsaremadelinkingthediﬀerent\nentriesinthetable(forexample,likeinback-oﬀorsmoothed n-grammodels,\nsection).12.4.1\n• R u nt i m e :   t h e c o s t o f i nfe r e nc e: Supposewewanttoperformaninference\ntaskwhereweuseourmodelofthejointdistribution P(x)tocomputesome\notherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "distribution P(x 2|x 1).Computingthesedistributionswillrequiresumming\nacrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe\nintractablememorycostofstoringthemodel.\n• R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample\nfromthemodel.Thenaivewaytodothisistosamplesomevalueu∼ U(0 ,1),\ntheniteratethroughthetable,addinguptheprobabilityvaluesuntilthey\nexceed uandreturntheoutcomecorrespondingtothatpositioninthetable.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "Thisrequiresreadingthroughthewholetableintheworstcase,soithas\nthesameexponentialcostastheotheroperations.\nTheproblemwiththetable-basedapproachisthatweareexplicitlymodeling\neverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The\nprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.\nUsually,mostvariablesinﬂuenceeachotheronlyindirectly.\nForexample,considermodelingtheﬁnishingtimesofateaminarelayrace.\nSupposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "therace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting\nherlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown\nlapandhandsthebatontoCarol,whorunstheﬁnallap.Wecanmodeleachof\ntheirﬁnishingtimesasacontinuousrandomvariable.Alice’sﬁnishingtimedoes\nnotdependonanyoneelse’s,sinceshegoesﬁrst.Bob’sﬁnishingtimedepends\nonAlice’s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice\nhascompletedhers. IfAliceﬁnishesfaster,Bobwillﬁnishfaster,allelsebeing\n5 6 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nequal.Finally,Carol’sﬁnishingtimedependsonbothherteammates.IfAliceis\nslow,Bobwillprobablyﬁnishlatetoo.Asaconsequence,Carolwillhavequitea\nlatestartingtimeandthusislikelytohavealateﬁnishingtimeaswell.However,\nCarol’sﬁnishingtimedependsonly i ndir e c t l yonAlice’sﬁnishingtimeviaBob’s.\nIfwealreadyknowBob’sﬁnishingtime,wewillnotbeabletoestimateCarol’s\nﬁnishingtimebetterbyﬁndingoutwhatAlice’sﬁnishingtimewas.Thismeans",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "wecanmodeltherelayraceusingonlytwointeractions: Alice’seﬀectonBoband\nBob’seﬀectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice\nandCarolfromourmodel.\nStructuredprobabilisticmodelsprovideaformalframeworkformodelingonly\ndirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohave\nsigniﬁcantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.\nThesesmallermodelsalsohavedramatically reducedcomputational costinterms",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "ofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom\nthemodel.\n16.2UsingGraphstoDescribeModelStructure\nStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof“nodes”or\n“vertices”connectedbyedges)torepresentinteractionsbetweenrandomvariables.\nEachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.\nThesedirectinteractionsimplyother,indirectinteractions,butonlythedirect\ninteractionsneedtobeexplicitlymodeled.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "interactionsneedtobeexplicitlymodeled.\nThereismore thanone wayto describe theinteractionsin aprobability\ndistributionusingagraph.Inthefollowingsectionswedescribesomeofthemost\npopularandusefulapproaches.Graphicalmodelscanbelargelydividedinto\ntwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon\nundirectedgraphs.\n1 6 . 2 . 1 D i rect ed Mo d el s\nOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,\notherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,).",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "Directedgraphicalmodelsarecalled“directed”becausetheiredgesaredirected,\n2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm “ B a y e s i a n n e t wo rk ” wh e n o n e wis h e s t o “ e m p h a s i z e\nt h e j u d g m e n t a l ” n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i . e . t o h i g h l i g h t t h a t t h e y u s u a l l y\nre p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s .\n5 6 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nt 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol\nFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice’sﬁnishing\ntimet 0inﬂuencesBob’sﬁnishingtimet 1,becauseBobdoesnotgettostartrunninguntil\nAliceﬁnishes.Likewise,CarolonlygetstostartrunningafterBobﬁnishes,soBob’s\nﬁnishingtimet 1directlyinﬂuencesCarol’sﬁnishingtimet 2.\nthatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "thedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable’s\nprobabilitydistributionisdeﬁnedintermsoftheother’s.Drawinganarrowfrom\natobmeansthatwedeﬁnetheprobabilitydistributionoverbviaaconditional\ndistribution,withaasoneofthevariablesontherightsideoftheconditioning\nbar.Inotherwords,thedistributionoverbdependsonthevalueofa.\nContinuingwiththerelayraceexamplefromsection,supposewename 16.1\nAlice’sﬁnishingtimet 0,Bob’sﬁnishingtimet 1,andCarol’sﬁnishingtimet 2.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "Aswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends\ndirectlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected\ngraphicalmodel,illustratedinﬁgure.16.2\nFormally,adirectedgraphicalmodeldeﬁnedonvariables xisdeﬁnedbya\ndirectedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel,\nandasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where\nP aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven\nby\np() = Πx i p(x i| P aG(x i)) . (16.1)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "by\np() = Πx i p(x i| P aG(x i)) . (16.1)\nInourrelayraceexample,thismeansthat,usingthegraphdrawninﬁgure,16.2\np(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) . (16.2)\nThisisourﬁrsttimeseeingastructuredprobabilisticmodelinaction.We\ncanexaminethecostofusingit,inordertoobservehowstructuredmodelinghas\nmanyadvantagesrelativetounstructuredmodeling.\nSupposewerepresentedtimebydiscretizingtimerangingfromminute0to\nminute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "variablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha\ntable,itwouldneedtostore999,999values(100valuesoft 0×100valuesoft 1×\n100valuesoft 2,minus1,sincetheprobabilityofoneoftheconﬁgurations ismade\n5 6 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we\nonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe\ndistributionovert 0requires99values,thetabledeﬁningt 1givent 0requires9900\nvalues,andsodoesthetabledeﬁningt 2givent 1.Thiscomestoatotalof19,899\nvalues.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof\nparametersbyafactorofmorethan50!",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "parametersbyafactorofmorethan50!\nIngeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe\nsingletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose\nwebuildadirectedgraphicalmodeloverthesevariables. If misthemaximum\nnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingle\nconditionalprobabilitydistribution,thenthecostofthetablesforthedirected\nmodelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we\ngetverydramaticsavings.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "getverydramaticsavings.\nInotherwords,solongaseachvariablehasfewparentsinthegraph,the\ndistributioncanberepresentedwithveryfewparameters. Somerestrictionson\nthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat\noperationslikecomputingmarginalorconditionaldistributionsoversubsetsof\nvariablesareeﬃcient.\nItisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin\nthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "areconditionallyindependentfromeachother.Itisalsopossibletomakeother\nkindsofsimplifyingassumptions. Forexample,supposeweassumeBobalways\nrunsthesameregardlessofhowAliceperformed.(Inreality,Alice’sperformance\nprobablyinﬂuencesBob’sperformance—dependingonBob’spersonality,ifAlice\nrunsespeciallyfastinagivenrace,thismightencourageBobtopushhardand\nmatchherexceptionalperformance,oritmightmakehimoverconﬁdentandlazy).\nThentheonlyeﬀectAlicehasonBob’sﬁnishingtimeisthatwemustaddAlice’s",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "ﬁnishingtimetothetotalamountoftimewethinkBobneedstorun.This\nobservationallowsustodeﬁneamodelwith O( k)parametersinsteadof O( k2).\nHowever,notethatt 0andt 1arestilldirectlydependentwiththisassumption,\nbecauset 1representstheabsolutetimeatwhichBobﬁnishes,notthetotaltime\nhehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom\nt 0tot 1.TheassumptionthatBob’spersonalrunningtimeisindependentfrom\nallotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "encodethisinformationinthedeﬁnitionoftheconditionaldistributionitself.The\nconditionaldistributionisnolongera k k×−1elementtableindexedbyt 0andt 1\nbutisnowaslightlymorecomplicatedformulausingonly k−1parameters.The\ndirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeﬁne\n5 6 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nourconditionaldistributions.Itonlydeﬁneswhichvariablestheyareallowedto\ntakeinasarguments.\n1 6 . 2 . 2 Un d i rec t ed Mo d el s\nDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-\nticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise\nknownasMarkovrandomﬁelds(MRFs)orMarkovnetworks(Kinder-\nmann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges\nareundirected.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "areundirected.\nDirectedmodelsaremostnaturallyapplicabletosituationswherethereis\naclearreasontodraweacharrowinoneparticulardirection.Oftentheseare\nsituationswhereweunderstandthecausalityandthecausalityonlyﬂowsinone\ndirection.Onesuchsituationistherelayraceexample.Earlierrunnersaﬀectthe\nﬁnishingtimesoflaterrunners;laterrunnersdonotaﬀecttheﬁnishingtimesof\nearlierrunners.\nNotallsituationswemightwanttomodelhavesuchacleardirectiontotheir",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "interactions.Whentheinteractionsseemtohavenointrinsicdirection,orto\noperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.\nAsanexampleofsuchasituation,supposewewanttomodeladistribution\noverthreebinaryvariables:whetherornotyouaresick,whetherornotyour\ncoworkerissick,andwhetherornotyourroommateissick.Asintherelayrace\nexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat\ntakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "other,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa\ncolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel\nit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and\nthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof\nacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe\ncoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour\nroommate.\nInthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "itisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional\nnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.\nAswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan\nedge,thentherandomvariablescorrespondingtothosenodesinteractwitheach\notherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno\narrow,andisnotassociatedwithaconditionalprobabilitydistribution.\n5 6 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh r h r h y h y h c h c\nFigure16.3:Anundirectedgraphrepresentinghowyourroommate’shealthh r,your\nhealthh y,andyourworkcolleague’s healthh caﬀecteachother.Youandyourroommate\nmightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,\nbutassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan\nonlyinfecteachotherindirectlyviayou.\nWedenotetherandomvariablerepresentingyourhealthash y,therandom",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "variablerepresentingyourroommate’shealthash r,andtherandomvariable\nrepresentingyourcolleague’shealthash c.Seeﬁgureforadrawingofthe 16.3\ngraphrepresentingthisscenario.\nFormally,anundirectedgraphicalmodelisastructuredprobabilisticmodel\ndeﬁnedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor φ(C)\n(alsocalledacliquepotential) measurestheaﬃnityofthevariablesinthatclique\nforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "non-negative.Togethertheydeﬁneanunnormalizedprobabilitydistribution\n˜ p() = Πx C∈G φ .()C (16.3)\nTheunnormalized probabilitydistributioniseﬃcienttoworkwithsolongas\nallthecliquesaresmall.Itencodestheideathatstateswithhigheraﬃnityare\nmorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe\ndeﬁnitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem\ntogetherwillyieldavalidprobabilitydistribution.Seeﬁgureforanexample 16.4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "ofreadingfactorizationinformationfromanundirectedgraph.\nOurexampleofthecoldspreadingbetweenyou,yourroommate,andyour\ncolleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis\ncliquecanbedeﬁnedbyatable,andmighthavevaluesresemblingthese:\nh y= 0h y= 1\nh c= 021\nh c= 1110\n3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f\nt h e g ra p h .\n5 6 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nAstateof1indicatesgoodhealth,whileastateof0indicatespoorhealth\n(havingbeen infectedwith acold).Both ofyou areusuallyhealthy, sothe\ncorrespondingstatehasthehighestaﬃnity.Thestatewhereonlyoneofyouis\nsickhasthelowestaﬃnity,becausethisisararestate.Thestatewherebothof\nyouaresick(becauseoneofyouhasinfectedtheother)isahigheraﬃnitystate,\nthoughstillnotascommonasthestatewherebotharehealthy.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "Tocompletethemodel,wewouldneedtoalsodeﬁneasimilarfactorforthe\ncliquecontainingh yandh r.\n1 6 . 2 . 3 T h e P a rt i t i o n F u n ct i o n\nWhiletheunnormalized probabilitydistributionisguaranteedtobenon-negative\neverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid\nprobabilitydistribution,wemustusethecorrespondingnormalizedprobability\ndistribution:4\np() =x1\nZ˜ p()x (16.4)\nwhere Zisthevalue thatresultsintheprobability distributionsummingor\nintegratingto1:\nZ=\n˜ p d . ()xx (16.5)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "integratingto1:\nZ=\n˜ p d . ()xx (16.5)\nYoucanthinkof Zasaconstantwhenthe φfunctionsareheldconstant.Note\nthatifthe φfunctionshaveparameters,then Zisafunctionofthoseparameters.\nItiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace.\nThenormalizingconstant Zisknownasthepartitionfunction,atermborrowed\nfromstatisticalphysics.\nSince Zisanintegralorsumoverallpossiblejointassignmentsofthestatex\nitisoftenintractabletocompute. Inordertobeabletoobtainthenormalized",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionofanundirectedmodel, themodelstructureandthe\ndeﬁnitionsofthe φfunctionsmustbeconducivetocomputing Zeﬃciently.In\nthecontextofdeeplearning, Zisusuallyintractable. Due totheintractability\nofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate\nalgorithmsarethetopicofchapter.18\nOneimportantconsiderationtokeepinmindwhendesigningundirectedmodels\nisthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral\n4A d i s t rib u t i o n d e ﬁ n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s\nd is t rib u t i on .\n5 6 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nof˜ povertheirdomaindiverges.Forexample,supposewewanttomodelasingle\nscalarvariablexwithasinglecliquepotential ∈ R φ x x () = 2.Inthiscase,\nZ=\nx2d x . (16.6)\nSincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto\nthischoiceof φ( x). Sometimes thechoiceofsomeparameterofthe φfunctions\ndetermineswhetherthe probabilit ydistribution isdeﬁned.For example, for\nφ( x; β) =exp− β x2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "φ( x; β) =exp− β x2\n,the βparameterdetermineswhether Zexists.Positive β\nresultsinaGaussiandistributionoverxbutallothervaluesof βmake φimpossible\ntonormalize.\nOnekeydiﬀerencebetweendirectedmodelingandundirectedmodelingisthat\ndirectedmodelsaredeﬁneddirectlyintermsofprobabilitydistributionsfrom\nthestart,whileundirectedmodelsaredeﬁnedmorelooselyby φfunctionsthat\narethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone\nmustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "whileworkingwithundirectedmodelsisthatthedomainofeachofthevariables\nhasdramaticeﬀectonthekindofprobabilitydistributionthatagivensetof φ\nfunctionscorrespondsto.Forexample,consideran n-dimensionalvector-valued\nrandomvariable xandanundirectedmodelparametrized byavectorofbiases\nb.Supposewehaveonecliqueforeachelementofx, φ( ) i(x i) =exp( b ix i).What\nkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo\nnothaveenoughinformation,becausewehavenotyetspeciﬁedthedomainofx.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "Ifx ∈ Rn,thentheintegraldeﬁning Zdivergesandnoprobabilitydistribution\nexists.Ifx∈{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with\np(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors\n({[1 ,0 , . . . ,0] ,[0 ,1 , . . . ,0] , . . . ,[0 ,0 , . . . ,1]})then p(x)=softmax ( b),soalarge\nvalueof b iactuallyreduces p(x j=1)for j= i. Often,itispossibletoleverage\ntheeﬀectofacarefullychosendomainofavariableinordertoobtaincomplicated",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "behaviorfromarelativelysimplesetof φfunctions.Wewillexploreapractical\napplicationofthisidealater,insection.20.6\n1 6 . 2 . 4 E n erg y-B a s ed Mo d el s\nManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-\nsumptionthat∀x ,˜ p(x) >0.Aconvenientwaytoenforcethisconditionistouse\nan (EBM)where energy-basedmodel\n˜ p E () = exp( x −())x (16.7)\n5 6 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas\n1\nZφ a b ,(ab ,) φ b c ,(bc ,) φ a d ,(ad ,) φ b e ,(be ,) φ e f ,(ef ,)foranappropriatechoiceofthe φfunc-\ntions.\nand E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall\nz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero\nforanystatex.Beingcompletely free to choose theenergyfunction makes",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "learningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse\nconstrainedoptimization toarbitrarilyimposesomespeciﬁcminimalprobability\nvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5\nTheprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero\nbutneverreachit.\nAnydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-\nmann distribution.For this reason, manyenergy-based models are called",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Boltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l .,\n1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall\namodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The\ntermBoltzmannmachinewasﬁrstintroducedtodescribeamodelwithexclusively\nbinaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted\nBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann\nmachineswereoriginallydeﬁnedtoencompassbothmodelswithandwithoutla-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate\nmodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables\naremoreoftencalledMarkovrandomﬁeldsorlog-linearmodels.\nCliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized\nprobabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdiﬀerent\ncliquesintheundirectedgraphcorrespondtothediﬀerenttermsoftheenergy\nfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "network:theexponentiationmakeseachtermintheenergyfunctioncorrespond\ntoafactorforadiﬀerentclique.Seeﬁgureforanexampleofhowtoreadthe 16.5\n5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s . Z\n5 7 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure 16.5:Thisgraph impliesthat E(abcdef , , , , ,)can be writtenas E a b ,(ab ,)+\nE b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique\nenergyfunctions.Notethatwecanobtainthe φfunctionsinﬁgurebysettingeach 16.4 φ\ntotheexponentialofthecorrespondingnegativeenergy,e.g., φ a b ,(ab ,) =exp(()) − Eab ,.\nformoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "energy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct\nofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto\nanotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan\nbethoughtofasan“expert”thatdetermineswhetheraparticularsoftconstraint\nissatisﬁed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly\nalow-dimensionalprojectionoftherandomvariables,butwhencombinedby\nmultiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "dimensionalconstraint.\nOnepartofthedeﬁnitionofanenergy-basedmodelservesnofunctionalpurpose\nfromamachinelearningpointofview:the−signinequation.This16.7 −sign\ncouldbeincorporatedintothedeﬁnitionof E.Formanychoicesofthefunction\nE,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The\n−signispresentprimarilytopreservecompatibilitybetweenthemachinelearning\nliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodeling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "wereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual,\nphysicalenergyanddoesnothavearbitrarysign. Terminologysuchas“energy”\nand“partitionfunction”remainsassociatedwiththesetechniques,eventhough\ntheirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey\nweredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986\nreferredtonegativeenergyasharmony)havechosentoemitthenegation,but\nthisisnotthestandardconvention.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thisisnotthestandardconvention.\nManyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute\np m o de l( x)butonly log ˜ p m o de l( x).Forenergy-basedmodelswithlatentvariables h,\nthesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,\n5 7 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b a s b\n(a) (b)\nFigure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis\nactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres\nisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis\nthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.\ncalledthe :freeenergy\nF − () = x log\nhexp(( )) − E x h , . (16.8)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "F − () = x log\nhexp(( )) − E x h , . (16.8)\nInthisbook,weusuallypreferthemoregeneral log ˜ p m o de l() xformulation.\n1 6 . 2 . 5 S ep a ra t i o n a n d D - S ep a r a t i o n\nTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften\nneedtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions\ncanbeenabledordisabledbyobservingothervariables.Moreformally,wewould\nliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "other,giventhevaluesofothersubsetsofvariables.\nIdentifyingtheconditionalindependencesinagraphisverysimpleinthecase\nofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph\niscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat\nAisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath\ninvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "pathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare\nseparated.Werefertopathsinvolvingonlyunobservedvariablesas“active”and\npathsincludinganobservedvariableas“inactive.”\nWhenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.\nSeeﬁgureforadepictionofhowactiveandinactivepathsinanundirected 16.6\nmodellookwhendrawninthisway.Seeﬁgureforanexampleofreading 16.7\nseparationfromanundirectedgraph.\nSimilar concepts apply todirected models ,except that inthe context of",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "directedmodels,theseconceptsarereferredtoasd-separation.The“d”stands\nfor“dependence.” D-separati onfordirectedgraphsisdeﬁnedthesameasseparation\n5 7 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na\nb c\nd\nFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here\nbisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom\natoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb\nalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.\nTherefore,aanddarenotseparatedgivenb.\nforundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies\nthatisindependentfromgiven. A B S\nAswithundirectedmodels,wecanexaminetheindependencesimpliedbythe\ngraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables\naredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch\npathexists.Indirectednets,determiningwhetherapathisactiveissomewhat\nmorecomplicated. Seeﬁgureforaguidetoidentifyingactivepathsina 16.8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "directedmodel.Seeﬁgureforanexampleofreadingsomepropertiesfroma 16.9\ngraph.\nItisimportanttorememberthatseparationandd-separationtellusonly\naboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h e g r a p h .Thereisno\nrequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,\nitisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)\ntorepresentanydistribution.Infact,somedistributionscontainindependences",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "thatarenotpossibletorepresentwithexistinggraphicalnotation.Context-\nspeciﬁcindependencesareindependencesthatarepresentdependentonthe\nvalueofsomevariablesinthenetwork. Forexample,consideramodelofthree\nbinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,\nbutwhenais1,bisdeterministicallyequaltoc. Encodingthebehaviorwhen\na= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb\nandcareindependentwhena.= 0\nIngeneral,agraphwillneverimplythatanindependenceexistswhenitdoes",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "not.However,agraphmayfailtoencodeanindependence.\n5 7 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b\na s b\na\nsb a s ba s b\nc( a ) ( b )\n( c ) ( d )\nFigure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom\nvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa.\nThiskindofpathbecomesblockedifsisobserved. Wehavealreadyseenthiskindof\npathintherelayraceexample. ( b )aandbareconnectedbya c o m m o n c a u s es.For\nexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaand",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "bmeasurethewindspeedattwodiﬀerentnearbyweathermonitoringoutposts.Ifwe\nobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This\nkindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we\nexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected\nwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing\nthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "pathisactive. ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe\ncollidercase. TheV-structurecausesaandbtoberelatedbytheexplainingaway\neﬀect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose\nsisavariableindicatingthatyourcolleagueisnotatwork. Thevariablearepresents\nherbeingsick,whilebrepresentsherbeingonvacation. Ifyouobservethatsheisnot\natwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "likelythatbothhavehappenedatthesametime.Ifyouﬁndoutthatsheisonvacation,\nthisfactissuﬃcienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n\nsick.Theexplainingawayeﬀecthappensevenifanydescendantof ( d ) sisobserved!For\nexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareport\nfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases\nyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "morelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha\nV-structureistoobservenoneofthedescendantsofthesharedchild.\n5 7 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nc\nd e\nFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples\ninclude:\n•aandbared-separatedgiventheemptyset.\n•aandeared-separatedgivenc.\n•dandeared-separatedgivenc.\nWecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome\nvariables:\n•aandbarenotd-separatedgivenc.\n•aandbarenotd-separatedgivend.\n5 7 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n1 6 . 2 . 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s\nWeoftenrefertoaspeciﬁcmachinelearningmodelasbeingundirectedordirected.\nForexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.\nThischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel\nisinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "usingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.\nDirectedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-\nvantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,\nweshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially\ndependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto\nuseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan\ncapturethemostindependencesintheprobabilitydistributionorwhichapproach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "usesthefewestedgestodescribethedistribution.Thereareotherfactorsthat\ncanaﬀectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle\nprobabilitydistribution,wemaysometimesswitchbetweendiﬀerentmodeling\nlanguages.Sometimesadiﬀerentlanguagebecomesmoreappropriateifweobserve\nacertainsubsetofvariables,orifwewishtoperformadiﬀerentcomputational\ntask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward\napproachtoeﬃcientlydrawsamplesfromthemodel(describedinsection)16.3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "whiletheundirectedmodelformulationisoftenusefulforderivingapproximate\ninferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19\nmodelsishighlightedinequation).19.56\nEveryprobabilitydistributioncanberepresentedbyeitheradirectedmodel\norbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany\ndistributionbyusinga“completegraph.”Inthecaseofadirectedmodel,the\ncompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingon",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "therandomvariables,andeachvariablehasallothervariablesthatprecedeitin\ntheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete\ngraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.\nSeeﬁgureforanexample. 16.10\nOfcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome\nvariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit\ndoesnotimplyanyindependences.\nWhenwerepresentaprobabilitydistributionwithagraph,wewanttochoose",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "agraphthatimpliesasmanyindependencesaspossible,withoutimplyingany\nindependencesthatdonotactuallyexist.\nFromthispointofview,somedistributionscanberepresentedmoreeﬃciently\n5 7 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.\nHereweshowexampleswithfourrandomvariables. ( L e f t )Thecompleteundirectedgraph.\nIntheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph. ( R i g h t )\nInthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe\nvariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "ordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom\nvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.\nusingdirectedmodels,whileotherdistributionscanberepresentedmoreeﬃciently\nusing undirectedmodels.In other words,directed models canencode some\nindependencesthatundirectedmodelscannotencode,andviceversa.\nDirectedmodelsareabletouseonespeciﬁckindofsubstructurethatundirected\nmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "Thestructureoccurswhentworandomvariablesaandbarebothparentsofa\nthirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither\ndirection.(Thename“immorality”mayseemstrange;itwascoinedinthegraphical\nmodelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel\nwithgraph Dintoanundirectedmodel,weneedtocreateanewgraph U. For\neverypairofvariablesxandy,weaddanundirectededgeconnectingxandyto\nUifthereisadirectededge(ineitherdirection)connectingxandyinDorifx",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "andyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa\nmoralizedgraph.Seeﬁgureforexamplesofconvertingdirectedmodelsto 16.11\nundirectedmodelsviamoralization.\nLikewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel\ncanrepresentperfectly.Speciﬁcally,adirectedgraphcannotcaptureallofthe D\nconditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop\noflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "asequenceofvariablesconnectedbyundirectededges,withthelastvariablein\nthesequenceconnectedbacktotheﬁrstvariableinthesequence. Achordisa\nconnectionbetweenanytwonon-consecutivevariablesinthesequencedeﬁninga\nloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese\nloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding\n5 7 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nFigure16.11: Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels\n(bottomrow)byconstructingmoralizedgraphs. ( L e f t )Thissimplechaincanbeconverted\ntoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The\nresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "independences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r )\ntoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely\nofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive\npathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude\nacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab⊥.\n( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing\nedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew\ndirectdependences.\n5 7 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nd ca b\nd ca b\nd c\nFigure16.12:Convertinganundirectedmodeltoadirectedmodel. ( L e f t )Thisundirected\nmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour\nwithnochords.Speciﬁcally,theundirectedmodelencodestwodiﬀerentindependencesthat\nnodirectedmodelcancapturesimultaneously:acbd ⊥|{ ,}andbdac ⊥|{ ,}.To ( C e n t e r )\nconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "ensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither\naddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis\nexample,wechoosetoaddtheedgeconnectingaandc.Toﬁnishtheconversion ( R i g h t )\nprocess,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany\ndirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,\nandalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "thatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose\nalphabeticalorder.\nthesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.\nThegraphformedbyaddingchordstoUisknownasachordalortriangulated\ngraph,becausealltheloopscannowbedescribedintermsofsmaller,triangular\nloops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign\ndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein\nD,ortheresultdoesnotdeﬁneavaliddirectedprobabilisticmodel.Oneway",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "toassigndirectionstotheedgesinDistoimposeanorderingontherandom\nvariables,thenpointeachedgefromthenodethatcomesearlierintheorderingto\nthenodethatcomeslaterintheordering.Seeﬁgureforademonstration. 16.12\n1 6 . 2 . 7 F a ct o r G ra p h s\nFactorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean\nambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In\nanundirectedmodel,thescopeofevery φfunctionmustbeaofsomeclique s u b s e t",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "inthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas\nacorrespondingfactorwhosescopeencompassestheentireclique—forexample,\nacliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,\normaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.\n5 7 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFactorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach φ\nfunction.Speciﬁcally,afactorgraphisagraphicalrepresentationofanundirected\nmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn\nascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected\nmodel. Therestofthenodesaredrawnassquares. Thesenodescorrespondto\nthefactors φoftheunnormalized probabilitydistribution.Variablesandfactors",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "maybeconnectedwithundirectededges.Avariableandafactorareconnected\ninthegraphifandonlyifthevariableisoneoftheargumentstothefactorin\ntheunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother\nfactorinthegraph,norcanavariablebeconnectedtoavariable.Seeﬁgure16.13\nforanexampleofhowfactorgraphscanresolveambiguityintheinterpretation of\nundirectednetworks.\na b\nca b\ncf 1 f 1a b\ncf 1 f 1f 2 f 2\nf 3 f 3\nFigure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "ofundirectednetworks. ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables:\na,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r )\nfactorgraphhasonefactoroverallthreevariables. Anothervalidfactorgraph ( R i g h t )\nforthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo\nvariables.Representation,inference,andlearningareallasymptoticallycheaperinthis\nfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "sameundirectedgraphtorepresent.\n16.3SamplingfromGraphicalModels\nGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.\nOneadvantageofdirectedgraphicalmodelsisthatasimpleandeﬃcientproce-\ndurecalledancestralsamplingcanproduceasamplefromthejointdistribution\nrepresentedbythemodel.\nThebasicideaistosortthevariablesx iinthegraphintoatopologicalordering,\nsothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables\n5 8 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ncanthenbesampledinthisorder.Inotherwords,weﬁrstsamplex 1∼ P(x 1),\nthensample P(x 2| P aG(x 2)),andsoon,untilﬁnallywesample P(x n| P aG(x n)).\nSolongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom,\nthenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation\nguaranteesthatwecanreadtheconditionaldistributionsinequationand16.1\nsamplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "sampleavariablebeforeitsparentsareavailable.\nForsomegraphs,morethanonetopologicalorderingispossible.Ancestral\nsamplingmaybeusedwithanyofthesetopologicalorderings.\nAncestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-\ntionaliseasy)andconvenient.\nOnedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical\nmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling\noperation.Whenwewishtosamplefromasubsetofthevariablesinadirected",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "graphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-\ningvariablescomeearlierthanthevariablestobesampledintheorderedgraph.\nInthiscase,wecansamplefromthelocalconditionalprobabilitydistributions\nspeciﬁedbythemodeldistribution.Otherwise,theconditionaldistributionswe\nneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.\nTheseposteriordistributionsareusuallynotexplicitlyspeciﬁedandparametrized",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "inthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere\nthisisthecase,ancestralsamplingisnolongereﬃcient.\nUnfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We\ncansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis\noftenrequiressolvingintractableinferenceproblems(todeterminethemarginal\ndistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing\nsomanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "fromanundirectedmodelwithoutﬁrstconvertingittoadirectedmodelseemsto\nrequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother\nvariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,\ndrawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass\nprocess.TheconceptuallysimplestapproachisGibbssampling.Supposewe\nhaveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We\niterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "variables,from p(x i|x− i).Duetotheseparationpropertiesofthegraphical\nmodel,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately,\nafterwehavemadeonepassthroughthegraphicalmodelandsampledall n\nvariables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe\n5 8 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nprocessandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors.\nAsymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom\nthecorrectdistribution.Itcanbediﬃculttodeterminewhenthesampleshave\nreachedasuﬃcientlyaccurateapproximationofthedesireddistribution.Sampling\ntechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin\nchapter.17\n16.4AdvantagesofStructuredModeling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "chapter.17\n16.4AdvantagesofStructuredModeling\nTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow\nustodramatically reducethecostofrepresentingprobabilitydistributionsaswell\naslearningandinference.Samplingisalsoacceleratedinthecaseofdirected\nmodels,whilethesituationcanbecomplicatedwithundirectedmodels.The\nprimarymechanismthatallowsalloftheseoperationstouselessruntimeand\nmemoryischoosingtonotmodelcertaininteractions. Graphicalmodelsconvey",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "informationbyleavingedgesout.Anywherethereisnotanedge,themodel\nspeciﬁestheassumptionthatwedonotneedtomodeladirectinteraction.\nAlessquantiﬁablebeneﬁtofusingstructuredprobabilisticmodelsisthat\ntheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof\nknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto\ndevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand\ninferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour\ndata.Wecanthencombinethesediﬀerentalgorithmsandstructuresandobtain\naCartesianproductofdiﬀerentpossibilities.Itwouldbemuchmorediﬃcultto\ndesignend-to-endalgorithmsforeverypossiblesituation.\n16.5LearningaboutDependencies\nAgoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe\nobservedor“visible” variables v.Oftenthediﬀerentelementsofvarehighly\ndependentoneachother.Inthecontextofdeeplearning,theapproachmost",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "commonlyusedtomodelthesedependenciesistointroduceseverallatentor\n“hidden”variables,h.Themodelcanthencapturedependenciesbetweenanypair\nofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and\ndirectdependenciesbetweenandv h j.\nAgoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto\n5 8 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nhaveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge\ncliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis\ncostly—bothinacomputational sense,becausethenumberofparametersthat\nmustbestoredinmemoryscalesexponentiallywiththenumberofmembersina\nclique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters\nrequiresawealthofdatatoestimateaccurately.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "requiresawealthofdatatoestimateaccurately.\nWhenthemodelisintendedtocapturedependenciesbetweenvisiblevariables\nwithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe\ngraphmustbedesignedtoconnectthosevariablesthataretightlycoupledand\nomitedgesbetweenothervariables.Anentireﬁeldofmachinelearningcalled\nstructurelearningisdevotedtothisproblemForagoodreferenceonstructure\nlearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "aformofgreedysearch.Astructureisproposed,amodelwiththatstructure\nistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand\npenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges\naddedorremovedarethenproposedasthenextstepofthesearch.Thesearch\nproceedstoanewstructurethatisexpectedtoincreasethescore.\nUsinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform\ndiscretesearchesandmultipleroundsoftraining.Aﬁxedstructureovervisible",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "andhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits\ntoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter\nlearningtechniqueswecanlearnamodelwithaﬁxedstructurethatimputesthe\nrightstructureonthemarginal . p()v\nLatentvariableshaveadvantagesbeyondtheirroleineﬃcientlycapturing p(v).\nThenewvariables halsoprovideanalternativerepresentationforv.Forexample,\nasdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This\nmeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo\nclassiﬁcation. Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14\ncodinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiﬁer,\norascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,\nbutdeepermodelsandmodelswithdiﬀerentkindsofinteractionscancreateeven\nricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "bylearninglatentvariables.Often,givensomemodelofvandh,experimental\nobservationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor\nv.\n5 8 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n16.6InferenceandApproximateInference\nOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout\nhowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask\nwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto\nextractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed\ntosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels\nusingtheprincipleofmaximumlikelihood.Because",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "usingtheprincipleofmaximumlikelihood.Because\nlog()= p v E h h∼ p (| v )[log( )log( )] p h v ,− p h v| ,(16.9)\nweoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof\ntheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof\nsomevariablesgivenothervariables,orpredicttheprobabilitydistributionover\nsomevariablesgiventhevalueofothervariables.\nUnfortunately,formostinterestingdeepmodels,theseinferenceproblemsare\nintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "graphstructureallowsustorepresentcomplicated,high-dimensionaldistributions\nwithareasonablenumberofparameters,butthegraphsusedfordeeplearningare\nusuallynotrestrictiveenoughtoalsoalloweﬃcientinference.\nItisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral\ngraphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe\ncomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem\nhasasolutionandﬁndingasolutionifoneexists.Problemsin#Prequirecounting",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "thenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat\nwedeﬁneagraphicalmodeloverthebinaryvariablesina3-SATproblem. We\ncanimposeauniformdistributionoverthesevariables.Wecanthenaddone\nbinarylatentvariableperclausethatindicateswhethereachclauseissatisﬁed.\nWecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare\nsatisﬁed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction\ntreeoflatentvariables,witheachnodeinthetreereportingwhethertwoother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "variablesaresatisﬁed.Theleavesofthistreearethevariablesforeachclause.\nTherootofthetreereportswhethertheentireproblemissatisﬁed. Duetothe\nuniformdistributionovertheliterals,themarginaldistributionovertherootofthe\nreductiontreespeciﬁeswhatfractionofassignmentssatisfytheproblem.While\nthisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical\nreal-worldscenarios.\nThismotivatestheuseofapproximate inference.In thecontextof deep",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "learning,thisusuallyreferstovariationalinference,inwhichweapproximate the\n5 8 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntruedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas\nclosetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth\ninchapter.19\n16.7TheDeepLearningApproachtoStructuredProb-\nabilisticModels\nDeeplearningpractitioners generallyusethesamebasiccomputational toolsas\nothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.\nHowever,inthecontextofdeeplearning,weusuallymakediﬀerentdesigndecisions",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "abouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat\nhaveaverydiﬀerentﬂavorfrommoretraditionalgraphicalmodels.\nDeeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe\ncontextofgraphicalmodels,wecandeﬁnethedepthofamodelintermsofthe\ngraphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa\nlatentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved\nvariableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "depthofanysuch h i.Thiskindofdepthisdiﬀerentfromthedepthinducedby\nthecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno\nlatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational\ngraphstodeﬁnetheconditionaldistributionswithinamodel.\nDeeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-\ntations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining\nshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "haveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave\nmorelatentvariablesthanobservedvariables.Complicated nonlinearinteractions\nbetweenvariablesareaccomplishedviaindirectconnectionsthatﬂowthrough\nmultiplelatentvariables.\nBycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat\nareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat\nrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "termsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween\nvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.\nThewaythatlatentvariablesaredesignedalsodiﬀersindeeplearning.The\ndeeplearningpractitionertypicallydoesnotintendforthelatentvariablesto\ntakeonanyspeciﬁcsemanticsaheadoftime—thetrainingalgorithmisfreeto\ninventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare\n5 8 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization\ntechniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When\nlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare\noftendesignedwithsomespeciﬁcsemanticsinmind—thetopicofadocument,\ntheintelligenceofastudent,thediseasecausingapatient’ssymptoms,etc.These\nmodelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "moretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare\nnotreusableinasmanydiﬀerentcontextsasdeepmodels.\nAnotherobviousdiﬀerenceisthekindofconnectivitytypicallyusedinthe\ndeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits\nthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween\ntwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels\nhaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "individuallydesigned.Thedesignofthemodelstructureistightlylinkedwith\nthechoiceofinferencealgorithm.Traditionalapproachestographicalmodels\ntypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint\nistoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled\nloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery\nsparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto\nconnecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "distributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo).\nDistributedrepresentationshavemanyadvantages,butfromthepointofview\nofgraphicalmodelsandcomputational complexity,distributedrepresentations\nhavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor\nthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe\nrelevant.Asaconsequence,oneofthemoststrikingdiﬀerencesbetweenthelarger\ngraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "loopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels\nareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms\neﬃcient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge\nnumberoflatentvariables,makingeﬃcientnumericalcodeessential.Thisprovides\nanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for\ngroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween\ntwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "witheﬃcientmatrixproductoperations,orsparselyconnectedgeneralizations ,like\nblockdiagonalmatrixproductsorconvolutions.\nFinally,thedeeplearningapproachtographicalmodelingischaracterizedby\namarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil\nallquantitieswemightwantcanbecomputedexactly,weincreasethepowerof\n5 8 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels\nwhosemarginaldistributionscannotbecomputed,andaresatisﬁedsimplytodraw\napproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable\nobjectivefunctionthatwecannotevenapproximate inareasonableamountof\ntime,butwearestillabletoapproximately trainthemodelifwecaneﬃciently\nobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "isoftentoﬁgureoutwhattheminimumamountofinformationweabsolutely\nneedis,andthentoﬁgureouthowtogetareasonableapproximation ofthat\ninformationasquicklyaspossible.\n1 6 . 7 . 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e\nTherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium\nisthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.\nTheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "thatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20\nseehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe\nRBMexempliﬁesmanyofthepracticesusedinawidevarietyofdeepgraphical\nmodels: itsunitsareorganizedintolargegroupscalledlayers,theconnectivity\nbetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the\nmodelisdesignedtoalloweﬃcientGibbssampling,andtheemphasisofthemodel\ndesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "werenotspeciﬁedbythedesigner.Later,insection,wewillrevisittheRBM 20.2\ninmoredetail.\nThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden\nunits.Itsenergyfunctionis\nE ,( v h b ) = −v c−h v−W h , (16.10)\nwhere b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan\nseethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction\nbetweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically\ninﬁgure.Asthisﬁguremakesclear,animportantaspectofthismodelis 16.14",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany\ntwohiddenunits(hencethe“restricted,”ageneralBoltzmannmachinemayhave\narbitraryconnections).\nTherestrictionsontheRBMstructureyieldtheniceproperties\np( ) = Π hv| i p(h i|v) (16.11)\n5 8 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\nFigure16.14:AnRBMdrawnasaMarkovnetwork.\nand\np( ) = Π vh| i p(v i|h) . (16.12)\nTheindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM\nweobtain:\nP(h i= 1 ) = |v σ\nvW : , i+ b i\n, (16.13)\nP(h i= 0 ) = 1 |v − σ\nvW : , i+ b i\n. (16.14)\nTogetherthesepropertiesallowforeﬃcientblockGibbssampling,whichalter-\nnatesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin\nﬁgure.16.15\nSincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis\neasytotakeitsderivatives.Forexample,\n∂\n∂ W i , jE ,(vh) = −v ih j . (16.15)\nThesetwoproperties—eﬃcientGibbssamplingandeﬃcientderivatives—make\ntrainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18\ntrainedbycomputingsuchderivativesappliedtosamplesfromthemodel.\nTrainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "E h h∼ p (| v )[] hasasetoffeaturestodescribe. v\nOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-\nicalmodels: representationlearningaccomplishedvialayersoflatentvariables,\ncombinedwitheﬃcientinteractionsbetweenlayersparametrized bymatrices.\nThelanguageofgraphicalmodelsprovidesanelegant,ﬂexibleandclearlanguage\nfordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,\namongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\n5 8 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith\npermissionfrom(). LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn\nusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow\nrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare\nhighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t )",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "thistothesamplesandweightsofalinearfactormodel,showninﬁgure.Thesamples 13.2\nherearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The\nRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,\ntheRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v|\nsothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable\ntohavebothanon-factorialandanon-factorial. p() h p( ) h v|\n5 8 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "Acknowledgments\nThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.\nWewouldliketothankthosewhocommentedonourproposalforthebook\nandhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,\nÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\nRohée.\nWewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthe\nbookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,Guillaume\nAlain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBiçici,Matko",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Bošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLuc\nCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent\nDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,\nNando deFreitas,Çağlar Gülçehre, Jurgen V anGael,JavierAlonso García,\nJonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,\nAsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf\nMathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,Roman",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Novak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,\nRousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,\nHalisSak, CésarSalgado,GrigorySapunov,YoshinoriSasaki, MikeSchuster,\nJulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,David\nSussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,\nMassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent\nVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Webb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.\nWewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\nindividualchapters:\n•Notation:ZhangYuanhang.\n•Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\nviii",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescu\nandAlfredoSolano.\n•Chapter, :AmjadAlmahairi,NikolaBanić,KevinBennett, 2LinearAlgebra\nPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\nSergeyOreshkov,IstvánPetrás,DennisPrangle,ThomasRohée,Gitanjali\nGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.\n•Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\nArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "AnttiRasmus,AlexeySurkovandVolkerTresp.\n•Chapter ,  :Tran LamAnIan Fischer andHu 4NumericalComputation\nYuhuang.\n•Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu.\n•Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\nElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKrueger\nandAdityaKumarPraharaj.",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "andAdityaKumarPraharaj.\n•Chapter, :MortenKolbæk,KshitijLauria, 7RegularizationforDeepLearning\nInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury.\n•Chapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter\nArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,\nKashifRasul,KlausStroblandNicholasTurner.\n•Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-\nstantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan\nStoutandWentaoWu.",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "StoutandWentaoWu.\n•Chapter,10SequenceModeling:RecurrentandRecursiveNets:Gökçen\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\nDmitriySerdyuk,DongyuShiandKaiyuYang.\n•Chapter, :DanielBeckstein. 11PracticalMethodology\n•Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\nRoscher.\n•Chapter,13LinearFactorModels:JayanthKoushik.\ni x",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n•Chapter, :KunalGhosh. 15RepresentationLearning\n•Chapter, : MinhLê 16StructuredProbabilisticModelsforDeepLearning\nandAntonVarfolom.\n•Chapter,18ConfrontingthePartitionFunction:SamBowman.\n•Chapter, :YujiaBao. 19ApproximateInference\n•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,\nWenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon.\n•Bibliography:LukasMichelbacherandLeslieN.Smith.\nWealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresor",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "datafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptions\nthroughoutthetext.\nWewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto\nmakethewebversionofthebook,andforoﬀeringsupporttoimprovethequality\noftheresultingHTML.\nWe would liketothank Ian’swifeDaniela FloriGoodfellowforpatiently\nsupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.\nWewouldliketothanktheGoogleBrainteamforprovidinganintellectual",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "environmentwhereIancoulddevoteatremendousamountoftimetowritingthis\nbookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\ntothankIan’sformermanager,GregCorrado,andhiscurrentmanager,Samy\nBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀrey\nHintonforencouragement whenwritingwasdiﬃcult.\nx",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 4\nNumericalComputation\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\nlution.Commonoperationsincludeoptimization (ﬁndingthevalueofanargument\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Evenjustevaluatingamathematical functiononadigitalcomputercanbediﬃcult\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\nusingaﬁniteamountofmemory.\n4. 1 O v erﬂ o w an d Un d erﬂ o w\nThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputer\nisthatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumber\nofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursome\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "cases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\nroundingerror.\nOneformofroundingerrorthatisparticularlydevastatingis under ﬂo w.\nUnderﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\nbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmall\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\n80",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\nisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformany\nfurtherarithmeticoperations).\nAnotherhighlydamagingformofnumericalerroris o v e r ﬂo w.Overﬂowoccurs\nwhennumberswithlargemagnitudeareapproximatedas∞or−∞.Further\narithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "Oneexampleofafunctionthatmustbestabilizedagainstunderﬂowand\noverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\ndeﬁnedtobe\nsoftmax() x i=exp( x i)n\nj = 1exp( x j). (4.1)\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\nwecanseethatalloftheoutputsshouldbeequalto1\nn.Numerically,thismay\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "underﬂow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheﬁnal\nresultisundeﬁned.When cisverylargeandpositive,exp( c)willoverﬂow,again\nresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃculties\ncanberesolvedbyinsteadevaluating softmax( z)where z= x−max i x i.Simple\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\nthepossibilityofunderﬂowinthedenominator leadingtoadivisionbyzero.\nThereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcause\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\nlogsoftmax( x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresultto\nthelogfunction,wecoulderroneouslyobtain −∞.Instead,wemustimplement\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "logsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\nthefunction. softmax\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "toimplementanewalgorithmandhavethenewimplementation automatically\n8 1",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nstabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012\nofasoftwarepackagethatautomatically detectsandstabilizesmanycommon\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.\n4. 2 P o or C on d i t i o n i n g\nConditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges\ninitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly\ncanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputs",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "canresultinlargechangesintheoutput.\nConsiderthefunction f( x)= A− 1x.When A∈ Rn n ×hasaneigenvalue\ndecomposition,its c o ndi t i o n num beris\nmax\ni , jλ i\nλ j. (4.2)\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\nThissensitivityisanintrinsicpropertyofthematrixitself,nottheresult\nofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "pre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\n4. 3 Gradi en t - Bas e d O p t i m i z a t i o n\nMostdeeplearningalgorithmsinvolveoptimization ofsomesort. Optimization\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\nx. Weusuallyphrasemostoptimization problemsintermsofminimizing f( x).\nMaximization maybeaccomplishedviaaminimization algorithmbyminimizing\n− f() x.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "− f() x.\nThefunctionwewanttominimizeormaximizeiscalledthe o b j e c t i v e f unc -\nt i o nor c r i t e r i o n.Whenweareminimizingit, wemayalsocallitthe c o st\nf unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n. Inthisbook,weusetheseterms\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\ntosomeoftheseterms.\nWeoftendenotethevaluethatminimizesormaximizesafunctionwitha\nsuperscript.Forexample,wemightsay ∗ x∗= argmin() f x.\n8 2",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n− − − − 20. 15. 10. 05 00 05 10 15 20 ......\nx−20.−15.−10.−05.00.05.10.15.20.\nGlobalminimumat= 0.x\nSincef() = 0,gradient x\ndescent haltshere.\nFor 0,wehave x< f() 0,x<\nsowecandecreasebyf\nmoving rightward.For 0,wehave x> f() 0,x>\nsowecandecreasebyf\nmoving leftward.\nf x() =1\n2x2\nf() = x x\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\nreviewofhowcalculusconceptsrelatetooptimization here.\nSupposewehaveafunction y= f( x),whereboth xand yarerealnumbers.\nThe der i v at i v eofthisfunctionisdenotedas f( x)orasd y\nd x.Thederivative f( x)\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspeciﬁeshowtoscale\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\noutput: f x  f x  f (+) ≈()+() x.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "output: f x  f x  f (+) ≈()+() x.\nThederivativeisthereforeusefulforminimizingafunctionbecauseittells\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\nweknowthat f( x −sign( f( x)))islessthan f( x)forsmallenough .Wecan\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative.\nThistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeﬁgureforan4.1\nexampleofthistechnique.\nWhen f( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "tomove.Pointswhere f( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\npoints,soitisnolongerpossibletodecrease f( x)bymakinginﬁnitesimalsteps.\nA l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\n8 3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nMinimum Maximum Saddlepoint\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.\nsoitisnotpossibletoincrease f( x)bymakinginﬁnitesimalsteps.Somecritical",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "pointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\nﬁgureforexamplesofeachtypeofcriticalpoint. 4.2\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um.\nItispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "veryﬂatregions.Allofthismakesoptimization verydiﬃcult,especiallywhenthe\ninputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndinga\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\nﬁgureforanexample.4.3\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rn→ R.Forthe\nconceptof“minimization” to makesense,theremuststillbeonlyone(scalar)\noutput.\nForfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\nder i v at i v e s.Thepartialderivative∂",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "der i v at i v e s.Thepartialderivative∂\n∂ x if( x)measureshow fchangesasonlythe\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\nvectorcontainingallofthepartialderivatives,denoted ∇ x f( x).Element iofthe\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\n8 4",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxf x()\nIdeally,wewouldlike\ntoarriveattheglobal\nminimum, butthis\nmight notbepossible.Thislocalminimum\nperformsnearlyaswellas\ntheglobalone,\nsoitisanacceptable\nhaltingpoint.\nThislocalminimumperforms\npoorlyandshouldbeavoided.\nFigure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhenthereare\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "tosigniﬁcantlylowvaluesofthecostfunction.\ncriticalpointsarepointswhereeveryelementofthegradientisequaltozero.\nThe di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\nofthefunction f( x+ α u)withrespectto α,evaluatedat α= 0.Usingthechain\nrule,wecanseethat∂\n∂ αf α (+ x u)evaluatesto u∇ x f α () xwhen = 0.\nTominimize f,wewouldliketoﬁndthedirectioninwhich fdecreasesthe",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "fastest.Wecandothisusingthedirectionalderivative:\nmin\nu u , u = 1u∇ x f() x (4.3)\n=min\nu u , u = 1|||| u 2||∇ x f() x|| 2cos θ (4.4)\nwhere θistheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\nignoringfactorsthatdonotdependon u,thissimpliﬁestomin ucos θ.Thisis\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "Thisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\nSteepestdescentproposesanewpoint\nx= x−∇  x f() x (4.5)\n8 5",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nwhere isthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep.\nWecanchoose inseveraldiﬀerentways.Apopularapproachistoset toasmall\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\nderivativevanish.Anotherapproachistoevaluate f  ( x−∇ x f()) xforseveral\nvaluesof andchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\nThislaststrategyiscalleda l i ne se ar c h.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "Thislaststrategyiscalleda l i ne se ar c h.\nSteepestdescentconvergeswheneveryelementofthegradientiszero(or,in\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\nequation ∇ x f() = 0 xfor. x\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\nsmallmove)towardsbetterconﬁgurations canbegeneralizedtodiscretespaces.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Ascendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\n( ,). RusselandNorvig2003\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\nSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinput\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\nknownasa J ac o bi an m at r i x.Speciﬁcally,ifwehaveafunction f: Rm→ Rn,\nthentheJacobianmatrix J∈ Rn m ×ofisdeﬁnedsuchthat f J i , j=∂\n∂ x jf() x i.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "∂ x jf() x i.\nWearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rn→ R,thederivative\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedas∂2\n∂ x i ∂ x jf.\nInasingledimension,wecandenoted2\nd x2 fby f ( x).Thesecondderivativetells\nushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportant\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "aswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\nthenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredicted\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \nalongthenegativegradient,andthecostfunctionwilldecreaseby .Ifthesecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "derivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\nactuallydecreasebymorethan .Finally,ifthesecondderivativeispositive,the\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan .See\n8 6",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxf x()N e g a t i v e c u r v a t u r e\nxf x()N o c u r v a t u r e\nxf x()P o s i t i v e c u r v a t u r e\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "thanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\nfunctioninadvertently.\nﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween 4.4\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue.\nWhenourfunctionhasmultipleinputdimensions,therearemanysecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "derivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\nHessian m at r i x.TheHessianmatrix isdeﬁnedsuchthat H x()( f)\nH x()( f) i , j=∂2\n∂ x i ∂ x jf .() x (4.6)\nEquivalently,theHessianistheJacobianofthegradient.\nAnywherethatthesecondpartialderivativesarecontinuous,thediﬀerential\noperatorsarecommutative,i.e.theirordercanbeswapped:\n∂2\n∂ x i ∂ x jf() = x∂2\n∂ x j ∂ x if .() x (4.7)\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\nHessianalmosteverywhere. Because theHessianmatrixisrealandsymmetric,\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\n8 7",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\neigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunit\nvector disgivenby dH d.When disaneigenvectorof H,thesecondderivative\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "derivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.\nThe(directional)secondderivativetellsushowwellwecanexpectagradient\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\ntothefunction aroundthecurrentpoint f() x x( 0 ):\nf f () x≈( x( 0 ))+( x x−( 0 ))g+1\n2( x x−( 0 ))H x x (−( 0 )) .(4.8)\nwhere gisthegradientand HistheHessianat x( 0 ). Ifweusealearningrate\nof ,thenthenewpoint xwillbegivenby x( 0 )−  g.Substitutingthisintoour\napproximation,weobtain",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "approximation,weobtain\nf( x( 0 )− ≈  g) f( x( 0 ))−  gg+1\n22gH g . (4.9)\nTherearethree termshere:theoriginalvalue ofthefunction, the expected\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\ngradientdescentstepcanactuallymoveuphill.When gH giszeroornegative,\ntheTaylorseriesapproximationpredictsthatincreasing foreverwilldecrease f\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge ,so",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "onemustresorttomoreheuristicchoicesof inthiscase.When gH gispositive,\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\nthefunctionthemostyields\n∗=gg\ngH g. (4.10)\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\nmaximaleigenvalue λ m a x,thenthisoptimalstepsizeisgivenby1\nλmax.Tothe\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\nrate.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "rate.\nThesecondderivativecanbeusedtodeterminewhetheracriticalpointis\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\npoint, f( x) = 0.Whenthesecondderivative f ( x) >0,theﬁrstderivative f( x)\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\n8 8",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nf( x −) <0and f( x+ ) >0forsmallenough .Inotherwords,aswemove\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\nbeginstopointuphilltotheleft. Thus,when f( x)=0and f ( x) >0,wecan\nconcludethat xisalocalminimum.Similarly,when f( x) = 0and f ( x) <0,we\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\nt e st.Unfortunately,when f ( x) = 0,thetestisinconclusive.Inthiscase xmay",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "beasaddlepoint,orapartofaﬂatregion.\nInmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\n∇ x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\nHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocal",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "minimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\nderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvalues\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\npossibletoﬁndpositiveevidenceofsaddlepointsinsomecases. Whenatleast\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "crosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond 4.5\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\nInmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirection\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "howmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasa\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\nremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.\nThestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "uphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\nstepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithless\ncurvature.Seeﬁgureforanexample.4.6\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\n8 9",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n\u0000   \u0000   \n\u0000    \nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\ninthisexampleis f( x)= x2\n1− x2\n2.Alongtheaxiscorrespondingto x 1,thefunction\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\nAlongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\neigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfrom",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "thesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\n9 0",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n− − − 3 0 2 0 1 0 0 1 0 2 0\nx 1− 3 0− 2 0− 1 001 02 0x 2\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\nhasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\ncurvatureisinthedirection[1 ,1]andtheleastcurvatureisinthedirection[1 ,−1].The",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "redlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\ndirectioninthiscontext.\n9 1",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o n’ s m e t ho d.\nNewton’smethodisbasedonusingasecond-orderTaylorseriesexpansionto\napproximatenearsomepoint f() x x( 0 ):\nf f () x≈( x( 0 ))+( x x−( 0 ))∇ x f( x( 0 ))+1\n2( x x−( 0 ))H x()( f( 0 ))( x x−( 0 )) .(4.11)\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\nx∗= x( 0 )− H x()( f( 0 ))− 1∇ x f( x( 0 )) . (4.12)\nWhen fisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsof",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "applyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\ndeﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12\ntimes. Iterativelyupdatingtheapproximation andjumpingtotheminimumof\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\npropertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis 8.2.3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\npointsunlessthegradientpointstowardthem.\nOptimization algorithmsthatuseonlythegradient,suchasgradientdescent,\narecalled ﬁr st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\nalsousetheHessianmatrix,suchasNewton’smethod,arecalled se c o nd-or d e r\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "The optimization algorithms employedin mostcontextsin this book are\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.\nDeeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\nusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominant\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\noffunctions.\nInthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "ingourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\nofchangeisboundedbya L i psc hi t z c o nst antL:\n∀∀| − |≤L||−|| x , y , f() x f() y x y 2 . (4.13)\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\n9 2",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\nwithrelativelyminormodiﬁcations.\nPerhapsthemostsuccessfulﬁeldofspecializedoptimization is c o n v e x o p-\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\napplicableonlytoconvexfunctions—functionsforwhichtheHessianispositive",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "semideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\nproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization.\nConvexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.\nIdeasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "moreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\norRockafellar1997().\n4. 4 C on s t ra i n ed O p t i m i z a t i o n\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\npossible values of x.Insteadwemay wishto ﬁnd themaximal or minimal\nvalue of f( x)for valuesof xinsome set S.Thisis known as c o nst r ai n e d\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\nconstrainedoptimization terminology.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "constrainedoptimization terminology.\nWeoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon\napproachinsuchsituationsistoimposeanormconstraint,suchas. ||||≤ x 1\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize ,\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\nalinesearch,wecansearchonlyoverstepsizes thatyieldnew xpointsthatare",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "feasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\nWhenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradient\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\nthelinesearch(,).Rosen1960\nAmoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\n9 3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nx∈ R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\ng( θ) = f([cossin θ , θ])withrespectto θ,thenreturn[cossin θ , θ]asthesolution\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\nbetweenoptimization problemsmustbedesignedspeciﬁcallyforeachcasewe\nencounter.\nThe K ar ush– K u h n – T uc k e r(KKT)approach1providesaverygeneralso-\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "newfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\nf unc t i o n.\nTodeﬁnetheLagrangian,weﬁrstneedtodescribe Sintermsofequations\nandinequalities. W ewantadescriptionof Sintermsof mfunctions g( ) iand n\nfunctions h( ) jsothat S={|∀ x i , g( ) i( x) = 0and∀ j , h( ) j( x)≤0}.Theequations\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "Weintroducenewvariables λ iand α jforeachconstraint,thesearecalledthe\nKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedas\nL , , f ( x λ α) = ()+ x\niλ i g( ) i()+ x\njα j h( ) j() x .(4.14)\nWecannowsolveaconstrainedminimization problemusingunconstrained\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x ∞\nmin\nxmax\nλmax\nα α , ≥ 0L , , . ( x λ α) (4.15)\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "min\nx ∈ Sf .() x (4.16)\nThisfollowsbecauseanytimetheconstraintsaresatisﬁed,\nmax\nλmax\nα α , ≥ 0L , , f , ( x λ α) = () x (4.17)\nwhileanytimeaconstraintisviolated,\nmax\nλmax\nα α , ≥ 0L , , . ( x λ α) = ∞ (4.18)\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s .\n9 4",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\noptimumwithinthefeasiblepointsisunchanged.\nToperformconstrainedmaximization, wecanconstructthegeneralizedLa-\ngrangefunctionof,whichleadstothisoptimization problem: − f() x\nmin\nxmax\nλmax\nα α , ≥ 0− f()+ x\niλ i g( ) i()+ x\njα j h( ) j() x .(4.19)\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\nmax\nxmin\nλmin\nα α , ≥ 0f()+ x\niλ i g( ) i() x−\njα j h( ) j() x .(4.20)",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "iλ i g( ) i() x−\njα j h( ) j() x .(4.20)\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneit\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\nanysignforeach λ i.\nTheinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\nh( ) i( x)is ac t i v eif h( ) i( x∗) = 0.Ifaconstraintisnotactive,thenthesolutionto\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "othersolutions.Forexample,aconvexproblemwithanentireregionofglobally\noptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthis\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\nthesolutiontomin xmax λmax α α , ≥ 0 L( x λ α , ,)willhave α i=0.Wecanthus",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "observethatatthesolution, α h( x)= 0.Inotherwords,forall i,weknow\nthatatleastoneoftheconstraints α i≥0and h( ) i( x)≤0mustbeactiveatthe\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\ntoinﬂuencethesolutionto x,ortheinequalityhasnoinﬂuenceonthesolution\nandwerepresentthisbyzeroingoutitsKKTmultiplier.\nAsimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\nbutnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:\n•ThegradientofthegeneralizedLagrangianiszero.\n•AllconstraintsonbothandtheKKTmultipliersaresatisﬁed. x\n9 5",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n•Theinequalityconstraintsexhibit“complementary slackness”: α h( x) = 0.\nFormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\n4. 5 E x am p l e: L i n ear L eas t S q u are s\nSupposewewanttoﬁndthevalueofthatminimizes x\nf() = x1\n2||−|| A x b2\n2 . (4.21)\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently.\nHowever,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\nasimpleexampleofhowthesetechniqueswork.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "asimpleexampleofhowthesetechniqueswork.\nFirst,weneedtoobtainthegradient:\n∇ x f() = x A( ) = A x b− AA x A−b . (4.22)\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\nfordetails.\nAl g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\n2||−|| A x b2\n2withrespectto x\nusinggradientdescent,startingfromanarbitraryvalueof. x\nSetthestepsize()andtolerance()tosmall,positivenumbers.  δ\nwhi l e|| AA x A−b|| 2 > δ do\nx x← − \nAA x A−b\ne nd whi l e",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "x x← − \nAA x A−b\ne nd whi l e\nOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,because\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewton’s\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\nstep.\nNowsuppose we wishto minimizethesame function,butsubjectto the\nconstraint xx≤1.Todoso,weintroducetheLagrangian\nL , λ f λ ( x) = ()+ x\nxx−1\n. (4.23)\nWecannowsolvetheproblem\nmin\nxmax\nλ , λ ≥ 0L , λ . ( x) (4.24)\n9 6",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda\nsolutionwheretheconstraintisactive.Bydiﬀerentiating theLagrangianwith\nrespectto,weobtaintheequation x\nAA x A−b x+2 λ= 0 . (4.25)\nThistellsusthatthesolutionwilltaketheform\nx A= (A I+2 λ)− 1Ab . (4.26)",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "x A= (A I+2 λ)− 1Ab . (4.26)\nThemagnitudeof λmustbechosensuchthattheresultobeystheconstraint.We\ncanﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ\n∂\n∂ λL , λ( x) = xx−1 . (4.27)\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\nuphillandincreasetheLagrangianwithrespectto λ,weincrease λ.Becausethe\ncoeﬃcientonthe xxpenaltyhasincreased,solvingthelinearequationfor xwill\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "andadjusting λcontinuesuntil xhasthecorrectnormandthederivativeon λis\n0.\nThisconcludesthemathematical preliminaries thatweusetodevelopmachine\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged\nlearningsystems.\n9 7",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "P a rt I I\nD e e p N e t w orks: Mo d e rn\nPractices\n166",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Thispartofthebooksummarizesthestateofmoderndeeplearningasitis\nusedtosolvepracticalapplications.\nDeeplearninghasalonghistoryandmanyaspirations.Severalapproaches\nhavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals\nhaveyettoberealized.Theseless-developedbranchesofdeeplearningappearin\ntheﬁnalpartofthebook.\nThispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\nnologiesthatarealreadyusedheavilyinindustry.",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "nologiesthatarealreadyusedheavilyinindustry.\nModern deeplearning provides avery powerful framework forsupervised\nlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan\nrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan\ninputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can\nbeaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃciently\nlargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "asassociatingonevectortoanother,orthatarediﬃcultenoughthataperson\nwouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remain\nbeyondthescopeofdeeplearningfornow.\nThispartofthebookdescribesthecoreparametricfunctionapproximation\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.\nWe begin by describingthe feedforward deepnetworkmodelthatisusedto\nrepresentthesefunctions.Next,wepresentadvancedtechniquesforregularization",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "andoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh\nresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\ntheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\nnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\nforthepracticalmethodologyinvolvedindesigning,building,andconﬁguringan\napplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep\nlearning.",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "learning.\nThesechaptersarethemostimportantforapractitioner—someone whowants\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\nproblemstoday.\n1 6 7",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 1\nPractical Methodology\nSuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\nknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\nwork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\nalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\nobtainedfromexperimentsinordertoimproveamachinelearningsystem.During\ndaytodaydevelopmentofmachinelearningsystems,practitioners needtodecide",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "whethertogathermoredata,increaseordecreasemodelcapacity,addorremove\nregularizingfeatures,improvetheoptimization ofamodel,improveapproximate\ninferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof\ntheseoperationsareattheveryleasttime-consuming totryout,soitisimportant\ntobeabletodeterminetherightcourseofactionratherthanblindlyguessing.\nMostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-\nrithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "importantingredienttobeingamachinelearningexpertisknowingawidevariety\nofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-\ntice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\nalgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\nanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\nrecommendations inthischapterareadaptedfrom().Ng2015\nWerecommendthefollowingpracticaldesignprocess:",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Werecommendthefollowingpracticaldesignprocess:\n•Determineyourgoals—whaterrormetrictouse,andyourtargetvaluefor\nthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\nproblemthattheapplicationisintendedtosolve.\n•Establishaworkingend-to-endpipelineassoonaspossible,includingthe\n421",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nestimationoftheappropriateperformancemetrics.\n•Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\nnosewhichcomponentsareperformingworsethanexpectedandwhetherit\nisduetooverﬁtting,underﬁtting, oradefectinthedataorsoftware.\n•Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\nhyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfrom\nyourinstrumentation.\nAsarunningexample,wewilluseStreetViewaddressnumbertranscription",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "system( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d\nbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\ntheGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork\nrecognizestheaddressnumberineachphotograph, allowingtheGoogleMaps\ndatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\ncommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\nmethodologyweadvocate.\nWenowdescribeeachofthestepsinthisprocess.\n11.1PerformanceMetrics",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "11.1PerformanceMetrics\nDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrst\nstepbecauseyourerrormetricwillguideallofyourfutureactions. Youshould\nalsohaveanideaofwhatlevelofperformanceyoudesire.\nKeepinmindthatformostapplications,itisimpossibletoachieveabsolute\nzeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopeto\nachieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobability\ndistribution.This isbecause your inputfeatures maynot contain complete",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "informationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\nstochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata.\nTheamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\ngoalistobuildthebestpossiblereal-worldproductorservice,youcantypically\ncollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\nthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\nmoney,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolves",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "performinginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestion\naboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark\n4 2 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nspeciﬁcationusuallydeterminesthetrainingsetandyouarenotallowedtocollect\nmoredata.\nHowcanonedetermineareasonablelevelofperformancetoexpect?Typically,\nintheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\nbasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\nhavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\ncost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "desirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.\nAnotherimportantconsiderationbesidesthetargetvalueoftheperformance\nmetricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetrics\nmaybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludes\nmachinelearningcomponents.Theseperformancemetricsareusuallydiﬀerent\nfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2\ncommontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "However,manyapplicationsrequiremoreadvancedmetrics.\nSometimesitismuchmorecostlytomakeonekindofamistakethananother.\nForexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\nincorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\nspammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\nmessagethantoallowaquestionablemessagetopassthrough.Ratherthan\nmeasuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeform",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "oftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\nofallowingspammessages.\nSometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsome\nrareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\nthatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\n99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiﬁer\ntoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\ncharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "toinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections\nreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\nthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\nperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\nwouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\nhavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina\nmillionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "withprecisiononthe y-axisandrecallonthe x-axis.Theclassiﬁergeneratesascore\nthatishigheriftheeventtobedetectedoccurred. Forexample,afeedforward\n4 2 3",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nnetworkdesignedtodetectadiseaseoutputs ˆ y= P( y=1| x),estimatingthe\nprobabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\nthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\nthreshold. Byvaryingthethreshold,wecantradeprecisionforrecall. Inmany\ncases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumber\nratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan\nF-scor egivenby\nF=2 pr\np r+. (11.1)",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "F-scor egivenby\nF=2 pr\np r+. (11.1)\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve.\nInsomeapplications,itispossibleforthemachinelearningsystemtorefuseto\nmakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\nhowconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncan\nbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\nViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "transcribetheaddressnumberfromaphotographinordertoassociatethelocation\nwherethephotowastakenwiththecorrectaddressinamap.Becausethevalue\nofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd\nanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem\nthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,\nthenthebestcourseofactionistoallowahumantotranscribethephotoinstead.\nOfcourse,themachinelearningsystemisonlyusefulifitisabletodramatically",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "reducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural\nperformancemetrictouseinthissituationis c o v e r age.Coverageisthefraction\nofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.\nItispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy\nbyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe\nStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\naccuracywhilemaintaining95%coverage.Human-levelperformanceonthistask\nis98%accuracy.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "is98%accuracy.\nManyothermetricsarepossible.Wecanforexample,measureclick-through\nrates,collectusersatisfactionsurveys,andsoon. Manyspecializedapplication\nareashaveapplication-speciﬁccriteriaaswell.\nWhatisimportantistodeterminewhichperformancemetrictoimproveahead\noftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,\nitcanbediﬃculttotellwhetherchangestoamachinelearningsystemmake\nprogressornot.\n4 2 4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\n11.2DefaultBaselineModels\nAfterchoosingperformancemetricsandgoals, thenextstepinanypractical\napplicationistoestablishareasonableend-to-endsystemassoonaspossible.In\nthissection,weproviderecommendations forwhichalgorithmstouseastheﬁrst\nbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\nprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\nafterthiswriting.\nDependingonthecomplexityofyourproblem,youmayevenwanttobegin",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "withoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\njustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\nstatisticalmodellikelogisticregression.\nIfyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobject\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\ntodowellbybeginningwithanappropriatedeeplearningmodel.\nFirst,choosethegeneralcategoryofmodelbasedonthestructureofyour",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "data.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,\nuseafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\ntopologicalstructure(forexample,iftheinputisanimage),useaconvolutional\nnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\nunit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If\nyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).\nAreasonablechoiceofoptimization algorithmisSGDwithmomentumwitha",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "decayinglearningrate(populardecayschemesthatperformbetterorworseon\ndiﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimumlearning\nrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10\neachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.\nBatchnormalization canhaveadramaticeﬀectonoptimization performance,\nespeciallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.\nWhileitisreasonabletoomitbatchnormalization fromtheveryﬁrstbaseline,it",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "shouldbeintroducedquicklyifoptimization appearstobeproblematic.\nUnlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\nshouldincludesomemildformsofregularizationfromthestart.Earlystopping\nshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\ntoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\nnormalization alsosometimesreducesgeneralization errorandallowsdropoutto\nbeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize\neachvariable.\n4 2 5",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\nwillprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalready\nknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\natrainedmodelfromthattask.Forexample,itiscommontousethefeatures\nfromaconvolutionalnetworktrainedonImageNettosolveothercomputervision\ntasks( ,). Girshicketal.2015\nAcommonquestioniswhethertobeginbyusingunsupervisedlearning,de-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "scribedfurtherinpart.Thisissomewhatdomainspeciﬁc.Somedomains,such III\nasnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-\nvisedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother\ndomains,suchascomputervision,currentunsupervisedlearningtechniquesdo\nnotbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberof\nlabeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour\napplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "thenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervised\nlearninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.You\ncanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\nbaselineoverﬁts.\n11.3DeterminingWhethertoGatherMoreData\nAftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-\nmanceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\nnovicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "However,itisoftenmuchbettertogathermoredatathantoimprovethelearning\nalgorithm.\nHowdoesonedecidewhethertogathermoredata?First,determinewhether\ntheperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\nsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\navailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\nsizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.\nAlso,tryimprovingthelearningalgorithm,forexamplebytuningthelearning",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "ratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms\ndonotworkwell,thentheproblemmightbetheofthetrainingdata.The quality\ndatamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\ndesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga\nrichersetoffeatures.\nIftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\n4 2 6",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\nthenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\ntrainingsetperformance,thengatheringmoredataisoneofthemosteﬀective\nsolutions. Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\ndata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\namountofdatathatisexpectedtobenecessarytoimprovetestsetperformance",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "signiﬁcantly. Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\nfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\nlessthantheotheralternatives,sotheanswerisalmostalwaystogathermore\ntrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\nthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\nmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\nalternativetogatheringmoredataistoreducethesizeofthemodelorimprove",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "regularization, byadjustinghyperparameters suchasweightdecaycoeﬃcients,\norbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegap\nbetweentrainandtestperformanceisstillunacceptable evenaftertuningthe\nregularizationhyperparameters ,thengatheringmoredataisadvisable.\nWhendecidingwhethertogathermoredata,itisalsonecessarytodecide\nhowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\ntrainingsetsizeandgeneralization error,likeinﬁgure.Byextrapolatingsuch 5.4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\nachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\nnumberofexampleswillnothaveanoticeableimpactongeneralization error.Itis\nthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,\nforexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.\nIfgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "domainofresearchandnotthedomainofadviceforappliedpractitioners.\n11.4SelectingHyperparameters\nMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany\naspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthetime\nandmemorycostofrunningthealgorithm.Someofthesehyperparameters aﬀect\nthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer\ncorrectresultswhendeployedonnewinputs.\nTherearetwobasicapproachestochoosingthesehyperparameters :choosing",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "themmanuallyandchoosingthemautomatically .Choosingthehyperparameters\n4 2 7",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\nlearningmodelsachievegoodgeneralization. Automatichyperparameterselection\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\nmuchmorecomputationally costly.\n1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g\nTosethyperparameters manually,onemustunderstandtherelationshipbetween\nhyperparameters,trainingerror,generalization errorandcomputational resources",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-\ndamentalideasconcerningtheeﬀectivecapacityofalearningalgorithmfrom\nchapter.5\nThegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-\nizationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow\ntodeterminetheruntimeandmemoryimpactofvarioushyperparametershere\nbecausethisishighlyplatform-dependent.\nTheprimarygoalofmanualhyperparametersearchistoadjusttheeﬀective",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "capacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacity\nisconstrainedbythreefactors: therepresentationalcapacityofthemodel,the\nabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\ntrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\nregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\nhigherrepresentationalcapacity—itiscapableofrepresentingmorecomplicated\nfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "thetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof\nminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid\nsomeofthesefunctions.\nThegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\nafunctionofoneofthehyperparameters ,asinﬁgure. Atoneextreme,the 5.3\nhyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh\nbecausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "thehyperparameter valuecorrespondstohighcapacity,andthegeneralization\nerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\ninthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\ngeneralization error,byaddingamediumgeneralization gaptoamediumamount\noftrainingerror.\nForsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-\nparameterislarge. Thenumberofhiddenunitsinalayerisonesuchexample,\n4 2 8",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.\nForsomehyperparameters ,overﬁttingoccurswhenthevalueofthehyperparame-\nterissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzero\ncorrespondstothegreatesteﬀectivecapacityofthelearningalgorithm.\nNoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.\nManyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "numberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\nalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\nareswitchesthat specify whetherornotto usesomeoptionalcomponentof\nthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\nfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.These\nhyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters\nhavesomeminimumormaximumvaluethatpreventsthemfromexploringsome",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "partofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.This\nmeansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotenter\ntheoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,\nsomehyperparameters canonlysubtractcapacity.\nThelearningrateisperhapsthemostimportanthyperparameter. Ifyou\nhave timeto tuneonly onehyperparameter, tune thelearning rate. It con-\ntrolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanother",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearning\nrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\nlargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\nillustratedinﬁgure.Whenthelearningrateistoolarge,gradientdescent 11.1\ncaninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\nquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "optimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\nisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.\nThiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).\nTuningtheparametersotherthanthelearningraterequiresmonitoringboth\ntrainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,\nthenadjustingitscapacityappropriately .\nIfyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "nochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\nconﬁdentthatyouroptimization algorithmisperformingcorrectly,thenyoumust\naddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\nincreasesthecomputational costsassociatedwiththemodel.\nIfyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan\n4 2 9",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\n1 0− 21 0− 11 00\nL e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\nthesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxed\ntrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya\nfactorproportionaltothelearningratereduction. Generalizationerrorcanfollowthis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "curveorbecomplicatedbyregularizationeﬀectsarisingoutofhavingatoolargeor\ntoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent\noverﬁtting,andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralization\nerror.\nnowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand\nthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\noﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetraining",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "errorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\ndrivenbythegapbetweentrainandtesterror. Yourgoalistoreducethisgap\nwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\nchangeregularizationhyperparameters toreduceeﬀectivemodelcapacity,suchas\nbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\nlargemodelthatisregularizedwell,forexamplebyusingdropout.\nMosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "decreasemodelcapacity.SomeexamplesareincludedinTable.11.1\nWhilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\ngoodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\nthisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\nizationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\nguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\nuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "costoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\n4 3 0",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nHyperparameterIncreases\ncapacity\nwhen...Reason Caveats\nNumberofhid-\ndenunitsincreasedIncreasingthenumberof\nhiddenunitsincreasesthe\nrepresentationalcapacity\nofthemodel.Increasingthenumber\nofhiddenunits increases\nboththetimeandmemory\ncostofessentiallyeveryop-\nerationonthemodel.\nLearningratetunedop-\ntimallyAnimproperlearningrate,\nwhether toohigh ortoo\nlow,resultsinamodel\nwithloweﬀectivecapacity\nduetooptimizationfailure\nConvolutionker-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "duetooptimizationfailure\nConvolutionker-\nnelwidthincreasedIncreasingthekernelwidth\nincreasesthenumberofpa-\nrametersinthemodelAwiderkernelresultsin\nanarroweroutputdimen-\nsion,reducingmodelca-\npacityunlessyouuseim-\nplicitzeropaddingtore-\nducethiseﬀect.Wider\nkernelsrequiremoremem-\noryforparameterstorage\nandincreaseruntime,but\nanarroweroutputreduces\nmemorycost.\nImplicitzero\npaddingincreasedAddingimplicitzerosbe-\nforeconvolutionkeepsthe\nrepresentationsizelargeIncreasedtimeandmem-\norycostofmostopera-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "orycostofmostopera-\ntions.\nWeightdecayco-\neﬃcientdecreasedDecreasingtheweightde-\ncaycoeﬃcientfreesthe\nmodelparameterstobe-\ncomelarger\nDropoutratedecreasedDroppingunitslessoften\ngivestheunitsmoreoppor-\ntunitiesto“conspire”with\neachothertoﬁtthetrain-\ningset\nTable11.1:Theeﬀectofvarioushyperparametersonmodelcapacity.\n4 3 1",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nprinciple,thisapproachcouldfailduetooptimization diﬃculties,butformany\nproblemsoptimization doesnotseemtobeasigniﬁcantbarrier,providedthatthe\nmodelischosenappropriately .\n1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s\nTheideallearningalgorithmjusttakesadatasetandoutputsafunction,without\nrequiringhand-tuning ofhyperparameters .Thepopularityofseverallearning",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "algorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\nperformwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan\nsometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but\noftenbeneﬁtsigniﬁcantlyfromtuningoffortyormorehyperparameters .Manual\nhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,\nsuchasonedeterminedbyothershavingworkedonthesametypeofapplication\nandarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "hyperparametervaluesforneuralnetworksappliedtosimilartasks.However,\nformanyapplications,thesestartingpointsarenotavailable.Inthesecases,\nautomatedalgorithmscanﬁndusefulvaluesofthehyperparameters .\nIfwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\ngoodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:\nwearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjective\nfunction,suchasvalidationerror,sometimesunderconstraints(suchasabudget",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "fortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,\nto develop h y p e r par am e t e r   o p t i m i z a t i o nalgorithms thatwrap a learnin g\nalgorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe\nlearningalgorithmfromtheuser.Unfortunately,hyperparameter optimization\nalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\nshouldbeexploredforeachofthelearningalgorithm’shyperparameters .However,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat\nacceptableperformancemaybeachievedonawiderangeoftasksusingthesame\nsecondaryhyperparameters foralltasks.\n1 1 . 4 . 3 G ri d S ea rch\nWhentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform\ng r i d se ar c h.Foreachhyperparameter, the userselectsasmallﬁnitesetof\nvaluestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\nspeciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvalues",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "foreachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\n4 3 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nGrid Random\nFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe\ndisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore. ( L e f t )To\nperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\nalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\nsets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "hyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependent\nfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\nuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa\nsamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\nhyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearch\nandrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "Theﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcant\ninﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\nhasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponential\ninthenumberofnon-inﬂuentialhyperparameters,whilerandomsearchtestsaunique\nvalueofeveryinﬂuentialhyperparameteronnearlyeverytrial.Figurereproducedwith\npermissionfrom (). BergstraandBengio2012\n4 3 3",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\nﬁgureforanillustrationofagridofhyperparameter values. 11.2\nHowshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\n(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen\nconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\nthattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "searchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning\nratetakenwithintheset{ .1 , .01 ,10−3,10−4,10−5},oranumberofhiddenunits\ntakenwiththeset . { } 501002005001000 2000 , , , , ,\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\nsupposethatweranagridsearchoverahyperparameter αusingvaluesof{−1 ,0 ,1}.\nIfthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 α\nliesandweshouldshiftthegridandrunanothersearchwith αin,forexample,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "{1 ,2 ,3}.Ifweﬁndthatthebestvalueof αis,thenwemaywishtoreﬁneour 0\nestimatebyzoominginandrunningagridsearchover. {− } . , , .101\nTheobviousproblemwithgridsearchisthatitscomputational costgrows\nexponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,\neachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials\nrequiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose\nparallelism(withalmostnoneedforcommunication betweendiﬀerentmachines",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "carryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,\nevenparallelization maynotprovideasatisfactorysizeofsearch.\n1 1 . 4 . 4 Ra n d o m S ea rch\nFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\nconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :\nrandomsearch( ,). BergstraandBengio2012\nArandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistribution\nforeachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued\nhyperparameters.Forexample,\nl o g l e a r n i n g r a t e __ ∼−− u(1 ,5) (11.2)\nl e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3)\nwhere u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b).\nSimilarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,\nlog(2000) ).\n4 3 4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues\nofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes\nnotincuradditionalcomputational cost. Infact,asillustratedinﬁgure,a11.2\nrandomsearchcanbeexponentiallymoreeﬃcientthanagridsearch,whenthere\nareseveralhyperparametersthatdonotstronglyaﬀecttheperformancemeasure.\nThisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "searchreducesthevalidationseterrormuchfasterthangridsearch,intermsof\nthenumberoftrialsrunbyeachmethod.\nAswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\nsearch,toreﬁnethesearchbasedontheresultsoftheﬁrstrun.\nThemainreasonwhyrandomsearchﬁndsgoodsolutionsfasterthangridsearch\nisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,\nwhentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )\nwouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "wouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\nwouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovalues\ndoesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,grid\nsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\nwillstillgivetwoindependentexplorationsoftheotherhyperparameters .\n1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\nThesearchforgoodhyperparameters canbecastasanoptimization problem.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\nvalidationseterrorthatresultsfromtrainingusingthesehyperparameters .In\nsimpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiable\nerrormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan\nsimplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal.\n2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\nduetoitshighcomputationandmemorycost,orduetohyperparametershaving",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "intrinsicallynon-diﬀerentiable interactionswiththevalidationseterror,asinthe\ncaseofdiscrete-valuedhyperparameters .\nTocompensateforthislackofagradient,wecanbuildamodelofthevalidation\nseterror,thenproposenewhyperparameterguessesbyperformingoptimization\nwithinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea\nBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\nerrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "mizationthusinvolvesatradeoﬀbetweenexploration(proposinghyperparameters\n4 3 5",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\nalsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel\nisconﬁdentwillperformaswellasanyhyperparameters ithasseensofar—usually\nhyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\napproachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012\nTPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "Currently,wecannotunambiguously recommendBayesianhyperparameter\noptimization asanestablishedtoolforachievingbetterdeeplearningresultsor\nforobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimization\nsometimesperformscomparablytohumanexperts,sometimesbetter,butfails\ncatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson\naparticularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeing\nsaid,hyperparameter optimization isanimportantﬁeldofresearchthat,while",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "oftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁt\nnotonlytheentireﬁeldofmachinelearningbutthedisciplineofengineeringin\ngeneral.\nOnedrawbackcommontomosthyperparameter optimization algorithmswith\nmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-\nperimenttoruntocompletionbeforetheyareabletoextractanyinformation\nfromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-\nmationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "practitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is\ncompletelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\nofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\npoints,thehyperparameter optimization algorithmcanchoosetobeginanew\nexperiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”\nandresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\nmoreinformation.\n11.5DebuggingStrategies",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "moreinformation.\n11.5DebuggingStrategies\nWhenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotell\nwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\nisabugintheimplementation ofthealgorithm. Machine learningsystemsare\ndiﬃculttodebugforavarietyofreasons.\nInmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\nalgorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\ndiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\n4 3 6",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nneuralnetworkonaclassiﬁcationtaskanditachieves5%testerror,wehave new\nnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal\nbehavior.\nAfurtherdiﬃcultyisthatmostmachinelearningmodelshavemultipleparts\nthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\nachieveroughlyacceptableperformance.Forexample,supposethatwearetraining\naneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "furtherthatwehavemanuallyimplemented thegradientdescentruleforeach\nparameterseparately,andwemadeanerrorintheupdateforthebiases:\nb b←− α (11.4)\nwhere αisthelearningrate.Thiserroneousupdatedoesnotusethegradientat\nall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\nisclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The\nbugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.\nDependingonthedistributionoftheinput,theweightsmaybeabletoadaptto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "compensateforthenegativebiases.\nMostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\nbothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthe\ncorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\npartoftheneuralnetimplementationinisolation.\nSomeimportantdebuggingtestsinclude:\nVisualizethemodelinaction:Whentrainingamodeltodetectobjectsin\nimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayed",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "superimposedontheimage.Whentrainingagenerativemodelofspeech,listento\nsomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\nfallintothepracticeofonlylookingatquantitativeperformancemeasurements\nlikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\nperformingitstaskwillhelptodeterminewhetherthequantitativeperformance\nnumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\ndevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "performingwellwhenitisnot.\nVisualizetheworstmistakes: Mostmodelsareabletooutputsomesortof\nconﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedona\nsoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\ntothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasin\nitsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthese\nvaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\n4 3 7",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless\nlikelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By\nviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan\noftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.\nForexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere\ntheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "someofthedigits.Thetranscriptionnetworkthenassignedverylowprobability\ntothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost\nconﬁdentmistakesshowedthattherewasasystematicproblemwiththecropping.\nModifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter\nperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded\ntobeabletoprocessgreatervariationinthepositionandscaleoftheaddress\nnumbers.\nReasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃcultto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "determinewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues\ncanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror\nishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe\nmodelisoverﬁttingforfundamentalalgorithmicreasons.Analternativepossibility\nisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe\nmodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata\nwasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainandtesterrorare",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "high,thenitisdiﬃculttodeterminewhetherthereisasoftwaredefectorwhether\nthemodelisunderﬁttingduetofundamentalalgorithmicreasons.Thisscenario\nrequiresfurthertests,describednext.\nFitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\nitisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmall\nmodelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,\naclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiases",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "oftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectly\nlabelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\nwithhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\nsingleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe\ntrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.\nCompareback-propagatedderivativestonumericalderivatives:Ifyouareusing\nasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "putations,orifyouareaddinganewoperationtoadiﬀerentiation libraryand\nmustdeﬁneitsbpropmethod,thenacommonsourceoferrorisimplementingthis\ngradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\n4 3 8",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nistocomparethederivativescomputedbyyourimplementation ofautomatic\ndiﬀerentiationtothederivativescomputedbya .Because ﬁni t e di ﬀ e r e nc e s\nf() =lim x\n →0f x  f x (+)−()\n, (11.5)\nwecanapproximate thederivativebyusingasmall,ﬁnite: \nf() x≈f x  f x (+)−()\n. (11.6)\nWecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di ﬀ e r -\ne nc e:\nf() x≈f x(+1\n2 f x )−(−1\n2 )\n. (11.7)",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "e nc e:\nf() x≈f x(+1\n2 f x )−(−1\n2 )\n. (11.7)\nTheperturbationsize mustchosentobelargeenoughtoensurethatthepertur-\nbationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations.\nUsually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\ng: Rm→ Rn.Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasingle\nderivativeatatime.Wecaneitherrunﬁnitediﬀerencing m ntimestoevaluateall\nofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "randomprojectionsatboththeinputandoutputof g.Forexample,wecanapply\nourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),\nwhere uand varerandomlychosenvectors.Computing f( x)correctlyrequires\nbeingabletoback-propagatethrough gcorrectly,yetiseﬃcienttodowithﬁnite\ndiﬀerencesbecause fhasonlyasingleinputandasingleoutput.Itisusually\nagoodideatorepeatthistestformorethanonevalueof uand vtoreduce\nthechancethatthetestoverlooksmistakesthatareorthogonaltotherandom\nprojection.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "projection.\nIfonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\naveryeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbers\nasinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\nobservationthat\nf x i  f x i  f (+) = ()+()+( x O 2) (11.8)\nreal((+)) = ()+( f x i  f x O 2)imag( ,f x i  (+)\n) = f()+( x O 2) ,(11.9)\nwhere i=√\n−1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeﬀect\nduetotakingthediﬀerencebetweenthevalueof fatdiﬀerentpoints.Thisallows",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "theuseoftinyvaluesof like = 10−150,whichmakethe O( 2)errorinsigniﬁcant\nforallpracticalpurposes.\n4 3 9",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\nstatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\noftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits\ncantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,\nhowoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,\ntheaverageoftheabsolutevalueofthepre-activationstellsushowsaturated",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "theunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\nquicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe\nmagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.\nAssuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015\noveraminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,\nnot50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay\nbethatsomegroupsofparametersaremovingatagoodpacewhileothersare",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "stalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay\nbeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir\nevolution.\nFinally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\ntheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\nimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\nproblems. Typicallythesecanbedebuggedbytestingeachoftheirguarantees.\nSomeguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjective",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "functionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\nrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\nandthatthegradientwithrespecttoallvariableswillbezeroatconvergence.\nUsuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\ncomputer,sothedebuggingtestshouldincludesometoleranceparameter.\n11.6Example:Multi-DigitNumberRecognition\nToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "inpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\nfromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\nmanyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\ndatabaseinfrastructure,andsoon,wereofparamountimportance.\nFromthepointofviewofthemachinelearningtask,theprocessbeganwith\ndatacollection. The carscollectedtherawdataandhumanoperatorsprovided\nlabels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdataset",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "curation,includingusingothermachinelearningtechniquestodetectthehouse\n4 4 0",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nnumberspriortotranscribingthem.\nThetranscriptionprojectbeganwithachoiceofperformancemetricsand\ndesiredvaluesforthesemetrics. Animportantgeneralprincipleistotailorthe\nchoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\niftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement\nforthisproject. Speciﬁcally,thegoalwastoobtainhuman-level,98%accuracy.\nThislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "thislevelofaccuracy,theStreetViewtranscriptionsystemsacriﬁcescoverage.\nCoveragethusbecamethemainperformancemetricoptimizedduringtheproject,\nwithaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame\npossibletoreducetheconﬁdencethresholdbelowwhichthenetworkrefusesto\ntranscribetheinput,eventuallyexceedingthegoalof95%coverage.\nAfterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-\nogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "convolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbegan\nwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\ntooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible\nbaseline,theﬁrstimplementation oftheoutputlayerofthemodelconsistedof n\ndiﬀerentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits\nweretrainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmax\nunittrainedindependently.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "unittrainedindependently.\nOurrecommendedmethodologyistoiterativelyreﬁnethebaselineandtest\nwhethereachchangemakesanimprovement.TheﬁrstchangetotheStreetView\ntranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\nmetricandthestructureofthedata.Speciﬁcally,thenetworkrefusestoclassify\naninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor\nsomethreshold t.Initially,thedeﬁnitionof p( y x|)wasad-hoc,basedonsimply",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "multiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment\nofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\nlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\nmuchmoreeﬀectively.\nAtthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical\nproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrument\nthetrainandtestsetperformanceinordertodeterminewhethertheproblem",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "isunderﬁttingoroverﬁtting.Inthiscase,trainandtestseterrorwerenearly\nidentical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe\navailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain\nandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue\n4 4 1",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\ntounderﬁttingorduetoaproblemwiththetrainingdata.Oneofthedebugging\nstrategieswerecommendistovisualizethemodel’sworsterrors.Inthiscase,that\nmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\nhighestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinput\nimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\nremovedbythecroppingoperation.Forexample,aphotoofanaddress“1849”",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "mightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblem\ncouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\nnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\ntheteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe\ncropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\npredicted.Thissinglechangeaddedtenpercentagepointstothetranscription\nsystem’scoverage.\nFinally,thelastfewpercentagepointsofperformancecamefromadjusting",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\ntainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror\nremainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitsweredue\ntounderﬁtting, aswellasduetoafewremainingproblemswiththedatasetitself.\nOverall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof\nmillionsofaddressestobetranscribedbothfasterandatlowercostthanwould\nhavebeenpossibleviahumaneﬀort.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "havebeenpossibleviahumaneﬀort.\nWehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\nothersimilarsuccesses.\n4 4 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 3\nProbabilityandInformation\nTheory\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.\nProbabilitytheoryisamathematical frameworkforrepresentinguncertain\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\nnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobability\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "expressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\nProbabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\nunderstandthematerialinthisbook.\nWhileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "thepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\nofuncertaintyinaprobabilitydistribution.\nIfyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\nbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "suggestthatyouconsultanadditionalresource,suchasJaynes2003().\n53",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.1WhyProbability?\nManybranchesofcomputersciencedealmostlywithentitiesthatareentirely\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\nexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butare\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "makesheavyuseofprobabilitytheory.\nThisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities.\nUncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\nbyPearl1988().\nNearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Infact,beyondmathematical statementsthataretruebydeﬁnition,itisdiﬃcult\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\nguaranteedtooccur.\nTherearethreepossiblesourcesofuncertainty:\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "whereweassumethatthecardsaretrulyshuﬄedintoarandomorder.\n2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar. Theoutcome\ngiventhecontestant’schoiceisdeterministic,butfromthecontestant’spoint\nofview,theoutcomeisuncertain.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "ofview,theoutcomeisuncertain.\n3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\nthe information wehave observed, the discarded i nformationresults in\nuncertaintyinthemodel’spredictions. Forexample,supposewebuilda\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\n54",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\ntheprecisepositionofobjects: eachobjectcouldbeanywherewithinthe\ndiscretecellthatitwasobservedtooccupy.\nInmanycases,itismorepracticaltouseasimplebutuncertainrulerather\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\nmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,the",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "simplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearule\noftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedto\nﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirds\nincludingthecassowary,ostrichandkiwi...” isexpensivetodevelop,maintainand\ncommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure.\nWhileitshouldbeclearthatweneedameansofrepresentingandreasoning\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "allofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheory\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable. Whenwe\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\ntheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportion\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "seemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,\nthismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasof\nthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatient\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheﬂu",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "and0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu. The\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\nofcertainty,isknownas B ay e si an pr o babili t y.\nIfwelistseveralpropertiesthatweexpectcommonsensereasoningabout\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\n55",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\nsee(). Ramsey1926\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "likelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\n3.2RandomVariables\nA r andom v ar i abl eisavariablethatcantakeondiﬀerentvaluesrandomly.We\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "itsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\nmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachof\nthesestatesare.\nRandomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\nisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthese\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\nassociatedwitharealvalue.\n3.3ProbabilityDistributions",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "3.3ProbabilityDistributions\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\ncontinuous.\n3.3.1DiscreteVariablesandProbabilityMassFunctions\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "acapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability\n56",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\nP P ()xisusuallynotthesameas()y.\nTheprobabilitymassfunctionmapsfromastateofarandomvariableto\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "todisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\nexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationto\nspecifywhichdistributionitfollowslater:xx. ∼P()\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "satisfythefollowingproperties:\n•Thedomainofmustbethesetofallpossiblestatesofx. P\n•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan 0 \nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\n•\nx ∈ xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\ntheprobabilityofoneofmanyeventsoccurring.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "theprobabilityofoneofmanyeventsoccurring.\nForexample,considerasinglediscreterandomvariablexwithkdiﬀerent\nstates.Wecanplacea uni f o r m di st r i but i o nonx—thatis,makeeachofits\nstatesequallylikely—bysettingitsprobabilitymassfunctionto\nPx (= x i) =1\nk(3.1)\nforalli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.\nThevalue1\nkispositivebecauseisapositiveinteger.Wealsoseethat k\n\niPx (= x i) =\ni1\nk=k\nk= 1, (3.2)\nsothedistributionisproperlynormalized.\n57",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\nfollowingproperties:\n•Thedomainofmustbethesetofallpossiblestatesofx. p\n•∀∈ ≥ ≤ xx,px() 0 () . p Notethatwedonotrequirex 1.\n•\npxdx()= 1.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "•\npxdx()= 1.\nAprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁc\nstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith\nvolumeisgivenby. δx pxδx()\nWecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofa\nsetofpoints.Speciﬁcally,theprobabilitythatxliesinsomeset Sisgivenbythe\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\nliesintheintervalisgivenby []a,b\n[ ] a , bpxdx().",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "[ ] a , bpxdx().\nForanexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁc\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\nwhereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans\n“parametrized by”;weconsiderxtobetheargumentofthefunction,whileaand\nbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobability\nmassoutsidetheinterval,wesayu(x;a,b)=0forallx∈[a,b] [.Withina,b],",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "uxa,b (;) =1\nb a −.Wecanseethatthisisnonnegativeeverywhere.Additionally,it\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\nbywritingx. ∼Ua,b()\n3.4MarginalProbability\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\ndistributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y\nForexample,supposewehavediscreterandomvariablesxandy,andweknow",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "P,(xy.Wecanﬁndxwiththe : ) P() sum r ul e\n∀∈xxx,P(= ) =x\nyPx,y. (= xy= ) (3.3)\n58",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThename“marginalprobability”comesfromtheprocessofcomputingmarginal\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\ndiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturalto\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\ntherightoftherow.\nForcontinuousvariables,weneedtouseintegrationinsteadofsummation:\npx() =\npx,ydy. () (3.4)\n3.5ConditionalProbability",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "px,ydy. () (3.4)\n3.5ConditionalProbability\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\nconditionalprobabilitycanbecomputedwiththeformula\nPyx (= y |x= ) =Py,x (= yx= )\nPx (= x ). (3.5)\nTheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcompute\ntheconditionalprobabilityconditionedonaneventthatneverhappens.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\nwhichwedonotexploreinthisbook.\n3.6TheChainRuleofConditionalProbabilities",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "3.6TheChainRuleofConditionalProbabilities\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\nintoconditionaldistributionsoveronlyonevariable:\nP(x( 1 ),...,x( ) n) = (Px( 1 ))Πn\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i −).(3.6)\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.\nItfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.5\n59",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nForexample,applyingthedeﬁnitiontwice,weget\nP,,P,P, (abc)= (ab|c)(bc)\nP,PP (bc)= ( )bc| ()c\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\n3.7IndependenceandConditionalIndependence\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\nonlyy:\n∀∈ ∈xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\nTworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "variableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\nwayforeveryvalueofz:\n∀∈ ∈ ∈ | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z.\n(3.8)\nWe candenoteindependence andconditionalindependence with compact\nnotation:xy⊥meansthatxandyareindependent,whilexyz ⊥|meansthatx\nandyareconditionallyindependentgivenz.\n3.8Expectation,VarianceandCovariance\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\nE x ∼ P[()] =fx\nxPxfx, ()() (3.9)\nwhileforcontinuousvariables,itiscomputedwithanintegral:\nE x ∼ p[()] =fx\npxfxdx. ()() (3.10)\n60",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\nIfitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[·]averagesover\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\nnoambiguity,wemayomitthesquarebrackets.\nExpectationsarelinear,forexample,",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "Expectationsarelinear,forexample,\nE x[()+ ()] = αfxβgxα E x[()]+fxβ E x[()]gx, (3.11)\nwhenandarenotdependenton. αβ x\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\nvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:\nVar(()) = fx E\n(() [()]) fx− Efx2\n. (3.12)\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "The c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\ntoeachother,aswellasthescaleofthesevariables:\nCov(()()) = [(() [()])(() [()])] fx,gy Efx− Efxgy− Egy.(3.13)\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "takeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\nrelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.\nThenotionsofcovarianceanddependencearerelated,butareinfactdistinct\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "ever,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\nzerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfroma\nuniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable\n61",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ns.Withprobability1\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\nthevalueofstobe−1.Wecanthengeneratearandomvariableybyassigning\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\nthemagnitudeof.However,y Cov() = 0x,y.\nThe c o v ar i anc e m at r i xofarandomvector x∈ Rnisannn×matrix,such\nthat\nCov() x i , j= Cov(x i,x j). (3.14)\nThediagonalelementsofthecovariancegivethevariance:\nCov(x i,x i) = Var(x i). (3.15)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "Cov(x i,x i) = Var(x i). (3.15)\n3.9CommonProbabilityDistributions\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\nlearning.\n3.9.1BernoulliDistribution\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.\nItiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityofthe\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\nP φ (= 1) = x (3.16)\nP φ (= 0) = 1x − (3.17)\nPxφ (= x ) = x(1 )−φ1 − x(3.18)\nE x[] = xφ (3.19)\nVar x() = (1 )xφ−φ (3.20)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "E x[] = xφ (3.19)\nVar x() = (1 )xφ−φ (3.20)\n3.9.2MultinoulliDistribution\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\nvariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis\n1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution.\nAmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "timeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.\nManytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying\nthattheyreferonlytothecase. n= 1\n62",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nparametrized byavector p∈[0,1]k − 1,wherep igivestheprobabilityofthei-th\nstate.Theﬁnal,k-thstate’sprobabilityisgivenby1− 1p.Notethatwemust\nconstrain 1p≤1.Multinoullidistributionsareoftenusedtorefertodistributions\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\norvarianceofmultinoulli-dis tributedrandomvariables.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "TheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-\nbutionovertheirdomain. They areabletodescribeanydistributionovertheir\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\nparametersmustimposestrictlimitsonthedistribution.\n3.9.3GaussianDistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "3.9.3GaussianDistribution\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\nt i o n,alsoknownasthe : G aussian di st r i but i o n\nN(;xµ,σ2) =\n1\n2πσ2exp\n−1\n2σ2( )xµ−2\n.(3.21)\nSeeﬁgureforaplotofthedensityfunction. 3.1\nThetwoparameters µ∈ Randσ∈(0,∞)controlthenormaldistribution.\nTheparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof\nthedistribution: E[x] =µ.Thestandarddeviationofthedistributionisgivenby\nσ,andthevariancebyσ2.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "σ,andthevariancebyσ2.\nWhenweevaluatethePDF,weneedtosquareandinvertσ.Whenweneedto\nfrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientway\nofparametrizing thedistributionistouseaparameterβ∈(0,∞)tocontrolthe\npr e c i si o norinversevarianceofthedistribution:\nN(;xµ,β− 1) =\nβ\n2πexp\n−1\n2βxµ (−)2\n. (3.22)\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "take,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\n63",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n− − − − 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .\nx000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x µ\nInﬂectionpointsat\nx µ σ = ±\nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;µ,σ2)exhibits\naclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,and\nthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormal\ndistribution,withand. µ= 0σ= 1\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "distributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\nstructuredbehavior.\nSecond,outofallpossibleprobabilitydistributionswiththesamevariance,\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "thatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\nsection.19.4.2\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\ndeﬁnitesymmetricmatrix: Σ\nN(; ) = x µ, Σ\n1\n(2)πndet() Σexp\n−1\n2( ) x µ−Σ− 1( ) x µ−\n.(3.23)\n64",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nTheparameter µstillgivesthemeanofthedistribution,thoughnowitis\nvector-valued.Theparameter Σgivesthecovariancematrixofthedistribution.\nAsintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\nmanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationally\neﬃcientwaytoparametrizethedistribution,sinceweneedtoinvert Σtoevaluate\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x β\nN(; x µ β,− 1) =\ndet() β\n(2)πnexp\n−1\n2( ) x µ−β x µ (−)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "det() β\n(2)πnexp\n−1\n2( ) x µ−β x µ (−)\n.(3.24)\nWeoftenﬁxthecovariancematrixtobeadiagonalmatrix.Anevensimpler\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\ntimestheidentitymatrix.\n3.9.4ExponentialandLaplaceDistributions\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\ndi st r i but i o n:\npxλλ (;) = 1 x ≥ 0exp( )−λx. (3.25)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "pxλλ (;) = 1 x ≥ 0exp( )−λx. (3.25)\nTheexponentialdistributionusestheindicatorfunction 1 x ≥ 0toassignprobability\nzerotoallnegativevaluesof.x\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\nofprobabilitymassatanarbitrarypointistheµ L apl ac e di st r i but i o n\nLaplace(;) =xµ,γ1\n2γexp\n−|−|xµ\nγ\n. (3.26)\n3.9.5TheDiracDistributionandEmpiricalDistribution\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "clustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusing\ntheDiracdeltafunction,:δx()\npxδxµ. () = (−) (3.27)\nTheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\nassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindof\n65",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeﬁnedintermsofits\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\nthanzero.\nBydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowand\ninﬁnitelyhighpeakofprobabilitymasswhere.xµ= \nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\ndi st r i but i o n,\nˆp() = x1\nmm",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "di st r i but i o n,\nˆp() = x1\nmm\ni = 1δ( x x−( ) i) (3.28)\nwhichputsprobabilitymass1\nmoneachofthempoints x( 1 ),..., x( ) mforminga\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\ntodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset.\nWecanviewtheempiricaldistributionformedfromadatasetoftraining\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\nonthisdataset. Anotherimportantperspectiveontheempiricaldistributionis\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\n(seesection).5.5\n3.9.6MixturesofDistributions\nItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimpler",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributions.Onecommon wayof combining distributionsis to\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\nmultinoullidistribution:\nP() =x\niPiPi (= c )( = xc| ) (3.29)\nwherecisthemultinoullidistributionovercomponentidentities. P()\nWehavealreadyseenoneexampleofamixturedistribution:theempirical",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "distributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\ncomponentforeachtrainingexample.\n66",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\nprobabilitydistributionsfromsimpleonesinmoredetail.\nThemixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeof\nparamountimportancelater—the l at e n t v ar i abl e.Alatentvariableisarandom\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "mixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\nvariablesarediscussedfurtherinsection.16.5\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "model,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\naseparatelyparametrized mean µ( ) iandcovariance Σ( ) i.Somemixturescanhave\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\nviatheconstraint Σ( ) i= Σ,i∀.AswithasingleGaussiandistribution,themixture\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\ndiagonalorisotropic.\nInadditiontothemeansandcovariances,theparametersofaGaussianmixture",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "specifythe pr i o r pr o babili t yα i=P(c=i) giventoeachcomponenti.Theword\n“prior”indicatesthatitexpressesthemodel’sbeliefsaboutc b e f o r eithasobserved\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\nspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\ncomponents.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "components.\nFigureshowssamplesfromaGaussianmixturemodel. 3.2\n3.10UsefulPropertiesofCommonFunctions\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\ntheprobabilitydistributionsusedindeeplearningmodels.\nOneofthesefunctionsisthe : l o g i st i c si g m o i d\nσx() =1\n1+exp()−x. (3.30)\nThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoulli\n67",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nx 1x 2\nFigure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\ncomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "separatelyalonganarbitrarybasisofdirections.\ndistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\nfortheφparameter.Seeﬁgureforagraphofthesigmoidfunction.The 3.3\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\nmeaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinits\ninput.\nAnothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l .\n2001):\nζx x. () = log(1+exp()) (3.31)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "2001):\nζx x. () = log(1+exp()) (3.31)\nThesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormal\ndistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulating\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\nfactthatitisasmoothedor“softened”versionof\nx+= max(0),x. (3.32)\nSeeﬁgureforagraphofthesoftplusfunction. 3.4\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\nthem:\n68",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n− − 1 0 5 0 5 1 0\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .σ x ( )\nFigure3.3:Thelogisticsigmoidfunction.\n− − 1 0 5 0 5 1 0\nx024681 0ζ x ( )\nFigure3.4:Thesoftplusfunction.\n69",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nσx() =exp()x\nexp()+exp(0)x(3.33)\nd\ndxσxσxσx () = ()(1−()) (3.34)\n1 () = () −σxσ−x (3.35)\nlog() = () σx −ζ−x (3.36)\nd\ndxζxσx () = () (3.37)\n∀∈x(01),,σ− 1() = logxx\n1−x\n(3.38)\n∀x>,ζ0− 1() = log(exp()1) x x− (3.39)\nζx() =x\n− ∞σydy() (3.40)\nζxζxx ()−(−) = (3.41)\nThefunctionσ− 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\nusedinmachinelearning.\nEquationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus 3.41",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "functionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\nfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothe\nnegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepart\nandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverx\nusingthesamerelationshipbetweenand,asshowninequation. ζx()ζx(−) 3.41\n3.11Bayes’Rule\nWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknow",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "P(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\nusing B a y e s’ r ul e:\nP( ) =xy|PP()x( )yx|\nP()y. (3.42)\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\nP() =y\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\n70",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nBayes’ruleis straightforwardto derivefrom thedeﬁnitionofconditional\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrst\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\nindependentlydiscoveredbyPierre-SimonLaplace.\n3.12TechnicalDetailsofContinuousVariables\nAproperformalunderstandingofcontinuousrandomvariablesandprobability",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "densityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\nthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryis\nemployedtoresolve.\nInsection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "S 1and S 2suchthatp( x∈ S 1) +p( x∈ S 2)>1but S 1∩ S 2=∅.Thesesets\naregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofreal\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedby\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\nprobabilityofwithoutencounteringparadoxes. Inthisbook,weonlyintegrate\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "becomesarelevantconcern.\nForourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\nasetissaidtohave m e asur e z e r o.Wedonotformallydeﬁnethisconceptinthis\ntextbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthataset\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "within R2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.\nLikewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\nnumbershasmeasurezero,forinstance).\nAnotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\n71",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\ntheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuous\nvalues.\nAnothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "tinuous,diﬀerentiabletransformation.Onemightexpectthatp y( y) =p x(g− 1( y)).\nThisisactuallynotthecase.\nAsasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\ny=x\n2andx∼U(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\neverywhereexcepttheinterval[0,1\n2] 1 ,anditwillbeonthisinterval.Thismeans\n\np y()=ydy1\n2, (3.43)\nwhichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake.\nTheproblemwiththisapproachisthatitfailstoaccountforthedistortionof",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "spaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\ninﬁnitesimallysmallregionwithvolumeδ xisgivenbyp( x)δ x.Sincegcanexpand\norcontractspace,theinﬁnitesimalvolumesurrounding xin xspacemayhave\ndiﬀerentvolumeinspace. y\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\npreservetheproperty\n|p y(())= gxdy||p x()xdx.| (3.44)\nSolvingfromthis,weobtain\np y() = yp x(g− 1())y∂x\n∂y(3.45)\norequivalently\np x() = xp y(())gx∂gx()\n∂x. (3.46)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "p x() = xp y(())gx∂gx()\n∂x. (3.46)\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\nm at r i x—thematrixwithJ i , j=∂ x i\n∂ y j.Thus,forreal-valuedvectorsand, x y\np x() = xp y(())g xdet∂g() x\n∂ x . (3.47)\n72",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.13InformationTheory\nInformationtheory isa branchof appliedmathematics thatrevolvesaround\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\nspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "machinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\nwheresomeofthesemessagelengthinterpretations donotapply.Thisﬁeldis\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.\nFormoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\n().2003\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "eventhas occurredismoreinformativethanlearningthata likely eventhas\noccurred.Amessagesaying“thesunrosethismorning”issouninformative as\ntobeunnecessarytosend,butamessagesaying“therewasasolareclipsethis\nmorning”isveryinformative.\nWewouldliketoquantifyinformationinawaythatformalizesthisintuition.\nSpeciﬁcally,\n•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\nwhatsoever.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "whatsoever.\n•Lesslikelyeventsshouldhavehigherinformationcontent.\n•Independenteventsshouldhaveadditiveinformation. Forexample,ﬁnding\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\nmuchinformationasﬁndingoutthatatossedcoinhascomeupasheads\nonce.\nInordertosatisfyallthreeoftheseproperties,wedeﬁnethe se l f - i nf o r m a t i o n\nofaneventxtobe = x\nIxPx. () = log− () (3.48)\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "deﬁnitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\n73",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ninformationgainedbyobservinganeventofprobability1\ne.Othertextsusebase-2\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\njustarescalingofinformationmeasuredinnats.\nWhenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\nguaranteedtooccur.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "guaranteedtooccur.\nSelf-information dealsonlywithasingleoutcome.Wecanquantifytheamount\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\nH() = x E x ∼ P[()] = Ix − E x ∼ P[log()]Px. (3.49)\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\narediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\nﬁgureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\nknownasthe di ﬀ e r e n t i al e nt r o p y.\nIfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\nrandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusing\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\nD K L( ) = PQ E x ∼ P\nlogPx()\nQx()",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "D K L( ) = PQ E x ∼ P\nlogPx()\nQx()\n= E x ∼ P[log()log()] Px−Qx.(3.50)\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\nthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuous\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerence\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\nbecauseitisnotsymmetric:D K L(PQ)=D K L(QP)forsomePandQ. This\n74",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\np0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.\nOnthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\nto.Theentropyisgivenby 1 (p−1)log(1−p)−pplog.Whenpisnear0,thedistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "isnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.\nWhenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\noutcomes.\nasymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\ntouseD K L( )PQorD K L( )QP.Seeﬁgureformoredetail.3.6\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "H(P,Q) =H(P)+D K L(PQ),whichissimilartotheKLdivergencebutlacking\nthetermontheleft:\nHP,Q( ) = − E x ∼ Plog()Qx. (3.51)\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\ntreattheseexpressionsaslim x → 0xxlog= 0.\n3.14StructuredProbabilisticModels",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "3.14StructuredProbabilisticModels\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\n75",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nxProbability Densityq∗= argminq D K L() p q \np x()\nq∗() x\nxProbability Densityq∗= argminq D K L() q p \np() x\nq∗() x\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\neitherD KL(pq)orD KL(qp).Weillustratetheeﬀectofthischoiceusingamixtureof\ntwoGaussiansforp,andasingleGaussianforq. Thechoiceofwhichdirectionofthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "KLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\ndirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeach\napplication. ( L e f t )TheeﬀectofminimizingD KL(pq).Inthiscase,weselectaqthathas",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "highprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\neﬀectofminimizingD KL(qp).Inthiscase,weselectaqthathaslowprobabilitywhere\nphaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,\nasinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "illustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\narenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionofthe\nKLdivergencecanstillchoosetoblurthemodes.\n76",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ndescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(both\ncomputationally andstatistically).\nInsteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\nForexample,supposewehavethreerandomvariables:a,bandc.Supposethat\nainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcare\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "variablesasaproductofprobabilitydistributionsovertwovariables:\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\nThesefactorizationscangreatlyreducethenumberofparametersneeded\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\nreducethecostofrepresentingadistributionifweareabletoﬁndafactorization\nintodistributionsoverfewervariables.\nWecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del.\nTherearetwomainkindsofstructuredprobabilisticmodels:directedand\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\ninthegraphcorrespondstoarandomvariable, and anedgeconnectingtwo\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "interactionsbetweenthosetworandomvariables.\nD i r e c t e dmodelsuse graphswithdirectededges, andtheyrepresentfac-\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove.\nSpeciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\ngiventheparentsofx i,denotedPa G(x i):\np() = x\nip(x i|Pa G(x i)). (3.53)\nSeeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability 3.7",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "distributionsitrepresents.\nU ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\n77",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nareusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "connectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\nmodelisassociatedwithafactorφ( ) i(C( ) i).Thesefactorsarejustfunctions,not\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\ndistribution.\nTheprobabilityofaconﬁgurationofrandomvariablesis pr o p o r t i o naltothe\nproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\nthereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegral\noverallstatesoftheproductoftheφfunctions,inordertoobtainanormalized\nprobabilitydistribution:\np() = x1\nZ\niφ( ) i\nC( ) i\n. (3.55)\nSeeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8\nprobabilitydistributionsitrepresents.\nKeep inmind thatthese graphicalrepresentationsof factorizations are a",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "languagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\n78",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,, (abcde) =1\nZφ( 1 )( )abc,,φ( 2 )()bd,φ( 3 )()ce,. (3.56)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\nways.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "ways.\nThroughoutpartsandofthisbook,wewillusestructuredprobabilistic III\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\ndiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\ndetail.\nThischapterhasreviewedthebasicconceptsofprobabilitytheorythatare",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "mostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\nremains:numericalmethods.\n79",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 8\nOptimizationforTrainingDeep\nModels\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms.\nOfallofthemanyoptimization problemsinvolvedindeeplearning,themost\ndiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "networktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit.\nThischapterpresentstheseoptimization techniquesforneuralnetworktraining.\nIfyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\noptimization ingeneral.\nThischapterfocusesononeparticularcaseofoptimization: ﬁndingtheparam-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "etersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunction J(θ),which\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\nwellasadditionalregularizationterms.\nWebeginwithadescriptionofhowoptimization usedasatrainingalgorithm\nforamachinelearningtaskdiﬀersfrompureoptimization. Next,wepresentseveral\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdiﬃcult.We\nthendeﬁneseveralpracticalalgorithms,includingbothoptimization algorithms",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "themselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\n274",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\nalgorithmsintohigher-levelprocedures.\n8.1HowLearningDiﬀersfromPureOptimization\nOptimization algorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.\nInmostmachinelearningscenarios,wecareaboutsomeperformancemeasure",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "P,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable.We\nthereforeoptimize Ponlyindirectly.Wereduceadiﬀerentcostfunction J(θ)in\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\ndeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureof\nmachinelearningobjectivefunctions.\nTypically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\nsuchas",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "suchas\nJ() = θ E ( ) ˆ x ,y ∼ pdataL f , y , ((;)xθ) (8.1)\nwhere Listheper-examplelossfunction, f(x;θ)isthepredictedoutputwhen\ntheinputisx,ˆ p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\nsupervisedcase,wheretheargumentsto Lare f(x;θ)and y.However,itistrivial\ntoextendthisdevelopment,forexample,toincludeθorxasarguments,orto\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\nunsupervisedlearning.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedlearning.\nEquationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We 8.1\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\novertheﬁnitetrainingset:\nJ∗() = θ E ( ) x ,y ∼ pdataL f , y . ((;)xθ) (8.2)\n8.1.1EmpiricalRiskMinimization\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "thattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\n2 7 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.\nThesimplestwaytoconvertamachinelearningproblembackintoanop-\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistributionˆ p(x , y)\ndeﬁnedbythetrainingset.Wenowminimizetheempiricalrisk\nE x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\nmm ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "mm \ni = 1L f((x( ) i;)θ , y( ) i)(8.3)\nwhereisthenumberoftrainingexamples. m\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyas\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\ncanbeexpectedtodecreasebyvariousamounts.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "canbeexpectedtodecreasebyvariousamounts.\nHowever,empiricalriskminimization ispronetooverﬁtting.Modelswith\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\nriskminimization isnotreallyfeasible.Themosteﬀectivemodernoptimization\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁned\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "rarelyuseempiricalriskminimization. Instead,wemustuseaslightlydiﬀerent\napproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent\nfromthequantitythatwetrulywanttooptimize.\n8.1.2SurrogateLossFunctionsandEarlyStopping\nSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnot\nonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "classiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.\nForexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorin\nexpectation.\n2 7 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\nonecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapart\nfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextracting",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "moreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\nminimizingtheaverage0-1lossonthetrainingset.\nAveryimportantdiﬀerencebetweenoptimization ingeneralandoptimization\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\nstopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased 7.8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\nandisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.\nTrainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\nwhichisverydiﬀerentfromthepureoptimization setting,whereanoptimization\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\n8.1.3BatchandMinibatchAlgorithms\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "optimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\nForexample,maximumlikelihoodestimationproblems,whenviewedinlog\nspace,decomposeintoasumovereachexample:\nθ M L= argmax\nθm \ni = 1log p m o de l(x( ) i, y( ) i;)θ . (8.4)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "i = 1log p m o de l(x( ) i, y( ) i;)θ . (8.4)\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\nempiricaldistributiondeﬁnedbythetrainingset:\nJ() = θ E x ,y ∼ ˆ pdatalog p m o de l(;)x , yθ . (8.5)\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\n2 7 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmostcommonlyusedpropertyisthegradient:\n∇ θ J() = θ E x ,y ∼ ˆ pdata∇ θlog p m o de l(;)x , yθ . (8.6)\nComputing this expectation exactly isvery expensive because it requires\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\nfromthedataset,thentakingtheaverageoveronlythoseexamples.\nRecallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "samplesisgivenby σ /√n ,where σisthetruestandarddeviationofthevalueof\nthesamples.Thedenominator of√nshowsthattherearelessthanlinearreturns\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "numberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\nofthegradientratherthanslowlycomputingtheexactgradient.\nAnotherconsiderationmotivatingstatisticalestimationofthegradientfroma\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "areunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlarge\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient.\nOptimization algorithmsthatusetheentiretrainingsetarecalledbatchor\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\nbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedby\nminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "impliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribe\nagroupofexamplesdoesnot. Forexample,itisverycommontousetheterm\n“batchsize”todescribethesizeofaminibatch.\nOptimization algorithmsthatuseonlyasingleexampleatatimearesometimes\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\ncreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveral\npassesaremade.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "passesaremade.\nMostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\n2 7 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\ncallthemstochasticmethods.\nThecanonicalexampleofastochasticmethodisstochasticgradientdescent,\npresentedindetailinsection.8.3.1\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\n•Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\nlessthanlinearreturns.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "lessthanlinearreturns.\n•Multicorearchitectures areusuallyunderutilized byextremelysmallbatches.\nThismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\nisnoreductioninthetimetoprocessaminibatch.\n•Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\nhardwaresetupsthisisthelimitingfactorinbatchsize.\n•Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀer\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\nsometimesbeingattemptedforlargemodels.\n•Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\nerrorisoftenbestforabatchsizeof1. Trainingwithsuchasmallbatch\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "duetotheneedtomakemoresteps,bothbecauseofthereducedlearning\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.\nDiﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-\nbatchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthan\nothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccurately\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "usuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\nH− 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\nsizesarerequiredtominimizeﬂuctuationsintheestimatesofH− 1g.Suppose\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\n2 7 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.\nVerysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\nH− 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\napproximately,sotheupdateH− 1gwillcontainevenmoreerrorthanwewould\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "unbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\nlistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "timesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthe\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\nholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselecting\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "examplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\nitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitin\nshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutive\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "signiﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycan\nseriouslyreducetheeﬀectivenessofthealgorithm.\nManyoptimization problemsinmachinelearningdecomposeoverexamples\nwellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamples\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\ndiscussedfurtherinsection.12.1.3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "discussedfurtherinsection.12.1.3\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\n2 8 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ndescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Onthe\nﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\nnewfairsamplesfromthedatageneratingdistribution.\nThefactthatstochasticgradientdescentminimizesgeneralization erroris",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "easiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\nfromastreamofdata.Inotherwords,insteadofreceivingaﬁxed-sizetraining\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y).\nInthisscenario,examplesareneverrepeated;everyexperienceisafairsample\nfrom p da t a.\nTheequivalenceiseasiesttoderivewhenbothxand yarediscrete. Inthis\ncase,thegeneralization error(equation)canbewrittenasasum 8.2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "J∗() =θ\nx\nyp da t a()((;)) x , y L fxθ , y , (8.7)\nwiththeexactgradient\ng= ∇ θ J∗() =θ\nx\nyp da t a()x , y∇ θ L f , y . ((;)xθ)(8.8)\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\nundermildassumptionsregarding p da t aand. L\nHence, wecanobtainanunbiasedestimatoroftheexactgradientof the",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "generalization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\nˆg=1\nm∇ θ\niL f((x( ) i;)θ , y( ) i) . (8.9)\nUpdatinginthedirectionof θ ˆgperformsSGDonthegeneralization error.\nOfcourse, thisinterpretation only applies whenexamplesarenotreused.\nNonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "unlessthetrainingsetisextremelylarge. When multiplesuchepochsareused,\nonlytheﬁrstepochfollowstheunbiasedgradientofthegeneralization error,but\n2 8 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreased\ntrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentraining\nerrorandtesterror.\nWithsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\nset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,so",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "underﬁttingandcomputational eﬃciencybecomethepredominant concerns.See\nalso ()foradiscussionoftheeﬀectofcomputational BottouandBousquet2008\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows.\n8.2ChallengesinNeuralNetworkOptimization\nOptimization ingeneralisanextremelydiﬃculttask.Traditionally,machine\nlearninghasavoidedthediﬃcultyofgeneraloptimization bycarefullydesigning\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "convex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\nfortrainingdeepmodels.\n8.2.1Ill-Conditioning\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\nmoredetailinsection.4.3.1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "moredetailinsection.4.3.1\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\n“stuck”inthesensethatevenverysmallstepsincreasethecostfunction.\nRecallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\ncostfunctionpredictsthatagradientdescentstepofwilladd − g\n1\n22gHgg− g (8.10)\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\n22gHg",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "22gHg\nexceeds gg. Todeterminewhetherill-conditioning isdetrimentaltoaneural\nnetwork training task, one canmonitorthe squaredgradientnormggand\n2 8 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n−50050100150200250\nTrainingtime(epochs)−20246810121416Gradient norm\n0 50100150200250\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classiﬁcationerrorrate\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "evaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcation\nerrordecreasestoalowlevel.\nthegHgterm.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantly",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "throughoutlearning,butthegHgtermgrowsbymorethananorderofmagnitude.\nTheresultisthatlearningbecomesveryslowdespitethepresenceofastrong\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\ncurvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly 8.1\nduringthesuccessfultrainingofaneuralnetwork.\nThoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\ntraining,someofthetechniquesusedtocombatitinothercontextsareless",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "applicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttool\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\nthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcant\nmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.\n8.2.2LocalMinima\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\ncanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis\n2 8 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionat\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\naﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\nknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.\nWithnon-convexfunctions,suchasneuralnets,itispossibletohavemany\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "anextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\nnecessarilyamajorproblem.\nNeuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\nvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁability\nproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcan\nruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariables\nareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanging",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "latentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\nunits.Thiskindofnon-identiﬁabilit yisknownasweightspacesymmetry.\nInadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\nadditionalcausesofnon-identiﬁabilit y.Forexample,inanyrectiﬁedlinearor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "maxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\nαifwealsoscaleallofitsoutgoingweightsby1\nα.Thismeansthat—ifthecost\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\nweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinear\normaxoutnetworkliesonan( m n×)-dimensionalhyperbolaofequivalentlocal\nminima.\nThesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylarge\norevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcost",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "function.However,alloftheselocalminimaarisingfromnon-identiﬁabilit yare\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\nnotaproblematicformofnon-convexity.\nLocalminimacanbeproblematiciftheyhavehighcostincomparisontothe\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "withhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\noptimization algorithms.\nItremainsanopenquestionwhethertherearemanylocalminimaofhighcost\n2 8 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\ncommonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\nnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavea\nlowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimum",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "ratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\netal.,).2014\nManypractitioners attributenearlyalldiﬃcultywithneuralnetworkoptimiza-\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspeciﬁc\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\ninsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "point.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\nspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaarethe\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.\n8.2.3Plateaus,SaddlePointsandOtherFlatRegions\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "whileothershavealowercost. Atasaddlepoint,theHessianmatrixhasboth\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\nmaximumalonganothercross-section.Seeﬁgureforanillustration. 4.5\nManyclasses ofrandomfunctionsexhibitthefollowingbehavior:inlow-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn→ Rof\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues. The\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\nImaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingle",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "dimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\n2 8 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost. In\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\nheads ntimesifweareatacriticalpointwithlowcost. Thismeansthatlocal\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "highcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\nhighcostaremorelikelytobelocalmaxima.\nThishappensformanyclassesofrandomfunctions.Doesithappenforneural\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "proofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "deepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\nfunctionsrelatedtoneuralnetworksdoessoaswell.\nWhataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\nrithms?Forﬁrst-orderoptimization algorithmsthatuseonlygradientinformation,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\ngiveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear 8.2\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\nmaybediﬀerentformorerealisticusesofgradientdescent.\nForNewton’smethod, itisclearthatsaddlepointsconstituteaproblem.\n2 8 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nP r o j e c t i o n2o f θ\nP r o j e c t i o n 1 o f θJ(\n)θ\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\nwithpermissionfromGoodfellow2015 e t a l .(). Thesevisualizationsappearsimilarfor\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\nvisualizationsusuallydonotshowmanyconspicuousobstacles. Priortothesuccessof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "stochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\nstructurethanisrevealedbytheseprojections. Theprimaryobstaclerevealedbythis\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.\nMostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\ninthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleinthe\nﬁgureviaanindirectarcingpath.\n2 8 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesigned\ntoseekacriticalpoint.Newton’smethod,however,isdesignedtosolvefora\npointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjump\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\ngradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "saddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\nimprovessigniﬁcantlyoverthetraditionalversion.Second-order methodsremain\ndiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\npromiseifitcouldbescaled.\nThereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\npoints.Therearealsomaxima, whic haremuchlikesaddlepointsfromthe\nperspectiveofoptimization—many algorithmsarenotattractedtothem, but\nunmodiﬁedNewton’smethodis.Maximaofmanyclassesofrandomfunctions",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "becomeexponentiallyrareinhighdimensionalspace,justlikeminimado.\nTheremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,the\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\nﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimization\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\n8.2.4CliﬀsandExplodingGradients",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "8.2.4CliﬀsandExplodingGradients\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\ncliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral 8.3\nlargeweightstogether.Onthefaceofanextremelysteepcliﬀstructure,the\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀ\nofthecliﬀstructurealtogether.\n2 8 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\n \n\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,a\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "optimizationworkthathadbeendone. FigureadaptedwithpermissionfromPascanu\ne t a l .().2013\nThecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\nwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithm",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "proposestomakeaverylargestep,thegradientclippingheuristicintervenesto\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cliﬀ\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\nmultiplication.\n8.2.5Long-TermDependencies",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "multiplication.\n8.2.5Long-TermDependencies\nAnotherdiﬃcultythatneuralnetworkoptimization algorithmsmustovercome\narises when thecomputational gra ph becomes extremely deep.Feedforward\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\n2 8 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\npronounceddiﬃculties.\nForexample,supposethatacomputational graphcontainsapaththatconsists\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V− 1.\nInthissimplecase,itisstraightforwardtoseethat\nWt=\nVλVdiag()− 1t= ()VdiagλtV− 1. (8.11)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "Wt=\nVλVdiag()− 1t= ()VdiagλtV− 1. (8.11)\nAnyeigenvalues λ ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\nvanishingandexplodinggradientproblemreferstothefactthatgradients\nthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradients\nmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprove\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "structuresdescribedearlierthatmotivategradientclippingareanexampleofthe\nexplodinggradientphenomenon.\nTherepeatedmultiplication byWateachtimestepdescribedhereisvery\nsimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueof\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\nnotsurprisingthatxWtwilleventuallydiscardallcomponentsofxthatare\northogonaltotheprincipaleigenvectorof.W\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "networksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\nvanishingandexplodinggradientproblem(,). Sussillo2014\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\n8.2.6InexactGradients\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\nanoisyorevenbiasedestimateofthesequantities. Nearlyeverydeeplearning",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "algorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\noftrainingexamplestocomputethegradient.\nInothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\nWhentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\n2 9 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\nofaBoltzmannmachine.\nVariousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.\n8.2.7PoorCorrespondencebetweenLocalandGlobalStructure",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "Manyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\nlossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepif J(θ)is\npoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddle\npointhidingtheopportunitytomakeprogressdownhillfromthegradient.\nItispossibletoovercomealloftheseproblemsatasinglepointandstill\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\nnotpointtowarddistantregionsofmuchlowercost.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "notpointtowarddistantregionsofmuchlowercost.\nGoodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\nmountain-shapedstructure.\nMuchofresearchintothediﬃcultiesofoptimization hasfocusedonwhether\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\n−log p( y|x;θ)canlackaglobalminimumpointandinsteadasymptotically\napproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwith\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "zero.Likewise,amodelofrealvalues p( y|x) =N( y; f(θ) , β− 1)canhavenegative\nlog-likelihoodthatasymptotestonegativeinﬁnity—if f(θ)isabletocorrectly\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\nβwithoutbound.Seeﬁgureforanexampleofafailureoflocaloptimization to 8.4\nﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\npoints.\nFutureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "inﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\n2 9 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nθJ ( ) θ\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyin\nthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingableto\ntraverseit. Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "suchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\nexcessivetrainingtime,asillustratedinﬁgure.8.2\noftheprocess.\nManyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor\nproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithms\nthatusenon-localmoves.\nGradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefor\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "canbediﬃculttocompute.Wemaybeabletocomputesomepropertiesofthe\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\nnotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactually\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "thesecases,localdescentwithstepsofsize maydeﬁneareasonablyshortpath\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\nstepsofsize δ .Inthesecases,localdescentmayormaynotdeﬁneapath\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\n2 9 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\nthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacritical\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\nforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoes\nnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "ﬁgure,oralonganunnecessarilylongtrajectorytothesolution,asinﬁgure. 8.4 8.2\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\nmakingneuralnetworkoptimization diﬃcult,andthisisanactiveareaofresearch.\nRegardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbe\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\nwithinthatwell-behavedregion. Thislastviewsuggestsresearchintochoosing",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "goodinitialpointsfortraditionaloptimization algorithmstouse.\n8.2.8TheoreticalLimitsofOptimization\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\nlittlebearingontheuseofneuralnetworksinpractice.\nSometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\nnetworkoutput discretevalues.However, most neuralnetworkunitsoutput",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "smoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\nitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Other\nresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,but\ninpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmany\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\ncontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexact",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "minimumofafunction,butseekonlytoreduceitsvaluesuﬃcientlytoobtaingood\ngeneralization error. Theoretical analysisofwhetheranoptimization algorithm\ncanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticbounds\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\ngoalformachinelearningresearch.\n2 9 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.3BasicAlgorithms\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\n8.3.1StochasticGradientDescent\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "optimization algorithmsformachinelearningingeneralandfordeeplearning\ninparticular. As discussedinsection,itispossibletoobtainanunbiased 8.1.3\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\nexamplesdrawni.i.dfromthedatageneratingdistribution.\nAlgorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\nRequire:Learningrate  k.\nRequire:Initialparameterθ\nwhile do stoppingcriterionnotmet",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "while do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate: ˆg←+1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nApplyupdate:θθ← − ˆg\nendwhile\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\nhavedescribedSGDasusingaﬁxedlearningrate .Inpractice,itisnecessaryto\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\natiterationas k  k.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "atiterationas k  k.\nThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\ndescent,sobatchgradientdescentcanuseaﬁxedlearningrate.Asuﬃcient\nconditiontoguaranteeconvergenceofSGDisthat\n∞\nk = 1 k= and ∞ , (8.12)\n2 9 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n∞\nk = 12\nk < .∞ (8.13)\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: τ\n k= (1 )− α  0+ α  τ (8.14)\nwith α=k\nτ.Afteriteration,itiscommontoleaveconstant. τ \nThelearningratemaybechosenbytrialanderror,butitisusuallybest\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "theparameterstochooseare  0,  τ,and τ.Usually τmaybesettothenumberof\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\n τshouldbesettoroughlythevalueof 1%  0.Themainquestionishowtoset  0.\nIfitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\nfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyif\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "useofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\nTypically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\nﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\naftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrst\nseveraliterationsandusealearningratethatishigherthanthebest-performing\nlearningrateatthistime,butnotsohighthatitcausessevereinstability.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\nconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithas\nprocessedtheentiretrainingset.\nTostudytheconvergencerateofanoptimization algorithmitiscommonto\nmeasuretheexcesserror J(θ)−min θ J(θ),whichistheamountthatthecurrent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "costfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\nproblem,theexcesserroris O(1√\nk)after kiterations,whileinthestronglyconvex\ncaseitis O(1\nk).Theseboundscannotbeimprovedunlessextraconditionsare\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\ngradientdescentintheory.However,theCramér-Raobound(,;, Cramér1946Rao\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\nk).Bottou\n2 9 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\nanoptimization algorithmthatconvergesfasterthan O(1\nk)formachinelearning\ntasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,the\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "outweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\ntheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelost\nintheconstantfactorsobscuredbythe O(1\nk)asymptoticanalysis.Onecanalso\ntradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygradually\nincreasingtheminibatchsizeduringthecourseoflearning.\nFormoreinformationonSGD,see(). Bottou1998\n8.3.2Momentum\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\nintheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\nofvelocity—itisthedirectionandspeedatwhichtheparametersmovethrough",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "parameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\naccordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.\nInthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter α∈[0 ,1)\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "Theupdateruleisgivenby:\nvv← α−∇  θ\n1\nmm \ni = 1L((fx( ) i;)θ ,y( ) i)\n, (8.15)\nθθv ← + . (8.16)\nThevelocityvaccumulatesthegradientelements∇ θ1\nmm\ni = 1 L((fx( ) i;)θ ,y( ) i)\n.\nThelarger αisrelativeto ,themorepreviousgradientsaﬀectthecurrentdirection.\nTheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\n2 9 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n− − − 3 0 2 0 1 0 0 1 0 2 0− 3 0− 2 0− 1 001 02 0\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\novercomestheﬁrstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "function.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\nnarrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient 4.6\ndescentwithoutmomentum.\n2 9 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\nobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachinga\nterminalvelocitywherethesizeofeachstepis\n||||g\n1− α. (8.17)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "||||g\n1− α. (8.17)\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\n1 − α.For\nexample, α= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\nthegradientdescentalgorithm.\nCommonvaluesof αusedinpracticeinclude .5, .9,and .99.Likethelearning\nrate, αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\nislaterraised.Itislessimportanttoadapt αovertimethantoshrink overtime.\nAlgorithm8.2Stochasticgradientdescent(SGD)withmomentum\nRequire:Learningrate,momentumparameter.  α",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "Require:Learningrate,momentumparameter.  α\nRequire:Initialparameter,initialvelocity. θ v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nComputevelocityupdate:vvg ← α− \nApplyupdate:θθv ← +\nendwhile\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "intuitionforhowthemomentumandgradientdescentalgorithmsbehave.\nThepositionoftheparticleatanypointintimeisgivenbyθ( t).Theparticle\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\nf() = t∂2\n∂ t2θ() t . (8.18)\nRatherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\ntandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:\nv() = t∂\n∂ tθ() t , (8.19)\n2 9 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nf() = t∂\n∂ tv() t . (8.20)\nThemomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvia\nnumericalsimulation.Asimplenumericalmethodforsolvingdiﬀerentialequations\nisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedby\ntheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.\nThisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyare\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "−∇ θ J(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.\nThegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\nuntilitbeginstogouphillagain.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "untilitbeginstogouphillagain.\nOneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\notherforce,proportionalto−v( t).Inphysicsterminology,thisforcecorresponds\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "syrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\nconvergetoalocalminimum.\nWhydoweuse−v( t)andviscousdraginparticular? Partofthereasonto\nuse−v( t)ismathematical convenience—anintegerpowerofthevelocityiseasy\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "velocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\nWhentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\nalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough\n2 9 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.\n8.3.3NesterovMomentum\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\ninspiredbyNesterov’sacceleratedgradientmethod(,,).The Nesterov19832004\nupdaterulesinthiscasearegivenby:\nvv← α−∇  θ\n1\nmm \ni = 1L\nfx(( ) i;+ )θ αv ,y( ) i\n,(8.21)\nθθv ← + , (8.22)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": ",(8.21)\nθθv ← + , (8.22)\nwheretheparameters αand playasimilarroleasinthestandardmomentum\nmethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumis\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.\nThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "convergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\nbyNesterov1983().Unfortunately, inthestochasticgradientcase,Nesterov\nmomentumdoesnotimprovetherateofconvergence.\nAlgorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\nRequire:Learningrate,momentumparameter.  α\nRequire:Initialparameter,initialvelocity. θ v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondinglabelsy( ) i.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "correspondinglabelsy( ) i.\nApplyinterimupdate: ˜θθv ← + α\nComputegradient(atinterimpoint):g←1\nm∇ ˜ θ\ni L f((x( ) i;˜θy) ,( ) i)\nComputevelocityupdate:vvg ← α− \nApplyupdate:θθv ← +\nendwhile\n3 0 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.4ParameterInitializationStrategies\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\ninanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "learningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\nmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedby\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\nencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "convergestoapointwithhigh orlowcost.Also, pointsofcomparablecost\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcanaﬀectthe\ngeneralization aswell.\nModerninitialization strategiesaresimpleandheuristic.Designingimproved\ninitialization strategiesisadiﬃculttaskbecauseneuralnetworkoptimization is\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "understandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\nafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpoints\nmaybebeneﬁcialfromtheviewpointofoptimization butdetrimentalfromthe\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointaﬀects\ngeneralization isespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselect\ntheinitialpoint.\nPerhapstheonlypropertyknownwithcompletecertaintyisthattheinitial",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "parametersneedto“breaksymmetry” betweendiﬀerentunits.Iftwohidden\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\ntheseunitsmusthavediﬀerentinitialparameters. Iftheyhavethesameinitial\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerent\nupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusually",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "besttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheother\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\nofback-propagation. Thegoalofhavingeachunitcomputeadiﬀerentfunction\n3 0 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\nforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\ndiﬀerentfunctionfromeachotherunit.Randominitialization fromahigh-entropy",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "distributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\ntoassignanyunitstocomputethesamefunctionaseachother.\nTypically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\nchosenconstantsmuchlikethebiasesare.\nWealmostalwaysinitializealltheweightsin themodel tovalues drawn",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "randomly froma Gaussian oruniform distribution.Thechoiceof Gaussian\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\nlargeeﬀectonboththeoutcomeoftheoptimization procedureandontheability\nofthenetworktogeneralize.\nLargerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helping\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "back-propagationthroughthelinearcomponentofeachlayer—largervaluesinthe\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\nofthedeterministicforwardpropagationprocedureappearsrandom). Tosome\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\nLargeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.\nTheperspectivesofregularizationandoptimization cangiveverydiﬀerent\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\n3 0 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesa\npriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recall\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\ntheeﬀectofinitialization. Wecanthinkofinitializingtheparametersθtoθ 0as",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "beingsimilartoimposingaGaussianprior p(θ)withmeanθ 0.Fromthispoint\nofview,itmakessensetochooseθ 0tobenear0.Thispriorsaysthatitismore\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\npreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ 0tolarge\nvalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,and\nhowtheyshouldinteract.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "howtheyshouldinteract.\nSomeheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\nnoutputsbysamplingeachweightfrom U(−1√m,1√m),whileGlorotandBengio\n()suggestusingthe 2010 normalizedinitialization\nW i , j∼ U\n−\n6\nm n+,\n6\nm n+\n. (8.23)\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "layerstohavethesamegradientvariance.Theformulaisderivedusingthe\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\nnonlinearcounterparts.\nSaxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "ateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesof\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\nUndersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.\nIncreasingthescalingfactor gpushesthenetworktowardtheregimewhere\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "gradientsincreaseinnormastheypropagatebackward.  ()showed Sussillo2014\nthatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas\n3 0 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n1,000layers,withoutneedingtouseorthogonalinitializations. A keyinsightof\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\nbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixat\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "ariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\noptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemay\nbeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethe\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "increasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\nnotexactlyequaltothetheoreticalpredictions.\nOnedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\nsamestandarddeviation,suchas1√m,isthateveryindividualweightbecomes\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "initializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\nvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”large",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "values,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\nthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.\nWhencomputational resourcesallowit,itisusuallyagoodideatotreatthe\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "thebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\nByrepeatedlyidentifyingtheﬁrstlayerwithunacceptably smallactivationsand\n3 0 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\nactivations. Thisprocedurecaninprinciplebeautomatedandisgenerallyless\ncomputationally costlythanhyperparameter optimization basedonvalidationset\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "singlebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmore\nformallyandstudiedby (). MishkinandMatas2015\nSo far we have focused on the initialization ofthe weights.Fortunately,\ninitialization ofotherparametersistypicallyeasier.\nTheapproachforsettingthebiasesmustbecoordinatedwiththeapproach\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "initialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\nnon-zerovalues:\n•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiasto\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\nonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivation\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\nForexample,iftheoutputisadistributionoverclassesandthisdistribution",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "isahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassiﬁersbutalsoto\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\nmatchthemarginaldistributionover.x\n•Sometimeswemay wanttochoosethebiastoavoidcausingtoo much",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "saturationatinitialization. Forexample,wemaysetthebiasofaReLU\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\nThisapproachisnotcompatiblewithweightinitialization schemesthatdo\nnotexpectstronginputfromthebiasesthough.Forexample, itisnot\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\n•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "h∈[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h. We\n3 0 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ncanview hasagatethatdetermineswhether u h u≈or u h≈0. Inthese\nsituations,wewanttosetthebiasfor hsothat h≈1mostofthetimeat\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\ntheLSTMmodel,describedinsection.10.10\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\nusingthemodel",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "usingthemodel\np y y (| Nx) = (|wTx+1) b , /β (8.24)\nwhere βisaprecisionparameter.Wecanusuallyinitializevarianceorprecision\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\nenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\nBesidesthesesimpleconstantorrandommethodsofinitializingmodelparame-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "ters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\nOnecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\noﬀersfasterconvergencethanarandominitialization. Someoftheseinitialization\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "encodeinformationaboutthedistributionintheinitialparametersofthemodel.\nOthersapparentlyperformwellprimarilybecausetheysettheparameterstohave\ntherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.\n8.5AlgorithmswithAdaptiveLearningRates\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\nofthehyperparameters thatisthemostdiﬃculttosetbecauseithasasigniﬁcant\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\n3 0 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\nthecourseoflearning.\nThe algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "thenthelearningrateshoulddecrease. Ofcourse,thiskindofrulecanonlybe\nappliedtofullbatchoptimization.\nMorerecently,anumberofincremental(ormini-batch-bas ed)methodshave\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\nwillbrieﬂyreviewafewofthesealgorithms.\n8.5.1AdaGrad\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "parameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\nhavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreater\nprogressinthemoregentlyslopeddirectionsofparameterspace.\nInthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—for\ntrainingdeepneuralnetworkmodels—theaccumulation ofsquaredgradientsfrom",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "thebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\neﬀectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\nmodels.\n8.5.2RMSProp\nTheRMSPropalgorithm(,)modiﬁesAdaGradtoperformbetterin Hinton2012\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\ntoaconvexfunction. When appliedtoanon-convexfunctiontotrainaneural",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "network,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresand\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\n3 0 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.4TheAdaGradalgorithm\nRequire:Globallearningrate \nRequire:Initialparameterθ\nRequire:Smallconstant,perhaps δ 10− 7,fornumericalstability\nInitializegradientaccumulationvariabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nAccumulatesquaredgradient:rrgg ←+\nComputeupdate: ∆θ←−",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "Computeupdate: ∆θ←−\nδ +√rg.(Divisionandsquarerootapplied\nelement-wise)\nApplyupdate:θθθ ← +∆\nendwhile\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.\nRMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\nextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifit\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.\nRMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "movingaverageintroducesanewhyperparameter, ρ,thatcontrolsthelengthscale\nofthemovingaverage.\nEmpirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.\n8.5.3Adam\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\nalgorithmandispresentedinalgorithm .Thename“Adam” derivesfrom 8.7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "thephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itis\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\ndirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)of\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\n3 0 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.5TheRMSPropalgorithm\nRequire:Globallearningrate,decayrate.  ρ\nRequire:Initialparameterθ\nRequire:Smallconstant δ, usually 10− 6, usedtostabilizedivision bysmall\nnumbers.\nInitializeaccumulation variablesr= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nAccumulatesquaredgradient:rrgg ← ρ+(1 )− ρ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "Accumulatesquaredgradient:rrgg ← ρ+(1 )− ρ\nComputeparameterupdate: ∆θ=−√\nδ + rg.(1√\nδ + rappliedelement-wise)\nApplyupdate:θθθ ← +∆\nendwhile\nbiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentum\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "earlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\nthesuggesteddefault.\n8.5.4ChoosingtheRightOptimizationAlgorithm\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\nchoose?\nUnfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "presentedavaluablecomparisonofalargenumberofoptimization algorithms\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged.\nCurrently,themostpopularoptimization algorithmsactivelyinuseinclude\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\n3 0 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\nRequire:Globallearningrate,decayrate,momentumcoeﬃcient.  ρ α\nRequire:Initialparameter,initialvelocity. θ v\nInitializeaccumulation variabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputeinterimupdate: ˜θθv ← + α\nComputegradient:g←1\nm∇ ˜ θ\ni L f((x( ) i;˜θy) ,( ) i)\nAccumulategradient:rrgg ← ρ+(1 )− ρ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "Accumulategradient:rrgg ← ρ+(1 )− ρ\nComputevelocityupdate:vv← α−√rg.(1√rappliedelement-wise)\nApplyupdate:θθv ← +\nendwhile\nlargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparameter\ntuning).\n8.6ApproximateSecond-OrderMethods\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\nrisk:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "risk:\nJ() = θ E x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\nmm \ni = 1L f((x( ) i;)θ , y( ) i) .(8.25)\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\ndiscussedinchapter.7\n8.6.1Newton’sMethod\nInsection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst- 4.3\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\noptimization. Themostwidelyusedsecond-ordermethodisNewton’smethod.We",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "nowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationto\nneuralnetworktraining.\n3 1 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.7TheAdamalgorithm\nRequire:Stepsize(Suggesteddefault: )  0001 .\nRequire:Exponentialdecayratesformomentestimates, ρ 1and ρ 2in[0 ,1).\n(Suggesteddefaults:andrespectively) 09 . 0999 .\nRequire:Smallconstant δusedfornumericalstabilization.(Suggesteddefault:\n10− 8)\nRequire:Initialparametersθ\nInitialize1stand2ndmomentvariables ,s= 0r= 0\nInitializetimestep t= 0\nwhile do stoppingcriterionnotmet",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "while do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nt t←+1\nUpdatebiasedﬁrstmomentestimate:s← ρ 1s+(1− ρ 1)g\nUpdatebiasedsecondmomentestimate:r← ρ 2r+(1− ρ 2)gg\nCorrectbiasinﬁrstmoment:ˆs←s\n1 − ρt\n1\nCorrectbiasinsecondmoment:ˆr←r\n1 − ρt\n2\nComputeupdate: ∆= θ− ˆs√\nˆ r + δ(operationsappliedelement-wise)\nApplyupdate:θθθ ← +∆\nendwhile",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "Applyupdate:θθθ ← +∆\nendwhile\nNewton’smethodisanoptimization schemebasedonusingasecond-orderTay-\nlorseriesexpansiontoapproximate J(θ)nearsomepointθ 0,ignoringderivatives\nofhigherorder:\nJ J () θ≈(θ 0)+(θθ− 0)∇ θ J(θ 0)+1\n2(θθ− 0)Hθθ (− 0) ,(8.26)\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Ifwethensolvefor\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\nθ∗= θ 0−H− 1∇ θ J(θ 0) (8.27)\nThusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescaling",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "thegradientbyH− 1,Newton’smethodjumpsdirectlytotheminimum. If the\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’s\nmethod,giveninalgorithm .8.8\n3 1 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.8Newton’smethodwithobjective J(θ) =\n1\nmm\ni = 1 L f((x( ) i;)θ , y( ) i).\nRequire:Initialparameterθ 0\nRequire:Trainingsetofexamples m\nwhile do stoppingcriterionnotmet\nComputegradient:g←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nComputeHessian:H←1\nm∇2\nθ\ni L f((x( ) i;)θ ,y( ) i)\nComputeHessianinverse:H− 1\nComputeupdate: ∆= θ−H− 1g\nApplyupdate:θθθ = +∆\nendwhile\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "deﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\ningthequadraticapproximation). Second, updatetheparametersaccordingto\nequation.8.27\nInsection,wediscussedhowNewton’smethodisappropriateonlywhen 8.2.3\ntheHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjective\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\nareproblematicforNewton’smethod. IftheeigenvaluesoftheHessianarenot",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "allpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactually\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes α\nθ∗= θ 0−[(( H fθ 0))+ ] αI− 1∇ θ f(θ 0) . (8.28)\nThisregularizationstrategyisusedinapproximations toNewton’smethod,such\nastheLevenberg–Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "worksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\nvalueof αwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.\nHowever,as αincreasesinsize,theHessianbecomesdominatedbythe αIdiagonal\nandthedirectionchosenbyNewton’smethodconvergestothestandardgradient\ndividedby α. Whenstrongnegativecurvatureispresent, αmayneedtobeso\nlargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "aproperlychosenlearningrate.\nBeyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\n3 1 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsuchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneural\nnetworksislimitedbythesigniﬁcantcomputational burdenitimposes.The\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\nkcanbeinthemillions),Newton’smethodwouldrequiretheinversionofa k k×\nmatrix—with computational complexityof O( k3).Also,sincetheparameterswill",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "changewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\ncanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’s\nmethodwhileside-steppingthecomputational hurdles.\n8.6.2ConjugateGradients\nConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverse\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "approachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀective\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\nsearchdirection.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "searchdirection.\nLettheprevioussearchdirectionbed t − 1.Attheminimum,wheretheline\nsearchterminates,thedirectionalderivativeiszeroindirectiond t − 1:∇ θ J(θ)·\nd t − 1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,\nd t=∇ θ J(θ) willhavenocontributioninthedirectiond t − 1.Thusd tisorthogonal\ntod t − 1.Thisrelationshipbetweend t − 1andd tisillustratedinﬁgurefor8.6\nmultipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "orthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\nseekstoaddressthisproblem.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "seekstoaddressthisproblem.\nInthemethodofconjugategradients,weseektoﬁndasearchdirectionthat\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\n3 1 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u0000  \u0000  \u0000      \u0000 \u0000 \u0000   \nFigure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The\nmethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline\ndeﬁnedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems\nseenwithusingaﬁxedlearningrateinﬁgure,butevenwiththeoptimalstepsize 4.6\nthealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeﬁnition,at",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "theminimumoftheobjectivealongagivendirection,thegradientattheﬁnalpointis\northogonaltothatdirection.\ntheform:\nd t= ∇ θ J β ()+θ td t − 1 (8.29)\nwhere β tisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection,d t − 1,\nweshouldaddbacktothecurrentsearchdirection.\nTwodirections,d tandd t − 1,aredeﬁnedasconjugateifd\ntHd t − 1= 0,where\nHistheHessianmatrix.\nThestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\neigenvectorsofHtochoose β t,whichwouldnotsatisfyourgoalofdeveloping",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "amethodthatismorecomputationally viablethanNewton’smethodforlarge\nproblems. Canwecalculatetheconjugatedirectionswithoutresortingtothese\ncalculations?Fortunatelytheanswertothatisyes.\nTwopopularmethodsforcomputingthe β tare:\n1. Fletcher-Reeves:\nβ t=∇ θ J(θ t)∇ θ J(θ t)\n∇ θ J(θ t − 1)∇ θ J(θ t − 1)(8.30)\n3 1 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n2. Polak-Ribière:\nβ t=(∇ θ J(θ t)−∇ θ J(θ t − 1))∇ θ J(θ t)\n∇ θ J(θ t − 1)∇ θ J(θ t − 1)(8.31)\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\nthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe\nminimumalongthepreviousdirections.Asaconsequence,ina k-dimensional\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\nachievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "Algorithm8.9Theconjugategradientmethod\nRequire:Initialparametersθ 0\nRequire:Trainingsetofexamples m\nInitializeρ 0= 0\nInitialize g 0= 0\nInitialize t= 1\nwhile do stoppingcriterionnotmet\nInitializethegradientg t= 0\nComputegradient:g t←1\nm∇ θ\ni L f((x( ) i;)θ ,y( ) i)\nCompute β t=( g t − g t −1 )g t\ng\nt −1g t −1(Polak-Ribière)\n(Nonlinearconjugategradient:optionallyreset β ttozero,forexampleif tis\namultipleofsomeconstant,suchas) k k= 5\nComputesearchdirection:ρ t= −g t+ β tρ t − 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "Computesearchdirection:ρ t= −g t+ β tρ t − 1\nPerformlinesearchtoﬁnd: ∗= argmin 1\nmm\ni = 1 L f((x( ) i;θ t+ ρ t) ,y( ) i)\n(Onatrulyquadraticcostfunction,analyticallysolvefor ∗ratherthan\nexplicitlysearchingforit)\nApplyupdate:θ t + 1= θ t+ ∗ρ t\nt t←+1\nendwhile\nNonlinearConjugateGradients:Sofarwehavediscussedthemethodof\nconjugategradientsasitisappliedtoquadraticobjectivefunctions. Ofcourse,\nourprimaryinterestinthischapteristoexploreoptimization methodsfortraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\nobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.\nWithoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\n3 1 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\nsearchalongtheunalteredgradient.\nPractitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "initializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.\n2011). Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshave\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\n1993).\n8.6.3BFGS",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "1993).\n8.6.3BFGS\nTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptsto\nbringsomeoftheadvantagesofNewton’smethodwithoutthecomputational\nburden.In thatrespect, BFGSissimilartotheconjugategradientmethod.\nHowever,BFGStakesamoredirectapproachtotheapproximation ofNewton’s\nupdate.RecallthatNewton’supdateisgivenby\nθ∗= θ 0−H− 1∇ θ J(θ 0) , (8.32)\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Theprimary\ncomputational diﬃcultyinapplyingNewton’supdateisthecalculationofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "inverseHessianH− 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\namatrixM tthatisiterativelyreﬁnedbylowrankupdatestobecomeabetter\napproximationofH− 1.\nThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany\ntextbooksonoptimization, includingLuenberger1984().\nOncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\nρ tisdeterminedbyρ t=M tg t.Alinesearchisperformedinthisdirectionto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "determinethesizeofthestep, ∗,takeninthisdirection.Theﬁnalupdatetothe\nparametersisgivenby:\nθ t + 1= θ t+ ∗ρ t . (8.33)\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\n3 1 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\nonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.\nThus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\nlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\nparameters.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "parameters.\nLimited Memory BFGS (or L-BFGS)The memory costs ofthe BFGS\nalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverse\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\nthatM( 1 ) t −istheidentitymatrix,ratherthanstoringtheapproximation fromone\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGS\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "procedureremainswellbehavedwhentheminimumofthelinesearchisreached\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\n8.7OptimizationStrategiesandMeta-Algorithms\nManyoptimization techniquesarenotexactlyalgorithms, butrathergeneral\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\nincorporatedintomanydiﬀerentalgorithms.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "incorporatedintomanydiﬀerentalgorithms.\n8.7.1BatchNormalization\nBatchnormalization ( ,)isoneofthemostexcitingrecent IoﬀeandSzegedy2015\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\nbythediﬃcultyoftrainingverydeepmodels.\nVerydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "layersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.\nWhenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\n3 1 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\nanddoesnotuseanactivationfunctionateachhiddenlayer:ˆ y= x w 1 w 2 w 3 . . . w l.\nHere, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i − 1 w i.\nTheoutput ˆ yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\nweights w i.Supposeourcostfunctionhasputagradientofon1 ˆ y,sowewishto\ndecreaseˆ yslightly.Theback-propagationalgorithmcanthencomputeagradient",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "g=∇ wˆ y.Considerwhathappenswhenwemakeanupdatewwg ← − .The\nﬁrst-orderTaylorseriesapproximation ofˆ ypredictsthatthevalueofˆ ywilldecrease\nby gg.Ifwewantedtodecreaseˆ yby .1,thisﬁrst-orderinformationavailablein\nthegradientsuggestswecouldsetthelearningrate to. 1\ngg.However,theactual\nupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforder l.\nThenewvalueofˆ yisgivenby\nx w( 1−  g 1)( w 2−  g 2)( . . . w l−  g l) . (8.34)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "Anexampleofonesecond-ordertermarisingfromthisupdateis 2g 1 g 2l\ni = 3 w i.\nThistermmightbenegligibleifl\ni = 3 w iissmall,ormightbeexponentiallylarge\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\ntochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetothe\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "second-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\nevenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimization\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\nthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions. Building\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\ndoinstead?\nBatchnormalization providesanelegantwayofreparametrizing almostanydeep",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "network.Thereparametrization signiﬁcantlyreducestheproblemofcoordinating\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\nH=Hµ−\nσ, (8.35)\nwhereµisavectorcontainingthemeanofeachunitandσisavectorcontaining\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "thevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Within\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting µ j\n3 1 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nanddividingby σ j.TherestofthenetworkthenoperatesonHinexactlythe\nsamewaythattheoriginalnetworkoperatedon.H\nAttrainingtime,\nµ=1\nm\niH i , : (8.36)\nand\nσ=\nδ+1\nm\ni( )Hµ−2\ni , (8.37)\nwhere δisasmallpositivevaluesuchas10− 8imposedtoavoidencountering\ntheundeﬁnedgradientof√zat z=0.Crucially, weback-propagatethrough\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "anoperation that actssimplytoincreasethestandard deviationormeanof\nh i;thenormalization operationsremovetheeﬀectofsuchanactionandzero\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\nnormalization approach. Previous approacheshadinvolvedaddingpenaltiesto\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\nTheformerapproachusuallyresultedinimperfectnormalization andthelatter",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "usuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedly\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\nunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.\nAttesttime,µandσmaybereplacedbyrunningaveragesthatwerecollected\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\nwithoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "Revisitingtheˆ y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\ndiﬃcultiesinlearningthismodelbynormalizing h l − 1.Supposethat xisdrawn\nfromaunitGaussian.Then h l − 1willalsocomefromaGaussian,becausethe\ntransformationfrom xto h lislinear.However, h l − 1willnolongerhavezeromean\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\nˆh l − 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "updatetothelowerlayers,ˆh l − 1willremainaunitGaussian.Theoutput ˆ ymay\nthenbelearnedasasimplelinearfunction ˆ y= w lˆ h l − 1.Learninginthismodelis\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\neﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian. In\nsomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelower\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\n3 1 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofoneofthelowerweightscanﬂiptherelationshipbetweenˆ h l − 1and y. These\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\nanextremeeﬀectonthestatisticsof h l − 1.Batchnormalization hasthusmade\nthismodelsigniﬁcantlyeasiertolearn. Inthisexample,theeaseoflearningof\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\nthelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhave",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "anybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecond\norderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneural\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\nunittochange.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "unittochange.\nBecausetheﬁnallayerofthenetworkisabletolearnalineartransformation,\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\ntheinspirationforbatchnormalization. Unfortunately, eliminating alllinear\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\npracticalapproach.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "practicalapproach.\nNormalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\npowerofthe neuralnetworkcontainingthatunit.Inordertomaintainthe\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\nactivationsHwithγH+βratherthansimplythenormalizedH.Thevariables\nγandβarelearnedparametersthatallowthenewvariabletohaveanymean\nandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidweset\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "anyarbitraryvalueβ?Theansweristhatthenewparametrization canrepresent\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\nparametrization hasdiﬀerentlearningdynamics.Intheoldparametrization, the\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\ninthelayersbelowH.Inthenewparametrization, themeanofγH+βis\ndeterminedsolelybyβ.Thenewparametrization ismucheasiertolearnwith\ngradientdescent.\nMostneuralnetworklayerstaketheformof φ(XW+b)where φissome",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "ﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.It\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\nX,ortothetransformedvalueXW+b. ()recommend IoﬀeandSzegedy2015\n3 2 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversion\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\nthe βparameterappliedbythebatchnormalization reparametrization. Theinput\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\nrectiﬁedlinearfunctioninapreviouslayer. Thestatisticsoftheinputarethus\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\nsamenormalizing µand σateveryspatiallocationwithinafeaturemap,sothat\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\n8.7.2CoordinateDescent\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\nvariable x i, then minimize it with respect to another variable x jand soon,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\nonecoordinateatatime. Moregenerally,blockcoordinatedescentrefersto\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\n“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellas\nthestrictlyindividualcoordinatedescent.\nCoordinatedescentmakesthemostsensewhenthediﬀerentvariablesinthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "optimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\nsigniﬁcantlymoreeﬃcientthanoptimization withrespecttoallofthevariables.\nForexample,considerthecostfunction\nJ ,(HW) =\ni , j| H i , j|+\ni , j\nXW−H2\ni , j.(8.38)\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\ntoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "HtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\nThefunction Jisnotconvex.However, wecandividetheinputstothe\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\n3 2 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nusanoptimization strategythatallowsustouseeﬃcientconvexoptimization\nalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizing\nHWwithﬁxed.\nCoordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\nstronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunction f(x) =\n( x 1− x 2)2+ α\nx2\n1+ x2\n2\nwhere αisapositiveconstant.Theﬁrsttermencourages\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "tobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolve\ntheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.\nHowever,forsmall α,coordinatedescentwillmakeveryslowprogressbecausethe\nﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀers\nsigniﬁcantlyfromthecurrentvalueoftheothervariable.\n8.7.3PolyakAveraging\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\npoints inthe trajectory through parameter spacevisited by anoptimization",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "algorithm. If titerationsofgradientdescentvisitpointsθ( 1 ), . . . ,θ( ) t,thenthe\noutputofthePolyakaveragingalgorithmisˆθ( ) t=1\nt\niθ( ) i. Onsomeproblem\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcation\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "thelocationsoneithersideshouldbeclosetothebottomofthevalleythough.\nInnon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\nverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameter\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\nexponentiallydecayingrunningaverage:\nˆθ( ) t= αˆθ( 1 ) t −+(1 )− αθ( ) t. (8.39)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "ˆθ( ) t= αˆθ( 1 ) t −+(1 )− αθ( ) t. (8.39)\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\netal.()forarecentexample. 2015\n3 2 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.4SupervisedPretraining\nSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitious\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itis\nsometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmake\nthemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolve\nasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthat\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "trainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\npretraining.\nGreedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "ﬁne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\nﬁnds.\nPretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\ndeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithms\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\nproblems.Thisapproachisknownas . greedysupervisedpretraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "Intheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\nthelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretraining\nisillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "theﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\nAnotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\nWhy would greedy sup ervised pretraining help?The hypothesis  initially\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\n3 2 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ny y\nh( 1 )h( 1 )\nx x\n( a )U( 1 )U( 1 )\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\nx x\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\ny yh( 1 )h( 1 )\nx x\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\ny yh( 1 )h( 1 )\nx x\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\nU( 2 )U( 2 )\nW( 2 )W( 2 )\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007\n( a )Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe ( b )",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\ndiscardthehidden-to-outputlayer.Wesendtheoutputoftheﬁrsthiddenlayerasinput\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\nastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\nTofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyat\ntheendorateachstageofthisprocess.\n3 2 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\ntermsofoptimization andintermsofgeneralization.\nAnapproachrelatedtosupervisedpretrainingextendstheideatothecontext\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\nandtheninitializeasame-sizenetworkwiththeﬁrst klayersoftheﬁrstnet.All",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "thelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\nthenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000\nImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetof\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\nsection.15.2\nAnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "becomesateacherforasecondnetwork,designatedthestudent.Thestudent\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\ndiﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "parametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\ntheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayer\noftheteachernetwork. Thelowerlayersofthestudentnetworksthushavetwo\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\nanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallow",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "network,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃcultto\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\nsolvetheproblem.\n3 2 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.5DesigningModelstoAidOptimization\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\ncomefromdesigningthemodelstobeeasiertooptimize.\nInprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\ndiﬃcult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "optimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.\nSpeciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-\nformationsbetweenlayersandactivationfunctionsthatarediﬀerentiable almost",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "everywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain. Inpar-\nticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunits\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\noptimization easier.Thegradientﬂowsthroughmanylayersprovidedthatthe\nJacobianofthelineartransformationhasreasonablesingularvalues. Moreover,\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’s",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "outputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\ncorrespondsreasonablywelltomovingtowardadistantsolution.\nOthermodeldesignstrategiescanhelptomakeoptimization easier.For\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\ntheshortestpathfromthelower layer’sparameters totheoutput, and thus",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "mitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\nanddeeply-supervisednets(,).These“auxiliaryheads”aretrained Leeetal.2014\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\ntheauxiliaryheadsmaybediscarded. Thisisanalternativetothepretraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "strategies,whichwereintroducedintheprevioussection.Inthisway,onecan\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\n3 2 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\n8.7.6ContinuationMethodsandCurriculumLearning\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\nproblemistoattempttoinitializetheparametersinaregionthatisconnected",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "tothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\ndiscover.\nContinuationmethodsareafamilyofstrategiesthatcanmakeoptimization\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\nminimizeacostfunction J(θ),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "Thesecostfunctionsaredesignedtobeincreasinglydiﬃcult,with J( 0 )beingfairly\neasytominimize,and J( ) n,themostdiﬃcult,being J(θ),thetruecostfunction\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\nmeanthatitiswellbehavedovermoreofθspace.Arandominitialization ismore\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "solvinganeasyproblemthenreﬁnethesolutiontosolveincrementally harder\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.\nTraditionalcontinuationmethods(predatingtheuseofcontinuationmethods\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\nSeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\nwhichaddsnoisetotheparameters(Kirkpatrick 1983etal.,).Continuation",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "methodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\n()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\novercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedto\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\nthesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”the\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "J( ) i() = θ Eθ∼ N ( θ; θ , σ()2 i) J(θ) (8.40)\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\n3 2 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\nenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndthe\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\napproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁne\naseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfrom\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "manyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\nNP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\nnomatterhowmuchitisblurred.Considerforexamplethefunction J(θ) =−θθ.\nSecond,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "originalcostfunction.\nThoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\neliminateﬂatregions,decreasevarianceingradientestimates,improveconditioning\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "easiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\nandprogresstowardaglobalsolution.\nBengio2009etal.()observedthatanapproachcalledcurriculumlearning\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "training(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\nlearning(,;,;,). () Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009\njustiﬁedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\nincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributions\ntothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),and\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "hasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\ntasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayin\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\n3 2 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmoreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurface\nwiththelessobviouscases.Curriculum-based strategiesaremoreeﬀectivefor\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\nalsoincreasetheeﬀectivenessofotherteachingstrategies( , BasuandChristensen\n2013).\nAnotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\nstochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalways\npresentedtothelearner,butwheretheaverageproportionofthemorediﬃcult\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\nfromthefulltrainingset)wasobserved.\nWehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "regularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\nlittleornomodiﬁcation.\n3 2 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 9\nC on v ol u t i on al N e t w orks\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\ntremendouslysuccessfulinpracticalapplications.Thename“convolutionalneural",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "network”indicatesthatthenetworkemploysamathematical operationcalled\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\nmultiplicationinatleastoneoftheirlayers.\nInthis chapter, wewillﬁrst describewhatconvolutionis.Next, wewill\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "employ.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\ncorrespondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuch\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\nwillalso show how convolutionmaybeappliedtomanykindsofdata, with\ndiﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\nmoreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁc",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "principlesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\n330",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.\nResearchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\nhere.\n9.1TheConvolutionOperation\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "valuedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamples\noftwofunctionswemightuse.\nSupposewearetrackingthelocationofaspaceshipwithalasersensor.Our\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\nt.Both xand tarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelaser\nsensoratanyinstantintime.\nNowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\nestimateofthespaceship’sposition,wewouldliketoaveragetogetherseveral",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "measurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.\nWecandothiswithaweightingfunction w( a),where aistheageofameasurement.\nIfweapplysuchaweightedaverageoperationateverymoment,weobtainanew\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\ns t() =\nx a w t a d a ()( −) (9.1)\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\ndenotedwithanasterisk:",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "denotedwithanasterisk:\ns t x w t () = ( ∗)() (9.2)\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁned\nforanyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforother\npurposesbesidestakingweightedaverages.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "purposesbesidestakingweightedaverages.\nInconvolutionalnetworkterminology,theﬁrstargument(inthisexample,the\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\n3 3 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\nreferredtoasthe . f e at ur e m ap\nInourexample,theideaofalasersensorthatcanprovidemeasurements\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "integervalues.Ifwenowassumethat xand waredeﬁnedonlyoninteger t,we\ncandeﬁnethediscreteconvolution:\ns t x w t () = ( ∗)() =∞\na = − ∞x a w t a ()( −) (9.3)\nInmachinelearningapplications,theinputisusuallyamultidimensional array\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "separately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\nﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\ncanimplementtheinﬁnitesummationasasummationoveraﬁnitenumberof\narrayelements.\nFinally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\ntouseatwo-dimensionalkernel: K\nS i , j I K i , j () = ( ∗)() =\nm\nnI m , n K i m , j n . ( )( − −)(9.4)",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "m\nnI m , n K i m , j n . ( )( − −)(9.4)\nConvolutioniscommutative,meaningwecanequivalentlywrite:\nS i , j K I i , j () = ( ∗)() =\nm\nnI i m , j n K m , n . ( − −)( )(9.5)\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\nand. n\nThecommutativepropertyofconvolutionarisesbecausewehave ﬂi pp e dthe\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "inputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂip\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\n3 3 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\nbutwithoutﬂippingthekernel:\nS i , j I K i , j () = ( ∗)() =\nm\nnI i m , j n K m , n . (+ +)( )(9.6)\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\nInthistextwewillfollowthisconventionofcallingbothoperationsconvolution,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "andspecifywhetherwemeantoﬂipthekernelornotincontextswherekernel\nﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\nbasedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelative\ntothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorarefor\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "notcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelor\nnot.\nSeeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1\ntoa2-Dtensor.\nDiscreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "m at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\nmatrixmultiplication anddoesnotdependonspeciﬁcpropertiesofthematrix\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "totheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\nfurtherspecializationsinordertodealwithlargeinputseﬃciently,buttheseare\nnotstrictlynecessaryfromatheoreticalperspective.\n3 3 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\na b c d\ne f g h\ni j k lw x\ny z\na w + b x +\ne y + f za w + b x +\ne y + f zb w + c x +\nf y + g zb w + c x +\nf y + g zc w + d x +\ng y + h zc w + d x +\ng y + h z\ne w + f x +\ni y + j ze w + f x +\ni y + j zf w + g x +\nj y + k zf w + g x +\nj y + k zg w + h x +\nk y + l zg w + h x +\nk y + l zI nput\nK e r ne l\nO ut put\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestrict\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\nupper-leftregionoftheinputtensor.\n3 3 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n9.2Motivation\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\nr e pr e se n t at i o ns.Moreover, convolutionprovidesameansforworkingwith\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\nTraditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "unitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "pixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\nmemoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italso\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\nineﬃciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\nmatrixmultiplication requires m n ×parametersandthealgorithmsusedinpractice\nhave O( m n ×)runtime(perexample).Ifwelimitthenumberofconnections\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "k n ×parametersand O( k n ×)runtime.Formanypracticalapplications,itis\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\nkseveralordersofmagnitudesmallerthan m. Forgraphicaldemonstrationsof\nsparseconnectivity,seeﬁgureandﬁgure.Inadeepconvolutionalnetwork, 9.2 9.3\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\nasshowninﬁgure.Thisallowsthenetworktoeﬃcientlydescribecomplicated 9.4\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "buildingblocksthateachdescribeonlysparseinteractions.\nP ar amet e r shar i ngreferstousingthesameparameterformorethanone\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "aconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\noftheinput(exceptperhapssomeoftheboundarypixels, dependingonthe\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\n3 3 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\nandalsohighlighttheoutputunitsin sthatareaﬀectedbythisunit. ( T o p )When sis\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby 3 x.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\nalloftheoutputsareaﬀectedby x 3.\n3 3 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e :  Wehighlightoneoutputunit, s 3,\nandalsohighlighttheinputunitsin xthataﬀectthisunit.Theseunitsareknown\nasthereceptiveﬁeldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "width,onlythreeinputsaﬀect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\nconnectivityisnolongersparse,soalloftheinputsaﬀect s 3.\nx 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\nx 4 x 4h 4 h 4\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\nFigure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetwork\nislargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesif\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling 9.12",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\ninputimage.\n3 3 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\nparameterintwodiﬀerentmodels.  ( T o p )Theblackarrowsindicateusesofthecentral\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "singleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\nhasnoparametersharingsotheparameterisusedonlyonce.\nforeverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeof\nforwardpropagation—it isstill O( k n ×)—butitdoesfurtherreducethestorage\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "practicallyinsigniﬁcantcomparedto m n ×.Convolutionisthusdramatically more\neﬃcientthandensematrixmultiplication intermsofthememoryrequirements\nandstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,\nseeﬁgure.9.5\nAsanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6\nhowsparseconnectivityandparametersharingcandramatically improvethe\neﬃciencyofalinearfunctionfordetectingedgesinanimage.\nInthecaseofconvolution,theparticularformofparametersharingcausesthe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "layertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\nSpeciﬁcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).\nInthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\n3 3 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nmappingoneimagefunctiontoanotherimagefunction,suchthat I= g( I)is\ntheimagefunctionwith I( x , y)= I( x −1 , y).Thisshiftseverypixelof Ione\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\ntheresultwillbethesameasifweappliedconvolutionto I,thenappliedthe\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\nthatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeatures",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "appearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "theﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\nonanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerent\nlocations—thepartofthenetworkprocessingthetopofthefaceneedstolookfor\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\nlookforachin.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "lookforachin.\nConvolutionisnotnaturallyequivarianttosomeothertransformations,such\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\nforhandlingthesekindsoftransformations.\nFinally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedby\nmatrixmultiplication withaﬁxed-shapematrix.Convolutionenablesprocessing\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\n9.3Pooling\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "Intheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproducea\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\nanonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction.\nThisstageissometimescalledthe det e c t o rstage. Inthethirdstage,weusea\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.\nApoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\n3 3 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.6: E ﬃ c i e n c y o f e d g e d e t e c t i o n. Theimageontherightwasformedbytaking\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\nleft. Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\nTheinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "requires319 ×280 ×3=267 ,960ﬂoatingpointoperations(twomultiplicationsand\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\ntransformationwithamatrixmultiplicationwouldtake320 ×280 ×319 ×280,orover\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientfor\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\nperformsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "timesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\nandconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute.\nThematrixwouldstillneedtocontain2 ×319 ×280=178 ,640entries.Convolution\nisanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelinear\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\nGoodfellow)\n3 4 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nConvolutional Layer\nInput to layerConvolution stage:\nAne transform ﬃDetector stage:\nNonlinearity\ne.g., rectiﬁed linearPooling stageNext layer\nInput to layersConvolution layer:\nAne transform  ﬃDetector layer: Nonlinearity\ne.g., rectiﬁed linearPooling layerNext layerComplex layer terminology Simple layer terminology\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "commonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\neachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemapping\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\n( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\nnotevery“layer”hasparameters.\n3 4 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\nbasedonthedistancefromthecentralpixel.\nInallcases,poolinghelpstomaketherepresentationbecomeapproximately\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "outputsdonotchange.Seeﬁgureforanexampleofhowthisworks. 9.8 Invariance\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\nimportanttopreservethelocationofafeature.Forexample,ifwewanttoﬁnda",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "cornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreserve\nthelocationoftheedgeswellenoughtotestwhethertheymeet.\nTheuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthat\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\nassumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.\nPoolingoverspatialregionsproducesinvariancetotranslation,butifwepool\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "whichtransformationstobecomeinvariantto(seeﬁgure).9.9\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\nﬁgureforanexample.Thisimprovesthecomputational eﬃciencyofthe 9.10\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "whenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\nreductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyand\nreducedmemoryrequirementsforstoringtheparameters.\nFormanytasks,poolingisessentialforhandlinginputsofvaryingsize. For\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcation\nlayermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\noﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "samenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\nﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummary\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\n3 4 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n0. 1 1. 0. 21. 1. 1.\n0. 10. 2\n. . . . . .. . . . . .\n0. 3 0. 1 1.1. 0. 3 1.\n0. 21.\n. . . . . .. . . . . .D E T E C T O R   S T A GEP O O L I N G  ST A GE\nP O O L I N G  ST A GE\nD E T E C T O R   S T A GE\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "andapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\n3 4 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nL ar ge   r e s pon s e\ni n  po ol i ng uni tL ar ge   r e s pon s e\ni n  po ol i ng uni t\nL ar ge\nr e s ponse\ni n  de t e c t or\nuni t   1L ar ge\nr e s ponse\ni n  de t e c t or\nuni t   3\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\ntheinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearn",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "tobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5.\nEachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsin\ntheinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetector\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\nwasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resulting\nintwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "thesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\ntransformations.\n0. 1 1. 0. 21. 0. 2\n0. 10. 1\n0. 0 0. 1\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "oftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\nwanttoignoresomeofthedetectorunits.\n3 4 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\ndiﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearna\nsinglepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection. The20.6\ninverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswill\nbecoveredinsection.20.10.6\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcation\nusingconvolutionandpoolingareshowninﬁgure.9.11",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "usingconvolutionandpoolingareshowninﬁgure.9.11\n9.4Convolutionand Pooling asan InﬁnitelyStrong\nPrior\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\naboutwhatmodelsarereasonable,beforewehaveseenanydata.\nPriorscanbeconsideredweakorstrongdependingonhowconcentratedthe\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "entropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\nmoreactiveroleindeterminingwheretheparametersendup.\nAninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\nsupportthedatagivestothosevalues.\nWecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "butwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongprior\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\nexceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathidden\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitely\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\n3 4 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nInput image: \n256x256x3Output of \nconvolution + \nReLU: 256x256x64Output of pooling \nwith stride 4: \n64x64x64Output of \nconvolution + \nReLU: 64x64x64Output of pooling \nwith stride 4: \n16x16x64Output of reshape to \nvector:\n16,384 unitsOutput of matrix \nmultiply: 1,000 unitsOutput of softmax: \n1,000 class \nprobabilities\nInput image: \n256x256x3Output of \nconvolution + \nReLU: 256x256x64Output of pooling \nwith stride 4: \n64x64x64Output of \nconvolution +",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "with stride 4: \n64x64x64Output of \nconvolution + \nReLU: 64x64x64Output of pooling to \n3x3 grid: 3x3x64Output of reshape to \nvector:\n576 unitsOutput of matrix \nmultiply: 1,000 unitsOutput of softmax: \n1,000 class \nprobabilities\nInput image: \n256x256x3Output of \nconvolution + \nReLU: 256x256x64Output of pooling \nwith stride 4: \n64x64x64Output of \nconvolution + \nReLU: 64x64x64Output of \nconvolution:\n16x16x1,000Output of average \npooling: 1x1x1,000Output of softmax: \n1,000 class \nprobabilities",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "1,000 class \nprobabilities\nOutput of pooling \nwith stride 4: \n16x16x64\nFigure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.The\nspeciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyare\ndesignedtobeveryshallowinordertoﬁtontothepage. Realconvolutionalnetworks\nalsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresused\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesaﬁxedimagesize.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\nconvolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therest\nofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinchapter.6\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\nbutaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "fullyconnectedportionofthenetwork. Aconvolutionalnetworkthatdoesnot ( R i g h t )\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\ntheargumenttothesoftmaxclassiﬁeratthetop.\n3 4 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\nequivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongprior\nthateachunitshouldbeinvarianttosmalltranslations.\nOfcourse,implementing aconvolutionalnetasafullyconnectednetwithan\ninﬁnitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\nofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcan\ngiveussomeinsightsintohowconvolutionalnetswork.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "giveussomeinsightsintohowconvolutionalnetswork.\nOnekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting. Like\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.\nSomeconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "highlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslation\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\ninappropriate.\nAnotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "areseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\nofspatialrelationshipshard-codedintothembytheirdesigner.\n9.5VariantsoftheBasicConvolutionFunction\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\nunderstoodinthemathematical literature.Thefunctionsusedinpracticediﬀer",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "slightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeuseful\npropertiesofthefunctionsusedinneuralnetworks.\nFirst,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\nnetworktoextractmanykindsoffeatures,atmanylocations.\n3 4 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\ngridofvector-valuedobservations. Forexample,acolorimagehasared,green\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\ntothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutput\nofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,we\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "oneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinates\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesin\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\nBecauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\nkernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeif",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "eachoperationhasthesamenumberofoutputchannelsasinputchannels.\nAssumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\ninput,withanoﬀsetof krowsand lcolumnsbetweentheoutputunitandthe\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "acrosswithoutﬂipping,then V K\nZ i , j , k=\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n (9.7)\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\narraysusingafortheﬁrstentry.Thisnecessitatesthe 1 −1intheaboveformula.\nProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\ntheaboveexpressionevensimpler.\nWemaywanttoskipoversomepositionsofthekernelinordertoreducethe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "computational cost(attheexpenseofnotextractingourfeaturesasﬁnely).We\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\ndeﬁneadownsampledconvolutionfunctionsuchthat c\nZ i , j , k= ( ) c K V , , s i , j , k=\nl , m , n\nVl , j s m , k s n ( − × 1 ) + ( − × 1 ) + K i , l , m , n\n.(9.8)\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\n3 4 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\ntodeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan9.12\nillustration.\nOneessentialfeatureofanyconvolutionalnetworkimplementationistheability\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\nateachlayer. Zeropaddingtheinputallowsustocontrolthekernelwidthand\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "choosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\nkernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork.\nSeeﬁgureforanexample. 9.13\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "thiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\nthekernelhaswidth k,theoutputwillbeofwidth m k −+1. Therateofthis\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "eventuallydropto1 ×1,atwhichpointadditionallayerscannotmeaningfully\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "inﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\ninanoutputimageofwidth m+ k −1.Inthiscase,theoutputpixelsnearthe\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\ncanmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsin",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "theconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\ntermsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”\nconvolution.\n3 4 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\nx 4 x 4 x 5 x 5s 3 s 3\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\nx 4 x 4z 4 z 4\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\nc onv ol ut i on\nD ow nsampl i n g\nC onv ol ut i on\nFigure 9.12:Convolution witha stride.Inthisexample,we use astride oftwo.\n( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation.  ( Bot-\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "convolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\nthatarethendiscarded.\n3 5 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n. . . . . .. . .\n. . . . . .. . . . . .. . . . . .\nFigure9.13: T h e e ﬀ e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\nonlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\nshrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "abletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\nshrinkingisinevitableinthiskindofarchitecture. Byaddingﬁveimplicitzeroes ( Bottom )\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\nmakeanarbitrarilydeepconvolutionalnetwork.\n3 5 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁed\nbya6-Dtensor W. Theindicesinto Warerespectively: i,theoutputchannel,\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoﬀset\nwithintheinput,and n,thecolumnoﬀsetwithintheinput.Thelinearpartofa\nlocallyconnectedlayeristhengivenby\nZ i , j , k=",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "locallyconnectedlayeristhengivenby\nZ i , j , k=\nl , m , n[ V l , j m , k n + − 1 + − 1 w i , j , k, l , m , n] . (9.9)\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\nconnections.\nLocallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "featureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\nimage.\nItcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\nwaytodothisistomaketheﬁrst moutputchannelsconnecttoonlytheﬁrst\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "inputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions 9.15\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\nreducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreduces\ntheamountofcomputationneededtoperformforwardandback-propagation. It\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.\nT i l e d c o n v o l ut i o n( ,;,)oﬀersacom- GregorandLeCun2010aLeetal.2010\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "learningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\nneighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\nfeaturemap.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiled 9.16\nconvolution,andstandardconvolution.\n3 5 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na                b a                b a                b a                b a               a                b c             d e           f g            h   i    \nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\nFigure9.14:Comparisonoflocalconnections,convolution,andfullconnections.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith\nauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.\n( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly\nthesameconnectivityasthelocallyconnectedlayer.Thediﬀerenceliesnotinwhichunits\ninteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer\nhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "acrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.\n( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach\nedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis\ndiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected\nlayer.\n3 5 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nI nput T e nsorO ut put T e nsor\nS p a t i a l   c o o r d i n a t e sC h a n n e l   c o o r d i n a t e s\nFigure9.15: Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedto\nonlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\nthesecondtwoinputchannels.\n3 5 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na                b a                b a                b a                b a             a                b c             d e           f g            h   i    \nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "x 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na                b c                 d a                b c                 d a               \nFigure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard\nconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame\nsizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.\nThediﬀerencesbetweenthemethodsliesinhowtheyshareparameters. ( T o p )Alocally",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "connectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight\nbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r )\ntdiﬀerentkernels.Hereweillustratethecaseof t= 2. Oneofthesekernelshasedges\nlabeled“a”and“b,”whiletheotherhasedgeslabeled“c”and“d.” Eachtimewemoveone\npixeltotherightintheoutput,wemoveontousingadiﬀerentkernel.Thismeansthat,\nlikethelocallyconnectedlayer,neighboringunitsintheoutputhavediﬀerentparameters.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "Unlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels,\nwecyclebacktotheﬁrstkernel.Iftwooutputunitsareseparatedbyamultipleof t\nsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom )\nconvolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated\ninthediagrambyusingthekernelwithweightslabeled“a”and“b”everywhere.\n3 5 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nTodeﬁnetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\nthedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthan\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\nthroughasetof tdiﬀerentchoicesofkernelstackineachdirection.If tisequalto\ntheoutputwidth,thisisthesameasalocallyconnectedlayer.\nZ i , j , k=\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "whereis themodulooperation,with % t% t=0(, t+1)% t=1,etc.It is\nstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeach\ndimension.\nBothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\ndiﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsof\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\nlearnedtransformation(seeﬁgure).Convolutionallayersarehard-codedtobe 9.9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "invariantspeciﬁcallytotranslation.\nOtheroperationsbesidesconvolutionareusuallynecessarytoimplementa\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\nInsomesimplecases, thisoperationcanbeperformedusingtheconvolution\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\ndonothavethisproperty.\nRecallthatconvolutionisalinearoperationandcanthusbedescribedasa",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "matrixmultiplication (ifweﬁrstreshapetheinputtensorintoaﬂatvector).The\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\nnetwork.\nMultiplication bythetransposeofthematrixdeﬁnedbyconvolutionisone\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "thathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\n3 5 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’s\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "resultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\ntoldwhatthesizeoftheoriginalinputwas.\nThesethreeoperations—convolution,backpropfromoutputtoweights,and\nbackpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradients\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\nconvolution. See ()forafullderivationoftheequationsinthe Goodfellow2010",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "fullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\nequationswork,wepresentthetwodimensional,singleexampleversionhere.\nSupposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\ndeﬁnedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "computethecostfunction J.Duringback-propagation, wewillreceiveatensor G\nsuchthat G i , j , k=∂\n∂ Z i , j , kJ , . ( V K)\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\nweightsinthekernel.Todoso,wecanuseafunction\ng , , s ( G V) i , j , k, l=∂\n∂ K i , j , k, lJ ,( V K) =\nm , nG i , m , n V j , m s k, n s l ( − × 1 ) + ( − × 1 ) + .(9.11)\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "Todoso,wecanuseafunction\nh , , s ( K G) i , j , k=∂\n∂ V i , j , kJ ,( V K) (9.12)\n=\nl , m\ns . t .\n( 1 ) + = l − × s m j\nn , p\ns . t .\n( 1 ) + = n − × s p k\nqK q , i , m , p G q , l , n .(9.13)\nAutoencodernetworks, describedinchapter, arefeedforwardnetworks 14\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\n3 5 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\nWW x.Itiscommonformore general autoencoders tousemultiplication\nbythetransposeoftheweightmatrixjustasPCAdoes. Tomakesuchmodels\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwedeﬁne\nareconstruction\nR K H = ( h , , s .) (9.14)\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "to Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\ndiﬀerentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\nback-propagationalgorithmonanystandardnetworkarchitectures.\nGenerally,wedonotuseonlyalinearoperationinordertotransformfrom\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "biastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\nofhowtoshareparametersamongthebiases. Forlocallyconnectedlayersitis\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxed\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "Separatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,but\nalsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerent\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.\n9.6StructuredOutputs\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\nobject,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorareal\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "standardconvolutionallayer.Forexample,themodelmightemitatensor S,where\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\nthatfollowtheoutlinesofindividualobjects.\nOneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\n3 5 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nˆ Y( 1 )ˆ Y( 1 )ˆ Y( 2 )ˆ Y( 2 )ˆ Y( 3 )ˆ Y( 3 )\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\nXXU U UV V V W W\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\nchannels(red,green,blue).Thegoalistooutputatensoroflabelsˆ Y,withaprobability\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "imagecolumns,andthediﬀerentclasses.Ratherthanoutputtingˆ Yinasingleshot,the\nrecurrentnetworkiterativelyreﬁnesitsestimateˆ Ybyusingapreviousestimateofˆ Y\nasinputforcreatinganewestimate. Thesameparametersareusedforeachupdated\nestimate,andtheestimatecanbereﬁnedasmanytimesaswewish.Thetensorof\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "hiddenvalues.Onallbuttheﬁrststep,thekernels Wareconvolvedoverˆ Ytoprovide\ninputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Because\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\ndescribedinchapter.10\ninputplane,asshowninﬁgure.Inthekindsofarchitectures typicallyusedfor 9.13\nclassiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatial\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "ordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\nuseapoolingoperatorwithunitstride.\nOnestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\noftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetween\nneighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondsto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "usingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\nthearchitectureofsucharecurrentconvolutionalnetwork.\n3 5 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).\nThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "canbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\n9.7DataTypes\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\neachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspace\nortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities 9.1\nandnumberofchannels.\nForanexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\n().2010\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "datahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\nevenwhencomputational costandoverﬁttingarenotsigniﬁcantissues.\nForexample,consideracollectionofimages,whereeachimagehasadiﬀerent\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "ﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\ndiﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\nmultiplication; thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblock\ncirculantmatrixforeachsizeofinput. Sometimes theoutputofthenetworkis\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "necessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,for\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\nmaintainaﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategy\nareshowninﬁgure.9.11\n3 6 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nSinglechannel Multi-channel\n1-DAudio waveform:The axis we\nconvolveovercorrespondsto\ntime.Wediscretizetimeand\nmeasuretheamplitudeofthe\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\ntionsof3-Dcomputer-rendered\ncharactersaregeneratedbyalter-\ningtheposeofa“skeleton”over\ntime.Ateachpointintime,the\nposeofthecharacterisdescribed\nbyaspeciﬁcationoftheanglesof\neachofthejointsinthecharac-\nter’sskeleton.Eachchannelin\nthedatawefeedtotheconvolu-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "thedatawefeedtotheconvolu-\ntionalmodelrepresentstheangle\naboutoneaxisofonejoint.\n2-DAudiodatathathasbeenprepro-\ncessedwithaFouriertransform:\nWecantransformtheaudiowave-\nformintoa2Dtensorwithdif-\nferentrowscorrespondingtodif-\nferentfrequencies anddiﬀerent\ncolumnscorrespondingtodiﬀer-\nentpointsintime.Usingconvolu-\ntioninthetimemakesthemodel\nequivarianttoshiftsintime.Us-\ningconvolutionacrossthefre-\nquencyaxismakesthemodel\nequivarianttofrequency,sothat\nthesamemelodyplayedinadif-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "thesamemelodyplayedinadif-\nferentoctaveproducesthesame\nrepresentationbutatadiﬀerent\nheightinthenetwork’soutput.Colorimagedata:Onechannel\ncontainstheredpixels,onethe\ngreen pixels, and one theblue\npixels.Theconvolutionkernel\nmovesoverboththehorizontal\nandverticalaxesofthe image,\nconferringtranslationequivari-\nanceinbothdirections.\n3-DVolumetricdata:Acommon\nsourceofthiskindofdataismed-\nicalimagingtechnology,suchas\nCTscans.Colorvideodata:Oneaxiscorre-\nspondstotime,onetotheheight",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "spondstotime,onetotheheight\nofthevideoframe,andoneto\nthewidthofthevideoframe.\nTable9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional\nnetworks.\n3 6 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\nofobservationofthesamekindofthing—diﬀeren tlengthsofrecordingsover\ntime,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerent\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "ourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\ncorrespondingtothetestscores.\n9.8EﬃcientConvolutionAlgorithms\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\nresources,asdiscussedinsection,areessential. However,inmanycasesit 12.1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "isalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\nalgorithm.\nConvolutionisequivalenttoconvertingboththeinputandthekerneltothe\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\nofthetwosignals, andconvertingbacktothetimedomainusinganinverse\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\nimplementationofdiscreteconvolution.\nWhena d-dimensionalkernelcanbeexpressedas theouterproductof d",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "vectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\nkernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocompose d\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\nissigniﬁcantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\nIfthekernelis welementswideineachdimension,thennaivemultidimensional\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "convolutionrequires O( w d ×)runtimeandparameterstoragespace.Ofcourse,\nnoteveryconvolutioncanberepresentedinthisway.\nDevisingfasterwaysofperformingconvolutionorapproximateconvolution\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\nniquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecause\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\nanetworkthantoitstraining.\n3 6 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n9.9RandomorUnsupervisedFeatures\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "trainingistousefeaturesthatarenottrainedinasupervisedfashion.\nTherearethreebasicstrategiesforobtaining con volutionkernelswithout\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel. PartIII",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "describesmanymoreunsupervisedlearningapproaches.Learningthefeatures\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\nclassiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\nassumingthelastlayerissomethinglikelogisticregressionoranSVM.\nRandomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "etal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.\n()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\nTheyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\naconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutional\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "Anintermediate approachistolearnthefeatures,butusingmethodsthatdo\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayer\ninisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthe\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "canonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworksoﬀer Leeetal.2009\n3 6 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\nWecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernels\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "process.Usingthisapproach,wecantrainverylargemodelsandincurahigh\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\n2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\nfromroughly2007–2013,whenlabeleddatasetsweresmallandcomputational\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\nentirenetworkoneachtrainingiteration.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "entirenetworkoneachtrainingiteration.\nAswithotherapproachestounsupervisedpretraining,itremainsdiﬃcultto\nteaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervised\npretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmay\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\ncostofthelearningrule.\n9.10TheNeuroscientiﬁcBasisforConvolutionalNet-\nworks\nConvolutional networksare perhaps the greatest successstory ofbiologically",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "inspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguided\nbymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworkswere\ndrawnfromneuroscience.\nThehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperiments\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "Wiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\naNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporary\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\ncats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojected\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁc\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "otherpatterns.\n3 6 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\nfocusonasimpliﬁed,cartoonviewofbrainfunction.\nInthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknown\nasthe pr i m ar y v i sual c o r t e x. V1istheﬁrstareaofthebrainthatbeginsto\nperformsigniﬁcantlyadvancedprocessingofvisualinput. Inthiscartoonview,\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "light-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\ncalledthelateralgeniculatenucleus. Themainrole,asfarasweareconcerned\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\ntheeyetoV1,whichislocatedatthebackofthehead.\nAconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\nmirroring the structure of theimage in the retina.For example, light\narrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfof\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\ndeﬁnedintermsoftwodimensionalmaps.\n2.V1containsmany si m pl e c e l l s.Asimplecell’sactivitycantosomeextent\nbecharacterizedbyalinear function oftheimagein asmall, spatially",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "localizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkare\ndesignedtoemulatethesepropertiesofsimplecells.\n3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\nlayersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconcept",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "andareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\n3 6 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nnicknamed“grandmother cells”—theideaisthatapersoncouldhaveaneuronthat\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\nshadow,etc.\nThesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\nTheyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividual\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuronﬁreswhena\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\nthewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.\nThesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "convolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\nnetwork’slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\n(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughthe\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst\n100msofglimpsinganobject. Ifapersonisallowedtocontinuelookingatthe\nobjectformoretime,theninformationwillbegintoﬂowbackwardsasthebrain",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "usestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.\nHowever,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthat\nresultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobe\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\nﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\nrecognitiontasks(,). DiCarlo2013\nThatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "andthemammalianvisionsystem.Someofthesediﬀerencesarewellknown\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\nthesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthe\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\n•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\n3 6 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "•Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\nsofararepurelyvisual.\n•Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\nabletounderstandentirescenesincludingmanyobjectsandrelationships\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "•EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\nhasnotyetbeenshowntooﬀeracompellingimprovement.\n•WhilefeedforwardITﬁringratescapturemuchofthesameinformationas\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\ncomputations are.Thebrainprobablyusesverydiﬀerentactivationand\npoolingfunctions.Anindividualneuron’sactivationprobablyisnotwell-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "characterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involves\nmultiplequadraticﬁltersforeachneuron(,).Indeedour Rustetal.2005\ncartoonpictureof“simplecells” and“complexcells” mightcreateanon-\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\nkindofcellbutwiththeir“parameters”enablingacontinuumofbehaviors\nrangingfromwhatwecall“simple”towhatwecall“complex.”\nItis alsoworthmentioningthatneuroscience hastold usrelativelylittle",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "abouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\nalgorithm.\n3 6 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nLang andHinton 1988()introducedthe use ofback-propagation totrain\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "themodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\nconvolutionappliedtoimages.\nSofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciﬁc\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "Inadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionof\nindividualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheir\nresponsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecan\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’s",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "retina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\nthenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximation of\ntheneuron’sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\nandShapley2004,).\nReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\nby G ab o r f unc t i o ns. TheGaborfunctiondescribestheweightata2-Dpoint\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "I( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\nlocations,deﬁnedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\nofview,theresponseofasimplecelltoanimageisgivenby\ns I() =\nx ∈ X\ny ∈ Yw x , y I x , y . ()() (9.15)\nSpeciﬁcally,takestheformofaGaborfunction: w x , y()\nw x , y α , β (; x , β y , f , φ , x 0 , y 0 , τ α) = exp− β x x 2− β y y 2\ncos( f x+) φ ,(9.16)\nwhere",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "cos( f x+) φ ,(9.16)\nwhere\nx= ( x x − 0)cos()+( τ y y − 0)sin() τ (9.17)\n3 6 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nand\ny= ( − x x − 0)sin()+( τ y y − 0)cos() τ . (9.18)\nHere, α, β x, β y, f, φ, x 0, y 0,and τareparametersthatcontroltheproperties\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\ndiﬀerentsettingsoftheseparameters.\nTheparameters x 0, y 0,and τdeﬁneacoordinatesystem. Wetranslateand\nrotate xand ytoform xand y.Speciﬁcally,thesimplecellwillrespondtoimage\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "aswemovealongalinerotatedradiansfromthehorizontal. τ\nViewedasafunctionof xand y,thefunction wthenrespondstochangesin\nbrightnessaswemovealongthe xaxis. Ithastwoimportantfactors:oneisa\nGaussianfunctionandtheotherisacosinefunction.\nTheGaussianfactor αexp\n− β x x 2− β y y 2\ncanbeseenasagatingtermthat\nensuresthesimplecellwillonlyrespondtovaluesnearwhere xand yareboth\nzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactor",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "αadjuststhetotalmagnitudeofthesimplecell’sresponse,while β xand β ycontrol\nhowquicklyitsreceptiveﬁeldfallsoﬀ.\nThecosinefactor cos( f x+ φ) controlshowthesimplecellrespondstochanging\nbrightnessalongthe xaxis.Theparameter fcontrolsthefrequencyofthecosine\nandcontrolsitsphaseoﬀset. φ\nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\ntoaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁc\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "hasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—when\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\nnegative.\nThecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\n2-Dvectorcontainingtwosimplecells’responses: c( I)=\ns 0() I2+ s 1() I2. An\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "for φ,and φissetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldeﬁnedinthisway\nrespondswhentheGaussianreweightedimage I( x , y)exp( − β x x 2− β y y 2) contains\nahighamplitudesinusoidalwavewithfrequency findirection τnear ( x 0 , y 0),\nregardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellis\ninvarianttosmalltranslationsoftheimageindirection τ,ortonegatingtheimage\n3 6 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters\nthatcontrolthecoordinatesystem: x 0, y 0,and τ. EachGaborfunctioninthisgridis\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and τischosenso\nthateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "Fortheothertwoplots, x 0, y 0,and τareﬁxedtozero.  Gaborfunctionswith ( C e n t e r )\ndiﬀerentGaussianscaleparameters β xand β y.Gaborfunctionsarearrangedinincreasing\nwidth(decreasing β x)aswemovelefttorightthroughthegrid,andincreasingheight\n(decreasing β y)aswemovetoptobottom.Fortheothertwoplots,the βvaluesareﬁxed\nto1.5 ×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters ( R i g h t ) f\nand φ.Aswemovetoptobottom, fincreases,andaswemovelefttoright, φincreases.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "Fortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth. φ f ×\n(replacingblackwithwhiteandviceversa).\nSomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\nasimpleunsupervisedlearningalgorithm, sparse coding,learnsfeatureswith\nreceptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "anextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\nlearningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19\nshowssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedge\ndetectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe\n“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "whenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerent\napproachestostatisticalmodeling.SeeHyvärinen 2009etal.()forareviewofthe\nﬁeldofnaturalimagestatistics.\n3 7 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁc\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\nimagepatches. ( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "convolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit.\n9.11ConvolutionalNetworksandtheHistoryofDeep\nLearning\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\ntheﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\nconsideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneural",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "networkstosolveimportantcommercialapplicationsandremainattheforefront\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "andmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\nforamorein-depthhistoryofconvolutionalnetworksupto2010.\nConvolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.\n()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\n3 7 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\nlessimpactforyearsearlier.\nConvolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwith\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\nsimplybethatconvolutionalnetworksweremorecomputationally eﬃcientthan\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "andtunetheirimplementation andhyperparameters.Largernetworksalsoseem\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouse",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\nConvolutionalnetworksprovideawaytospecializeneuralnetworkstowork\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "anotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\nnetworks.\n3 7 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1\nI n t ro d u ct i on\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\nbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\nGalatea,Talos,andPandoramayallberegardedasartiﬁciallife( , OvidandMartin\n2004Sparkes1996Tandy1997 ;,;,).\nWhenprogrammable computerswereﬁrstconceived,peoplewonderedwhether\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "built(Lovelace1842,).Today, ar t i ﬁc i al i n t e l l i g e nc e(AI)isathrivingﬁeldwith\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\ninmedicineandsupportbasicscientiﬁcresearch.\nIntheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolved\nproblemsthatareintellectually diﬃcultforhumanbeingsbutrelativelystraight-\nforwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "ematicalrules. Thetruechallengetoartiﬁcialintelligenceprovedtobesolving\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\nformally—probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\nspokenwordsorfacesinimages.\nThisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\nhierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimpler",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "concepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\n1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\nthisreason,wecallthisapproachtoAI . deep l e ar ni ng\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\ntheworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworld\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "inonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis a\ntremendousaccomplishment, butthechallengeisnotduetothediﬃcultyof\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\nprovidedaheadoftimebytheprogrammer.\nIronically,abstractandformaltasksthatareamongthemostdiﬃcultmental\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "havelongbeenabletodefeateventhebesthumanchessplayer,butareonly\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\norspeech.Aperson’severydayliferequiresanimmenseamountofknowledge\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\ndiﬃculttoarticulateinaformalway.Computersneedtocapturethissame\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\nartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "Severalartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabout\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\nk no wl e dge baseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasled\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\ncalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisan",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "unwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\nenginedetectedaninconsistencyinthestory: itknewthatpeopledonothave\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\nentity“FredWhileShaving”containedelectricalparts.Itthereforeaskedwhether\nFredwasstillapersonwhilehewasshaving.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "Fredwasstillapersonwhilehewasshaving.\nThediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\n2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.\nTheperformanceofthesesimplemachinelearningalgorithmsdependsheavily",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "onthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e.\nLogisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\nvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesare",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "deﬁnedinanyway. IflogisticregressionwasgivenanMRIscanofthepatient,\nratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeuseful\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\ncomplications thatmightoccurduringdelivery.\nThisdependenceonrepresentationsisageneralphenomenon thatappears\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "thecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\narithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuch\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\nenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimple\nvisualexample,seeﬁgure.1.1\nManyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetof\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfrom\nsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrong\nclueastowhetherthespeakerisaman,woman,orchild.\nHowever,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted.\nForexample,supposethatwewouldliketowriteaprogramtodetectcarsin\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\nofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactlywhata",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "wheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀ\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\nobscuringpartofthewheel,andsoon.\n3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n                \n                \nFigure1.1:Exampleofdiﬀerentrepresentations:supposewewanttoseparatetwo\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "Onesolutiontothisproblemistousemachinelearningtodiscovernotonly\nthemappingfromrepresentationtooutputbutalsotherepresentationitself.\nThisapproachisknownas r e pr e se n t at i o n l e ar ni ng. Learnedrepresentations\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "months.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\nhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.\nThequintessentialexampleofarepresentationlearningalgorithmisthe au-\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\nconvertstheinputdataintoadiﬀerentrepresentation,anda dec o derfunction\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "throughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\nrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimto\nachievediﬀerentkindsofproperties.\nWhendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\ncontext,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\n4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\nobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.\nTheymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\nWhenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’s",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "age,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\nandtheangleandbrightnessofthesun.\nAmajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplications\nisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweare\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\ntoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "Mostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\nonesthatwedonotcareabout.\nOfcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeatures\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,\ncanbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingof\nthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvethe\noriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "D e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\nDeeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\nwhichareinturndeﬁnedintermsofedges.\nThequintessentialexampleofadeeplearningmodelisthefeedforwarddeep",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "networkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\napplicationofadiﬀerentmathematical functionasprovidinganewrepresentation\noftheinput.\nTheideaoflearningtherightrepresentationforthedataprovidesoneperspec-\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "computertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\ncanbethoughtofasthestateofthecomputer’smemoryafterexecutinganother\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\ninstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselater\n5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nVisible layer\n(input pixels)1st hidden layer\n(edges)2nd hidden layer\n(corners and\ncontours)3rd hidden layer\n(object parts)CARPERSONANIMALOutput\n(object identity)\nFigure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstand\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Deeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoa\nseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.The\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\nfeaturesfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiven\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "therelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasily\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthidden\nlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\nlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "candetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursand\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\nfromZeilerandFergus2014().\n6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nx 1 x 1σ\nw 1 w 1×\nx 2 x 2 w 2 w 2×+El e me n t\nS e t\n+\n×\nσ\nxx wwEl e me n t\nS e t\nL ogi s t i c\nR e gr e s s i onL ogi s t i c\nR e gr e s s i on\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\noutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.\nThecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "σ ( wTx ),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\ninstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\nviewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarily\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "stateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.\nThisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,\nbutithelpsthemodeltoorganizeitsprocessing.\nTherearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewis\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "aﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgiven\nitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengths\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\nbedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionswe\nallowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis 1.3\nchoiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.\nAnotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "modelasbeingnotthedepthofthecomputational graphbutthedepthofthe\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\n7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\noftheﬂowchartofthecomputations neededtocomputetherepresentationof\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\nThisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁned\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.\nAfterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\npresentaswell. Inthiscase,thegraphofconceptsonlyincludestwolayers—a",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "layerforeyesandalayerforfaces—butthegraphofcomputations includes 2n\nlayersifwereﬁneourestimateofeachconceptgiventheothertimes. n\nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthe\ncomputational graph,orthedepthoftheprobabilisticmodelinggraph—ismost\nrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelements\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "acomputerprogram. Nor isthereaconsensusabouthowmuchdepthamodel\nrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthe\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.\nTosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\nSpeciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputer\nsystemstoimprovewithexperienceanddata. Accordingtotheauthorsofthis",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "book,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\nkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningto\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedin\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\noflessabstractones.Figureillustratestherelationshipbetweenthesediﬀerent 1.4\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "1. 1 Wh o S h ou l d R ead T h i s Bo ok ?\nThisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\narebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.The\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "learningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\n8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nAIMachine learningRepresentation learningDeep learning\nExample:\nKnowledge\nbasesExample:\nLogistic\nregressionExample:\nShallow\nautoencoders Example:\nMLPs\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\n9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nInputHand-\ndesigned \nprogramOutput\nInputHand-\ndesigned \nfeaturesMapping from \nfeaturesOutput\nInputFeaturesMapping from \nfeaturesOutput\nInputSimple \nfeaturesMapping from \nfeaturesOutput\nAdditional \nlayers of more \nabstract \nfeatures\nRule-based\nsystemsClassic\nmachine\nlearning Representation\nlearningDeep\nlearning\nFigure1.5: FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeach\notherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\nlearnfromdata.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "learnfromdata.\n1 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\nsearchengines,onlineadvertisingandﬁnance.\nThisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "widelybelievedtobeimportantforfutureresearchindeeplearning.\nReadersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\nchapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization 1.6\nofthebook.\nWedoassumethatallreaderscomefromacomputersciencebackground. We",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "assumefamiliaritywithprogramming, abasicunderstandingofcomputational\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\nterminologyofgraphtheory.\n1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\n•Deeplearninghashadalongandrichhistory,buthasgonebymanynames\nreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedin\npopularity.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "popularity.\n•Deeplearninghasbecomemoreusefulastheamountofavailabletraining\ndatahasincreased.\n•Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\n(bothhardwareandsoftware)fordeeplearninghasimproved.\n•Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\naccuracyovertime.\n1 1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1. Introduction\nPart I: Applied Math and Machine Learning Basics\n2. Linear Algebra3. Probability and \nInformation Theory\n4. Numerical \nComputation5. Machine Learning \nBasics\nPart II: Deep Networks: Modern Practices\n6. Deep Feedforward \nNetworks\n7. Regularization8. Optimization 9.  CNNs10.  RNNs\n11. Practical \nMethodology12. Applications\nPart III: Deep Learning Research\n13. Linear Factor \nModels14. Autoencoders15. Representation \nLearning\n16. Structured",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Learning\n16. Structured \nProbabilistic Models17. Monte Carlo \nMethods\n18. Partition \nFunction19. Inference\n20. Deep Generative \nModels\nFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother\nindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.\n1 2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\nw o rks\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\nexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabook\naboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deep\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "diﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeld\nhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchers\nanddiﬀerentperspectives.\nAcomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\nHowever,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\nlearning known as c y b e r net i c sin the 1940s–1960s, deep learning knownas",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "c o nnec t i o n i s minthe1980s–1990s,andthecurrentresurgenceunderthename\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedinﬁgure.1.7\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\nhappensorcouldhappeninthebrain. Asaresult,oneofthenamesthatdeep\nlearninghasgonebyis ar t i ﬁc i al neur al net w o r k s(ANNs).Thecorresponding\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "bythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\nWhilethekindsofneuralnetworksusedformachinelearninghavesometimes\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "computational principlesbehindthebrainandduplicateitsfunctionality.Another\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\nlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolve\nengineeringapplications.\nThemodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspective\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "principleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\nlearningframeworksthatarenotnecessarilyneurallyinspired.\n1 3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1940 1950 1960 1970 1980 1990 2000\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\nc y b e r n e t i c s\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\nFigure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnets\nresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or\n“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "ﬁrstwavestartedwithcyberneticsinthe1940s–1960s, withthedevelopmentoftheories\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\ntheﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\nthecorrespondingscientiﬁcactivityoccurred.\n1 4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\nmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedto\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y.\nThesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\nf ( x w, ) =x 1w 1 + · · · +x nw n.Thisﬁrstwaveofneuralnetworksresearchwas\nknownascybernetics,asillustratedinﬁgure.1.7\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "ofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesof\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\ntocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobe\nsetcorrectly.Theseweightscouldbesetbythehumanoperator. Inthe1950s,\ntheperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearn\ntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory.\nThe adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "time,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\nandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata.\nThesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\nmodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominant\ntrainingalgorithmsfordeeplearningmodelstoday.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "trainingalgorithmsfordeeplearningmodelstoday.\nModelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\nlearningmodels,thoughinmanycasestheyare t r a i ne dindiﬀerentwaysthanthe\noriginalmodelsweretrained.\nLinearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "andf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseﬂawsinlinearmodelscaused\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\n1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.\nToday,neuroscienceisregardedasanimportantsourceofinspirationfordeep\nlearningresearchers,butitisnolongerthepredominant guidefortheﬁeld.\nThemainreasonforthediminishedrole ofneuroscienceindeeplearning\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "touseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\n1 5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nwell-studiedpartsofthebrain( ,). OlshausenandField2005\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\ncansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto\n“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\ndiﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "researchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudying\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.\nWeareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\nthe r e c t i ﬁed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\nfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrom",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "manyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\nneuroscienceasaninﬂuence,and ()citingmoreengineering- Jarrett e t a l .2009\norientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,it\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\ndiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealism\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "neurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\ndonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuch\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.\nMediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\nWhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\ninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernel",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "machinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\n1 6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nneuroscienceatall.\nItis worth notingthat theeﬀorttounderstandhowthe brainworkson\nan algorithmic lev el is alive andwell.This endeavor is primarily knownas\n“computational neuroscience”andisaseparateﬁeldofstudyfromdeeplearning.\nItiscommonforresearcherstomovebackandforthbetweenbothﬁelds.The\nﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\nthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldof",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "computational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\nmodelsofhowthebrainactuallyworks.\nInthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\ni ng( ,; ,). Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\ntounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.During",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "theearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\nDespitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsof\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\nThecentralideainconnectionism isthatalargenumberofsimplecomputational",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "unitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\ncomputational models.\nSeveralkeyconceptsaroseduringtheconnectionism movementofthe1980s\nthatremaincentraltotoday’sdeeplearning.\nOneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "possibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\nbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuron\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "describingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\n1 7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\nofonespeciﬁccategoryofobjects. Theconceptofdistributedrepresentationis\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.\nDuringthe1990s,researchersmadeimportantadvancesinmodelingsequences\nwithneuralnetworks.()and ()identiﬁedsomeof Hochreiter1991Bengio e t a l .1994\nthefundamentalmathematical diﬃcultiesinmodelinglongsequences,describedin\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\nmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTM",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "iswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\nprocessingtasksatGoogle.\nThesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁll\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\notherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l .",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "1992CortesandVapnik1995Schölkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\nDuringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "ThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHinton\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\nandcomputervision.\nAtthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃcult\ntotrain. Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "thesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\nwiththehardwareavailableatthetime.\nThethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\n1 8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbelief\nnetworkcouldbeeﬃcientlytrainedusingastrategycalledgreedylayer-wisepre-\ntraining( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1\nTheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategy\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "testexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\nterm“deeplearning”toemphasizethatresearcherswerenowabletotraindeeper\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "ofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\nmodelstoleveragelargelabeleddatasets.\n1 . 2 . 2 In creasin g D a t a s et S i zes\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "crucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswere\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\nFortunately,theamountofskillrequiredreducesastheamountoftrainingdata\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "todayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\n1 9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nintoadatasetappropriateformachinelearningapplications.Theageof“Big\nData”hasmademachinelearningmucheasierbecausethekeyburdenofstatistical\nestimation—generalizingwelltonewdataafterobservingonlyasmallamount\nofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\nlearning.\n1 . 2 . 3 In creasin g Mo d el S i zes\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\nAnindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\nBiologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.\nIntermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\nsmalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden 1.11",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "units,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\nallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberof\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\nrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiological",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksmaybeevenlargerthanthisplotportrays.\nInretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\nneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-\nlems.Eventoday’snetworks,whichweconsiderquitelargefromacomputational\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\nprimitivevertebrateanimalslikefrogs.\nTheincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\n2 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1900 1950 198520002015\nYear100101102103104105106107108109Datasetsize(numberexamples)\nIrisMNISTPublicSVHN\nImageNet\nCIFAR-10ImageNet10k\nILSVRC  2014Sports-1M\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\nWMT\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "ofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\ndemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(Widrow\nandHoﬀ1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\nofthousandsofexamplessuchastheMNISTdataset(showninﬁgure)ofscans 1.9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "ofhandwrittennumbers( ,).Intheﬁrstdecadeofthe2000s,more LeCun e t a l .1998b\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\ntheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousands\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.\nThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l .",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructed\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.\n2 1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nFigure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNational\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\nThe“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewith\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\nandassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimple\nclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "research.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\nGeoﬀreyHintonhasdescribeditas“the d r o s o p h i l aofmachinelearning,”meaningthat\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\nconditions,muchasbiologistsoftenstudyfruitﬂies.\n2 2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\nexpectedtocontinuewellintothefuture.\n1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "accuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\nappliedwithsuccesstobroaderandbroadersetsofapplications.\nTheearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerent\ncategoriesofobjects. ThelargestcontestinobjectrecognitionistheImageNet\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\nwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "state-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\nforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthis\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\nasshowninﬁgure.1.12",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "asshowninﬁgure.1.12\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\ne t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\nthishistoryinmoredetailinsection.12.3\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "imagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\n2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan\n2 3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\n12\n34\n567\n89\n1 0\nF r ui t ﬂyMo useC a tH um a n\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneural\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\nneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyas\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "tohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\nnetworksizesfrom (). Wikipedia2015\n1.Adaptivelinearelement( ,) WidrowandHoﬀ1960\n2.Neocognitron(Fukushima1980,)\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "7.Distributedautoencoder(,) Le e t al.2012\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n10.GoogLeNet( ,) Szegedy e t al.2014a\n2 4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\ne t a l .,).2012\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\nsohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\nelementsofthesequence( ,).Recurrentneuralnetworks, GülçehreandBengio2013",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "suchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustﬁxedinputs.\nThissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\n2015).\nThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "toreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\nfuturecouldinprinciplebeappliedtonearlyanytask.\nAnothercrowningachievementofdeeplearningisitsextensiontothedomainof\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "agentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\nalsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics\n(,). Finn e t a l .2015\nManyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "isnowused bymanytoptechnologycompanies includi ngGoogle, Microsoft,\nFacebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC.\nAdvancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\nDistBelief(,),Caﬀe(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "TensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\ncommercialproducts.\nDeeplearninghasalsomadecontributionsbacktoothersciences.Modern\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\n2 5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\nscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "Barley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁc\nﬁeldsinthefuture.\nInsummary,deeplearningisanapproachtomachinelearningthathasdrawn\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "bringittonewfrontiers.\n2 6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1950 198520002015 2056\nYear10− 210− 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\n123\n456\n78\n91011\n121314\n151617\n181920\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\nFigure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubled\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015\n1.Perceptron(,,) Rosenblatt19581962\n2.Adaptivelinearelement( ,) WidrowandHoﬀ1960\n3.Neocognitron(Fukushima1980,)",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "3.Neocognitron(Fukushima1980,)\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\n7.Meanﬁeldsigmoidbeliefnetwork(,) Saul e t al.1996\n8.LeNet-5( ,) LeCun e t al.1998b\n9.Echostatenetwork( ,) JaegerandHaas2004\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n16.OMP-1network( ,) CoatesandNg2011\n17.Distributedautoencoder(,) Le e t al.2012\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n20.GoogLeNet( ,) Szegedy e t al.2014a\n2 7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n2010 2011 2012 2013 2014 2015\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC  classiﬁcationerrorrate\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\neveryyear,andyieldedlowerandlowererrorrateseachtime. DatafromRussakovsky\ne t a l . e t a l . ()and2014b He().2015\n2 8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 7\nRegularization f or D e e p L e ar n i n g\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\nasregularization. As wewillseethereareagreatmanyformsofregularization\navailabletothedeeplearningpractitioner. Infact, developingmoreeﬀective",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "regularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld.\nChapterintroducedthebasicconceptsofgeneralization, underﬁtting,overﬁt- 5\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone.\nInthischapter,wedescriberegularizationinmoredetail,focusingonregular-\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\ntoformdeepmodels.\nSomesectionsofthischapterdealwithstandardconceptsinmachinelearning.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevant\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\nbasicconceptstotheparticularcaseofneuralnetworks.\nInsection,wedeﬁnedregularizationas“anymodiﬁcationwemaketo 5.2.2\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\nitstrainingerror.”Therearemanyregularizationstrategies.Someputextra\nconstraints ona machine learning model, such asadding restrictionson the",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "parametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\n228",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\nspeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\npromotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\nbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtable\ntrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwe\ndiscussedgeneralization andoverﬁttinginchapter,wefocusedonthreesituations, 5\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\nprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetrue",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "datageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\notherpossiblegeneratingprocesses—theoverﬁttingregimewherevariancerather\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\nmodelfromthethirdregimeintothesecondregime.\nInpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "wecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\nextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)into",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "aroundhole(ourmodelfamily).\nWhatthismeansisthatcontrollingthecomplexityofthemodelisnota\nsimplematterofﬁndingthemodeloftherightsize,withtherightnumberof\nparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,\nwealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizing\ngeneralization error)isalargemodelthathasbeenregularizedappropriately .\nWenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\nmodel.\n2 2 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.1ParameterNormPenalties\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\nandeﬀectiveregularizationstrategies.\nManyregularizationapproachesarebasedonlimitingthecapacityofmodels,\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\nrameternormpenalty Ω(θ)totheobjectivefunction J.Wedenotetheregularized\nobjectivefunctionby˜ J:",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "objectivefunctionby˜ J:\n˜ J , J , α (;θXy) = (;θXy)+Ω()θ (7.1)\nwhere α∈[0 ,∞)isahyperparameter thatweightstherelativecontributionofthe\nnormpenaltyterm,,relativetothestandardobjectivefunction Ω J.Setting αto0\nresultsinnoregularization. Largervaluesof αcorrespondtomoreregularization.\nWhenourtrainingalgorithmminimizestheregularizedobjectivefunction ˜ Jit\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\nofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "choicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred. Ω\nInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties\nonthemodelparameters.\nBeforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethat\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩ\npenalizes oftheaﬃnetransformationateachlayerandleaves onlytheweights\nthebiasesunregularized. Thebiasestypicallyrequirelessdatatoﬁtaccurately",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "thantheweights. Eachweightspeciﬁeshowtwovariablesinteract. Fittingthe\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\ncanintroduceasigniﬁcantamountofunderﬁtting. Wethereforeusethevectorw\ntoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethe\nvectorθdenotesalloftheparameters,includingbothwandtheunregularized\nparameters.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "parameters.\nInthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\npenaltywithadiﬀerent αcoeﬃcientforeachlayerofthenetwork.Becauseitcan\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\nsearchspace.\n2 3 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\nbyaddingaregularizationtermΩ(θ) =1\n2w2\n2totheobjectivefunction.Inother\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\nTikhonovregularization.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "Tikhonovregularization.\nWecangainsomeinsightintothebehaviorofweightdecayregularization\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\npresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthe\nfollowingtotalobjectivefunction:\n˜ J , (;wXy) =α\n2wwwXy +( J; ,) , (7.2)\nwiththecorrespondingparametergradient\n∇ w˜ J , α (;wXy) = w+∇ w J , . (;wXy) (7.3)\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\nwww ← −  α( +∇ w J , . (;wXy)) (7.4)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "www ← −  α( +∇ w J , . (;wXy)) (7.4)\nWrittenanotherway,theupdateis:\nww ← −(1  α)−∇  w J , . (;wXy) (7.5)\nWecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearning\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\nasinglestep.Butwhathappensovertheentirecourseoftraining?\nWewillfurthersimplifytheanalysisbymakingaquadraticapproximation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "totheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\nobtainsminimalunregularized trainingcost,w∗=argminw J(w).Iftheobjective\nfunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i ﬁ c p o i n t i n s p a c e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "a n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e ﬀ e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "m o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n .\n2 3 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nmeansquarederror,thentheapproximationisperfect.Theapproximation ˆ Jis\ngivenby\nˆ J J () = θ (w∗)+1\n2(ww−∗)Hww (−∗) , (7.6)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Thereis\nnoﬁrst-orderterminthisquadraticapproximation, becausew∗isdeﬁnedtobea\nminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofa\nminimumof,wecanconcludethatispositivesemideﬁnite. J H\nTheminimumofˆ Joccurswhereitsgradient\n∇ wˆ J() = (wHww−∗) (7.7)\nisequalto. 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "∇ wˆ J() = (wHww−∗) (7.7)\nisequalto. 0\nTostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe 7.7\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\nversionofˆ J.Weusethevariable ˜wtorepresentthelocationoftheminimum.\nα˜wH+ (˜ww−∗) = 0 (7.8)\n(+ )H αI˜wHw = ∗(7.9)\n˜wHI = (+ α)− 1Hw∗. (7.10)\nAs αapproaches0,theregularizedsolution ˜wapproachesw∗.Butwhat\nhappensas αgrows?BecauseHisrealandsymmetric,wecandecomposeit\nintoadiagonalmatrix Λandanorthonormal basisofeigenvectors,Q,suchthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "HQQ = Λ.Applyingthedecompositiontoequation,weobtain:7.10\n˜wQQ = ( Λ+ ) αI− 1QQ Λw∗(7.11)\n=\nQIQ (+ Λ α)− 1\nQQ Λw∗(7.12)\n= (+ )Q Λ αI− 1ΛQw∗. (7.13)\nWeseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedby\ntheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththe\ni-theigenvectorofHisrescaledbyafactorofλ i\nλ i + α.(Youmaywishtoreview\nhowthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "where λ i α,theeﬀectofregularizationisrelativelysmall.However,components\nwith λ i αwillbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustrated\ninﬁgure.7.1\n2 3 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w∗\n˜ w\nFigure7.1:Anillustrationoftheeﬀectof L2(orweightdecay)regularizationonthevalue\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\nthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,the\neigenvalueoftheHessianof Jissmall. Theobjectivefunctiondoesnotincreasemuch",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "whenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpress\nastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.\nTheregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\nisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,\nindicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionof w2relatively\nlittle.\nOnlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducing",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "theobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\ntellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.\nComponentsoftheweightvectorcorrespondingtosuchunimportant directions\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.\nSofarwehavediscussedweightdecayintermsofitseﬀectontheoptimization\nofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelateto",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "machinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,a\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\n2 3 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthesumofsquarederrors:\n( )Xwy−( )Xwy− . (7.14)\nWhenweadd L2regularization, theobjectivefunctionchangesto\n( )Xwy−( )+Xwy−1\n2αww . (7.15)\nThischangesthenormalequationsforthesolutionfrom\nwX= (X)− 1Xy (7.16)\nto\nwX= (XI+ α)− 1Xy . (7.17)\nThematrixXXinequationisproportionaltothecovariancematrix 7.161\nmXX.\nUsing L2regularizationreplacesthismatrixwith\nXXI+ α− 1inequation.7.17\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof αtothe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "diagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\nto“perceive”theinputXashavinghighervariance,whichmakesitshrinkthe\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\nthisaddedvariance.\n7 . 1 . 2 L1Regu l a ri z a t i o n\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\nwaystopenalizethesizeofthemodelparameters. Anotheroptionistouse L1\nregularization.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "regularization.\nFormally, L1regularizationonthemodelparameter isdeﬁnedas:w\nΩ() = θ ||||w 1=\ni| w i| , (7.18)\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\nnowdiscusstheeﬀectof L1regularizationonthesimplelinearregressionmodel,\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\nparticular,weareinterestedindelineatingthediﬀerencesbetween L1and L2forms",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\ni n t ro d u c e t h e t e rmΩ() = θ ||− w w( ) o|| 1=\ni| w i− w( ) o\ni| .\n2 3 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter Ω α.\nThus,theregularizedobjectivefunction ˜ J , (;wXy)isgivenby\n˜ J , α (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\nwiththecorrespondinggradient(actually,sub-gradient):\n∇ w˜ J , α (;wXy) = sign( )+w ∇ w J ,(Xyw;) (7.20)\nwhere issimplythesignofappliedelement-wise. sign( )w w",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "Byinspectingequation,wecanseeimmediately thattheeﬀectof 7.20 L1\nregularizationisquitediﬀerentfromthatof L2regularization. Speciﬁcally,wecan\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\nregularization.\nOursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "viaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\ninthissettingisgivenby\n∇ wˆ J() = (wHww−∗) , (7.21)\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww∗.\nBecausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\naccomplishedusingPCA.\nOurquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\nposesintoasumovertheparameters:\nˆ J , J (;wXy) = (w∗; )+Xy ,\ni1\n2H i , i(w i−w∗\ni)2+ α w| i|\n.(7.22)\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\n(foreachdimension),withthefollowingform: i\nw i= sign( w∗\ni)max\n| w∗\ni|−α\nH i , i,0\n. (7.23)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "i)max\n| w∗\ni|−α\nH i , i,0\n. (7.23)\n2 3 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConsiderthesituationwhere w∗\ni > i 0forall.Therearetwopossibleoutcomes:\n1.Thecasewhere w∗\ni≤α\nH i , i.Heretheoptimalvalueof w iundertheregularized\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\ntotheregularizedobjective˜ J(w;Xy ,)isoverwhelmed—indirection i—by\nthe L1regularizationwhichpushesthevalueof w itozero.\n2.Thecasewhere w∗\ni >α\nH i , i.Inthiscase,theregularizationdoesnotmovethe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "optimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\ndistanceequaltoα\nH i , i.\nAsimilarprocesshappenswhen w∗\ni <0,butwiththe L1penaltymaking w iless\nnegativebyα\nH i , i,or0.\nIncomparisonto L2regularization, L1regularizationresultsinasolutionthat\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\ndiﬀerentbehaviorthanariseswith L2regularization. Equationgavethe7.13",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "solution ˜ wfor L2regularization. Ifwerevisitthatequationusingtheassumption\nofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisof\nL1regularization,weﬁndthat˜ w i=H i , i\nH i , i + αw∗\ni.If w∗\niwasnonzero,then ˜ w iremains\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\ntobecomesparse,while L1regularizationmaydosoforlargeenough. α\nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "asafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearning\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\nInsection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "asMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\ntoMAPBayesianinferencewithaGaussianpriorontheweights. For L1regu-\nlarization,thepenalty αΩ(w)= α\ni| w i|usedtoregularizeacostfunctionis\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26w∈ Rn:\nlog() = pw\nilogLaplace( w i;0 ,1\nα) = −|||| αw 1+log log2 n α n− .(7.24)\n2 3 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\nignorethe termsbecausetheydonotdependon. log log2 α− w\n7.2NormPenaltiesasConstrainedOptimization\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\n˜ J , J , α . (;θXy) = (;θXy)+Ω()θ (7.25)\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "functionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,\ncalledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresenting\nwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthan\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\nL − (; ) = (; )+(Ω() θ , αXy , JθXy , αθ k .) (7.26)\nThesolutiontotheconstrainedproblemisgivenby\nθ∗= argmin\nθmax\nα , α≥ 0L()θ , α . (7.27)\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 θ",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "and α.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\nconstraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinall\nprocedures αmustincreasewheneverΩ(θ) > kanddecreasewheneverΩ(θ) < k.\nAllpositive αencourage Ω(θ)toshrink.Theoptimalvalue α∗willencourage Ω(θ)\ntoshrink,butnotsostronglytomakebecomelessthan. Ω()θ k\nTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁx α∗andview\ntheproblemasjustafunctionof:θ",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "theproblemasjustafunctionof:θ\nθ∗= argmin\nθL(θ , α∗) = argmin\nθJ , α (;θXy)+∗Ω()θ .(7.28)\nThisisexactlythesameastheregularizedtrainingproblemofminimizing ˜ J.\nWecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\nweights.Ifisthe Ω L2norm,thentheweightsareconstrainedtolieinan L2\nball. Ifisthe Ω L1norm,thentheweightsareconstrainedtolieinaregionof\n2 3 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\nimposebyusingweightdecaywithcoeﬃcient α∗becausethevalueof α∗doesnot\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\nbetween kand α∗dependsontheformof J.Whilewedonotknowtheexactsize\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing α\ninordertogroworshrinktheconstraintregion.Larger αwillresultinasmaller",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "constraintregion.Smallerwillresultinalargerconstraintregion. α\nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\ndescenttotakeastepdownhillon J(θ)andthenprojectθbacktothenearest\npointthatsatisﬁesΩ(θ) < k.Thiscanbeusefulifwehaveanideaofwhatvalue\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof αthat\ncorrespondstothis. k\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "constraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\nprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentraining\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\n“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthe\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\nallverysmall. Whentrainingwithapenaltyonthenormoftheweights,these\nconﬁgurations canbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduce",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Jbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\nhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraint\nregion.\nFinally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "largegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\nconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfrom\ntheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojection\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\nwithoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\nsomestability.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "somestability.\nInparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\n2 3 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "obeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\nanexplicitconstraintwithreprojection.\n7.3RegularizationandUnder-ConstrainedProblems\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\nerlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregression\nandPCA,dependoninvertingthematrixXX.Thisisnotpossiblewhenever\nXXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "butiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\nXXI+ αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.\nTheselinearproblemshaveclosedformsolutionswhentherelevantmatrix\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "theclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\nclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.\nAniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\nimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweights\ntocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowthe\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "Mostformsofregularizationareabletoguaranteetheconvergenceofiterative\nmethodsappliedtounderdetermined problems. Forexample,weightdecaywill\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\nslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.\nTheideaofusingregularizationtosolveunderdetermined problemsextends\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\nproblems.\nAswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\n2 3 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMoore-Penrosepseudoinverse.Recallthatonedeﬁnitionofthepseudoinverse\nX+ofamatrixisX\nX+=lim\nα 0(XXI+ α)− 1X. (7.29)\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\ndecay.Speciﬁcally,equationisthelimitofequationastheregularization 7.29 7.17\ncoeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\nunderdetermined problemsusingregularization.\n7.4DatasetAugmentation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "7.4DatasetAugmentation\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset.\nForsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\nfakedata.\nThisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Thismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevariety\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\ntheinputsinourtrainingset. x\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\nisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehave\nalreadysolvedthedensityestimationproblem.\nDatasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁc\nclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandinclude",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "anenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.\nOperationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\ntheimagehavealsoprovenquiteeﬀective.\nOnemustbecarefulnottoapplytransformationsthatwouldchangethecorrect",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "class.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\ndiﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontal\nﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthese\ntasks.\n2 4 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariant\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\nbeimplementedasasimplegeometricoperationontheinputpixels.\nDatasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly\nandHinton2013,).\nInjectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\ncanalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationand",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "evensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "augmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\nthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthe\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\nmultiplyingbynoise.\nWhencomparingmachinelearningbenchmarkresults,itisimportanttotake\ntheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddataset",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "augmentationschemescandramaticallyreducethegeneralization errorofamachine\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "numeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "whileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomly\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.\n2 4 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.5NoiseRobustness\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\naugmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimal\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "appliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\nofthatapproach.\nAnotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,). Thiscan\nbeinterpretedasa stochasticimplementation of Bayesianinference overthe\nweights. TheBayesiantreatmentoflearningwouldconsiderthemodelweights",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "tobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthis\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂect\nthisuncertainty.\nNoiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\nafunction ˆ y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "costfunctionbetweenthemodelpredictions ˆ y()xandthetruevalues: y\nJ= E p x , y ( )(ˆ y y ()x−)2\n. (7.30)\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.\nWenowassumethatwitheachinputpresentationwealsoincludearandom\nperturbation  W∼N(; 0 , ηI)ofthenetworkweights.Letusimaginethatwe\nhaveastandard l-layerMLP.Wedenotetheperturbedmodelasˆ y  W(x).Despite\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "outputofthenetwork.Theobjectivefunctionthusbecomes:\n˜ J W= E p , y , ( x  W )\n(ˆ y  W() )x− y2\n(7.31)\n= E p , y , ( x  W )\nˆ y2\n W()2ˆx− y y  W()+x y2\n.(7.32)\nForsmall η,theminimization of Jwithaddedweightnoise(withcovariance\nηI)isequivalenttominimization of Jwithanadditionalregularizationterm:\n2 4 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nη E p , y ( x )∇ Wˆ y()x2\n.Thisformofregularizationencouragestheparametersto\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\narelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodel\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\nweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedby\nﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinear",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "regression(where,forinstance, ˆ y(x) =wx+ b),thisregularizationtermcollapses\ninto η E p ( ) x\nx2\n,whichisnotafunctionofparametersandthereforedoesnot\ncontributetothegradientof˜ J Wwithrespecttothemodelparameters.\n7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "constant ,thetrainingsetlabel yiscorrectwithprobability 1− ,andotherwise\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\nsoftmaxwith koutputvaluesbyreplacingthehardandclassiﬁcationtargets 0 1\nwithtargetsof\nk− 1and1− ,respectively.Thestandardcross-entropylossmay\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "classiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcannever\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\ndiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980s\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "etal.,).2015\n7.6Semi-SupervisedLearning\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\nx.\nInthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\n2 4 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\nsimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebetter\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "analysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojected\ndata).\nInsteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\nthentrade-oﬀthesupervisedcriterion −log P( y x|)withtheunsupervisedor\ngenerativeone(suchas−log P( x)or−log P( x y ,)).Thegenerativecriterionthen\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "learningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\ninthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerative\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\nBengio2008,).\nSalakhutdinovandHinton2008()describeamethodforlearningthekernel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "functionofakernelmachineusedforregression,inwhichtheusageofunlabeled\nexamplesformodeling improvesquitesigniﬁcantly. P() x P( ) y x|\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\n7.7Multi-TaskLearning\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\narisingoutofseveraltasks. Inthesamewaythatadditionaltrainingexamples",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "putmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\nconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyielding\nbettergeneralization.\nFigureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\ndiﬀerentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\n2 4 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\nparameters:\n1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtask\ntoachievegoodgeneralization). Thesearetheupperlayersoftheneural\nnetworkinﬁgure.7.2\n2.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthe\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\ninﬁgure.7.2\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "h( s h a r e d )h( s h a r e d )\nxx\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\nandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbut\ninvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\ncanbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectively\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "sharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "associatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).\nImprovedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\n2 4 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n0 50 100 150 200 250\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\nV a l i d a t i o n s e t l o s s\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "increaseagain,forminganasymmetricU-shapedcurve.\ngreatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\nthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssome\nofthetasks.\nFromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\nfollowing:amongthefactorsthat explainthevariations observed inthedata",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "associatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.\n7.8EarlyStopping\nWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁt\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\nvalidationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis 7.3\nbehavior.Thisbehavioroccursveryreliably.\nThismeanswecanobtainamodelwithbettervalidationseterror(andthus,\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "timewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\n2 4 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\nvalidationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureis\nspeciﬁedmoreformallyinalgorithm .7.1\nAlgorithm 7.1Theearlystopping meta-algorithmfor determiningthe best\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\nvalidationset.\nLetbethenumberofstepsbetweenevaluations. n",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "Letbethenumberofstepsbetweenevaluations. n\nLet pbethe“patience,”thenumberoftimestoobserveworseningvalidationset\nerrorbeforegivingup.\nLetθ obetheinitialparameters.\nθθ← o\ni←0\nj←0\nv←∞\nθ∗←θ\ni∗← i\nwhiledo j < p\nUpdatebyrunningthetrainingalgorithmforsteps. θ n\ni i n ←+\nv←ValidationSetError ()θ\nif v< vthen\nj←0\nθ∗←θ\ni∗← i\nv v←\nelse\nj j←+1\nendif\nendwhile\nBestparametersareθ∗,bestnumberoftrainingstepsis i∗\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "usedformofregularizationindeeplearning.Itspopularityisduebothtoits\neﬀectivenessanditssimplicity.\nOnewaytothinkofearlystoppingisasaveryeﬃcienthyperparameter selection\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\nWecanseeinﬁgurethatthishyperparameter hasaU-shapedvalidationset 7.3\n2 4 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\nU-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof 5.3\nearlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermining\nhowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbe\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\natthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "“trainingtime” hyperparam eterisuniqueinthatbydeﬁnitionasinglerunof\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcost\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "smallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.\nAnadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "training,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.\nEarlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\nsmallweights.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "smallweights.\nEarlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\noccuratalocalminimumofthetrainingobjective.\nEarlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "trainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\nonecanuseforthissecondtrainingprocedure.\nOnestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\n2 4 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\ntheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Thereare\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\ntrainingsetisbigger.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "trainingsetisbigger.\nAlgorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\ntotrain,thenretrainingonallthedata.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "returns i∗,theoptimalnumberofsteps.\nSettorandomvaluesagain. θ\nTrainonX( ) t r a i nandy( ) t r a i nfor i∗steps.\nAnotherstrategyforusingallofthedataistokeeptheparametersobtained\nfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallof\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\nofanumberofsteps. Instead,wecanmonitortheaveragelossfunctiononthe\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "setobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.\nThisprocedureispresentedmoreformallyinalgorithm .7.3\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "oftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithout\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\nthegradientsofsuchadditionalterms.\nHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\n2 4 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\ntivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nupdates.θ\n J , ←(θX( ) s ubtr a i n,y( ) s ubtr a i n)\nwhile J ,(θX( v a l i d ),y( v a l i d )) > do\nTrainonX( ) t r a i nandy( ) t r a i nforsteps. n\nendwhile\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\n()and ()arguedthatearlystoppinghastheeﬀectof 1995aSjöbergandLjung1995\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "spaceintheneighborhoodoftheinitialparametervalueθ o,asillustratedin\nﬁgure.Morespeciﬁcally,imaginetaking 7.4 τoptimization steps(corresponding\nto τtrainingiterations)andwithlearningrate .Wecanviewtheproduct  τ\nasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restricting\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\nspacereachablefromθ o.Inthissense,  τbehavesasifitwerethereciprocalof\nthecoeﬃcientusedforweightdecay.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "thecoeﬃcientusedforweightdecay.\nIndeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic\nerrorfunctionandsimplegradientdescent—earlystoppingisequivalentto L2\nregularization.\nInordertocomparewithclassical L2regularization, weexamineasimple\nsettingwheretheonlyparametersarelinearweights(θ=w).Wecanmodel\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\nempiricallyoptimalvalueoftheweightsw∗:\nˆ J J () = θ (w∗)+1\n2(ww−∗)Hww (−∗) , (7.33)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "ˆ J J () = θ (w∗)+1\n2(ww−∗)Hww (−∗) , (7.33)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Giventhe\nassumptionthatw∗isaminimumof J(w),weknowthatHispositivesemideﬁnite.\nUnderalocalTaylorseriesapproximation,thegradientisgivenby:\n∇ wˆ J() = (wHww−∗) . (7.34)\n2 5 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w∗\n˜ w\nw 1w 2w∗\n˜ w\nFigure7.4:Anillustrationoftheeﬀectofearlystopping. ( L e f t )Thesolidcontourlines\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗that\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.\n( R i g h t )Anillustrationoftheeﬀectof L2regularizationforcomparison.Thedashedcircles",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "indicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\nnearertheoriginthantheminimumoftheunregularizedcost.\nWearegoingtostudythetrajectoryfollowedbytheparametervectorduring\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\nanalyzinggradientdescentonˆ J:\nw( ) τ= w( 1 ) τ−−∇  wˆ J(w( 1 ) τ−) (7.35)\n= w( 1 ) τ−− Hw(( 1 ) τ−−w∗) (7.36)\nw( ) τ−w∗= ( )(IH− w( 1 ) τ−−w∗) . (7.37)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "w( ) τ−w∗= ( )(IH− w( 1 ) τ−−w∗) . (7.37)\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\ntheeigendecompositionofH:H=QQ Λ,where ΛisadiagonalmatrixandQ\nisanorthonormalbasisofeigenvectors.\nw( ) τ−w∗= (IQQ −  Λ)(w( 1 ) τ−−w∗)(7.38)\nQ(w( ) τ−w∗) = ( )I−  ΛQ(w( 1 ) τ−−w∗) (7.39)\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "a l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\ni n i t i a l v a l u e w( 0 ).\n2 5 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAssumingthatw( 0 )=0andthat ischosentobesmallenoughtoguarantee\n|1−  λ i| <1,theparametertrajectoryduringtrainingafter τparameterupdates\nisasfollows:\nQw( ) τ= [ ( )I−I−  Λτ]Qw∗. (7.40)\nNow,theexpressionforQ˜winequationfor7.13 L2regularizationcanberear-\nrangedas:\nQ˜wI = (+ Λ α)− 1ΛQw∗(7.41)\nQ˜wII = [−(+ Λ α)− 1α]Qw∗(7.42)\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 ,\nα τ,andarechosensuchthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "α τ,andarechosensuchthat\n( )I−  Λτ= (+ ) Λ αI− 1α , (7.43)\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\nthatifall λ iaresmall(thatis,  λ i1and λ i /α1)then\nτ≈1\n α, (7.44)\nα≈1\nτ . (7.45)\nThatis,undertheseassumptions,thenumberoftrainingiterations τplaysarole",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "inverselyproportionaltothe L2regularizationparameter,andtheinverseof τ \nplaystheroleoftheweightdecaycoeﬃcient.\nParametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(ofthe\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\ntodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameters\ncorrespondingtodirectionsoflesscurvature.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "correspondingtodirectionsoflesscurvature.\nThederivationsinthissectionhaveshownthatatrajectoryoflength τends\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "determinesthecorrectamountofregularizationwhileweightdecayrequiresmany\ntrainingexperimentswithdiﬀerentvaluesofitshyperparameter.\n2 5 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.9ParameterTyingandParameterSharing\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\ntotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.\nForexample, L2regularization(orweightdecay)penalizesmodelparametersfor\ndeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedother\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\nSometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "butweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\nshouldbesomedependencies betweenthemodelparameters.\nAcommontypeofdependencythatweoftenwanttoexpressisthatcertain\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\nhavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetof\nclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodel\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "maptheinput totwo diﬀerent, but related outputs:ˆ y( ) A= f(w( ) A,x)and\nˆ y( ) B= ( gw( ) B,x).\nLetusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\ntoeachother: ∀ i, w( ) A\nishouldbecloseto w( ) B\ni.Wecanleveragethisinformation\nthroughregularization. Speciﬁcally,wecanuseaparameternormpenaltyofthe\nform: Ω(w( ) A,w( ) B)=w( ) A−w( ) B2\n2. Hereweusedan L2penalty,butother\nchoicesarealsopossible.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "choicesarealsopossible.\nThiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\ntheparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,to\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\nconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbe\npairedtocorrespondingparametersintheunsupervisedmodel.\nWhileaparameternormpenaltyisonewaytoregularizeparameterstobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "closetooneanother,themorepopularwayistouseconstraints:toforcesets\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\nassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharing\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\nmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcant",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "reductioninthememoryfootprintofthemodel.\n2 5 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\ntocomputervision.\nNaturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\nForexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "iscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁnda\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\ni+1intheimage.\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\nmodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringa\ncorrespondingincreaseintrainingdata. Itremainsoneofthebestexamplesof\nhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture.\nCNNswillbediscussedinmoredetailinchapter.9\n7.10SparseRepresentations",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "7.10SparseRepresentations\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\npenaltyonthemodelparameters.\nWehave alreadydiscussed (insection)how7.1.2 L1penalizationinduces\nasparseparametrization—meaning thatmanyoftheparametersbecomezero\n(orcloseto zero).Representationalsparsity, on theother hand, des cribesa",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "representationwheremanyoftheelementsoftherepresentationarezero(orclose\ntozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextof\nlinearregression:\n\n18\n5\n15\n−9\n−3\n=\n400 20 0 −\n00 10 3 0 −\n050 0 0 0\n100 10 4 − −\n100 0 50 −\n\n2\n3\n−2\n−5\n1\n4\n\ny∈ RmA∈ Rm n×x∈ Rn(7.46)\n2 5 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n\n−14\n1\n19\n2\n23\n=\n3 12 54 1 − −\n4 2 3 11 3 − −\n− − − 15 4 2 3 2\n3 1 2 30 3 − −\n− − − − 54 22 5 1\n\n0\n2\n0\n0\n−3\n0\n\ny∈ RmB∈ Rm n×h∈ Rn(7.47)\nIntheﬁrstexpression,wehaveanexampleofasparselyparametrized linear\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\ntheinformationpresentin,butdoessowithasparsevector. x",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "Representationalregularizationisaccomplishedbythesamesortsofmechanisms\nthatwehaveusedinparameterregularization.\nNormpenaltyregularizationofrepresentationsisperformedbyaddingtothe\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\nΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜ J:\n˜ J , J , α (;θXy) = (;θXy)+Ω()h (7.48)\nwhere α∈[0 ,∞)weightstherelativecontributionofthenormpenaltyterm,with\nlargervaluesofcorrespondingtomoreregularization. α",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "Justasan L1penaltyontheparametersinducesparametersparsity,an L1\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\nΩ(h) =||||h 1=\ni| h i|. Ofcourse,the L1penaltyisonlyonechoiceofpenalty\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "Lee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\nm\nih( ) i,to\nbenearsometargetvalue,suchasavectorwith.01foreachentry.\nOtherapproachesobtainrepresentationalsparsitywithahardconstrainton\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\noptimization problem\nargmin\nh h , 0 < k− xWh2, (7.49)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "argmin\nh h , 0 < k− xWh2, (7.49)\nwhere h 0isthenumberofnon-zeroentriesofh. Thisproblemcanbesolved\neﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\n2 5 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nOMP- kwiththevalueof kspeciﬁedtoindicatethenumberofnon-zerofeatures\nallowed. ()demonstratedthatOMP-canbeaveryeﬀective CoatesandNg2011 1\nfeatureextractorfordeeparchitectures.\nEssentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\ncontexts.\n7.11BaggingandOtherEnsembleMethods\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "eralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\ntrainseveraldiﬀerentmodelsseparately,thenhaveallofthemodelsvoteonthe\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\nasensemblemethods.\nThereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusually\nnotmakeallthesameerrorsonthetestset.\nConsiderforexampleasetof kregressionmodels.Supposethateachmodel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "makesanerror  ioneachexample, withtheerrorsdrawnfromazero-mean\nmultivariatenormaldistributionwithvariances E[ 2\ni] = vandcovariances E[  i  j] =\nc. Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\n1\nk\ni  i.Theexpectedsquarederroroftheensemblepredictoris\nE\n\n1\nk\ni i2\n=1\nk2E\n\ni\n 2\ni+\nj i= i  j\n\n(7.50)\n=1\nkv+k−1\nkc . (7.51)\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "errorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\nensembleisonly1\nkv.Thismeansthattheexpectedsquarederroroftheensemble\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\nindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers.\nDiﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\n2 5 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n8\n8F i r s t   e nse m b l e   m e m b e r\nSe c ond e nse m b l e   m e m b e rO r i gi nal   data s e t\nF i r s t   r e s am pl e d   d a t a s e t\nSe c ond re s am p l e d   d a t a s e t\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerent\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "bysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthis\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\nclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\nachievingmaximalconﬁdenceonlywhenbothloopsofthe8arepresent.\ndiﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\nfunctiontobereusedseveraltimes.\nSpeciﬁcally,bagginginvolvesconstructing kdiﬀerentdatasets.Eachdataset\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\ni.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultin\ndiﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample.7.5\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\noftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesame\ndataset.Diﬀerencesinrandominitialization, randomselectionofminibatches,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "diﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-determinis ticimple-\nmentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersofthe\n2 5 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nensembletomakepartiallyindependenterrors.\nModelaveragingisanextremelypowerfulandreliablemethodforreducing\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\nforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\nForthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\nMachinelearningcontestsareusuallywonbymethodsusingmodelaverag-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "ingoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrand\nPrize(Koren2009,).\nNotalltechniquesforconstructingensemblesaredesignedtomaketheensemble\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\nunitstotheneuralnetwork.\n7.12Dropout\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\npowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "wheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\nofﬁvetotenneuralnetworks— ()usedsixtowintheILSVRC— Szegedy etal.2014a\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\nneuralnetworks.\nSpeciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthat\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "asillustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\naﬃnetransformationsandnonlinearities, wecaneﬀectivelyremoveaunitfroma\nnetworkbymultiplyingitsoutputvaluebyzero. Thisprocedurerequiressome\nslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake\n2 5 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresent\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\nbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthe\nnetwork.\nRecallthattolearnwithbagging,wedeﬁne kdiﬀerentmodels,construct k\ndiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "exponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\nrandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhidden\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\nincluded)isahyperparameter ﬁxedbeforetrainingbegins. Itisnotafunction",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "ofthecurrentvalueofthemodelparametersortheinputexample.Typically,\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\nwithdropout.\nMoreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,\nand J(θµ ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.\nThendropouttrainingconsistsinminimizing E µ J(θµ ,).Theexpectationcontains",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "exponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\nbysamplingvaluesof.µ\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\nparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromthe\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "caseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\naretheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "theoriginaltrainingsetsampledwithreplacement.\n2 5 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1yy\nh 2 h 2\nx 1 x 1 x 2 x 2\nyy\nh 1 h 1\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2yy\nx 1 x 1 x 2 x 2yy\nh 2 h 2\nx 2 x 2\nyy\nh 1 h 1\nx 1 x 1yy\nh 1 h 1\nx 2 x 2yy\nh 2 h 2\nx 1 x 1yy\nx 1 x 1\nyy\nx 2 x 2yy\nh 2 h 2yy\nh 1 h 1yyB ase   ne t w or k\nE nse m bl e   of   s u b n e t w or k s",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "E nse m bl e   of   s u b n e t w or k s\nFigure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbe\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\nbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "theinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwider\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\nsmaller.\n2 6 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nˆ x 1ˆ x 1\nµ x 1 µ x 1 x 1 x 1ˆ x 2ˆ x 2\nx 2 x 2 µ x 2 µ x 2h 1 h 1 h 2 h 2µ h 1 µ h 1 µ h 2 µ h 2ˆ h 1ˆ h 1ˆ h 2ˆ h 2yyyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\npropagationwithdropout,werandomlysampleavectorµwithoneentryforeachinput",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "orhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependently\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\nﬁgureandrunningforwardpropagationthroughit. 7.6\n2 6 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\nitsmembers.Werefertothisprocessasinferenceinthiscontext. Sofar,our\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\nprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobability\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\nofthesedistributions,\n1\nkk",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "ofthesedistributions,\n1\nkk\ni = 1p( ) i( ) y|x . (7.52)\nInthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-\nabilitydistribution p( y ,|xµ).Thearithmeticmeanoverallmasksisgiven\nby\nµp p y , ()µ(|xµ) (7.53)\nwhere p(µ)istheprobabilitydistributionthatwasusedtosampleµattraining\ntime.\nBecausethissumincludesanexponentialnumberofterms,itisintractable\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\nofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractable",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "simpliﬁcation.Instead, wecan approximatetheinferencewithsampling, by\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\nsuﬃcienttoobtaingoodperformance.\nHowever,thereisanevenbetterapproach,thatallowsustoobtainagood\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\nthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "Farley2014etal.()presentargumentsandempiricalevidencethatthegeometric\nmeanperformscomparablytothearithmeticmeaninthiscontext.\nThegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\ndistributiondeﬁneddirectlybythegeometricmeanisgivenby",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "˜ p e nse m bl e( ) = y|x 2d\nµp y , (|xµ) (7.54)\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\ndistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare\n2 6 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\np e nse m bl e( ) = y|x˜ p e nse m bl e( ) y|x\ny˜ p e nse m bl e( y|x). (7.55)\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\ni.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueofthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "outputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.\nThereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\nBecauseweusuallyuseaninclusionprobabilityof1\n2,theweightscalingrule\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\nmissingonaverage.\nFormanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\nclassiﬁerwithinputvariablesrepresentedbythevector: n v\nP y (= y | v) = softmax\nWv+b\ny. (7.56)\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\ninputwithabinaryvector: d",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "inputwithabinaryvector: d\nP y (= y | v;) = dsoftmax\nW( )+d vb\ny.(7.57)\nTheensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverall\nensemblemembers’predictions:\nP e nse m bl e(= ) =y y| v˜ P e nse m bl e(= )y y| v\ny˜ P e nse m bl e(= y y| v)(7.58)\nwhere\n˜ P e nse m bl e(= ) =y y| v2n\nd∈{} 0 1 ,nP y . (= y | v;)d (7.59)\n2 6 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nToseethattheweightscalingruleisexact,wecansimplify ˜ P e nse m bl e:\n˜ P e nse m bl e(= ) =y y| v2n\nd∈{} 0 1 ,nP y (= y | v;)d(7.60)\n= 2n\nd∈{} 0 1 ,nsoftmax (W( )+)d vby (7.61)\n= 2n\nd∈{} 0 1 ,nexp\nWy , :( )+d v b y\n\nyexp\nW\ny , :( )+d v b y (7.62)\n=2n\nd∈{} 0 1 ,nexp\nWy , :( )+d v b y\n2n\nd∈{} 0 1 ,n\nyexp\nW\ny , :( )+d v b y(7.63)\nBecause˜ Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "areconstantwithrespectto: y\n˜ P e nse m bl e(= ) y y| v∝2n\nd∈{} 0 1 ,nexp\nWy , :( )+d v b y\n(7.64)\n= exp\n1\n2n\nd∈{} 0 1 ,nW\ny , :( )+d v b y\n (7.65)\n= exp1\n2W\ny , : v+ b y\n. (7.66)\nSubstitutingthisbackintoequationweobtainasoftmaxclassiﬁerwithweights 7.58\n1\n2W.\nTheweightscalingruleisalsoexactinothersettings,includingregression\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "mationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\nbetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximations tothe\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\nthatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand\n2 6 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\napproximationisproblem-dependent.\nSrivastava2014etal.()showedthatdropoutismoreeﬀectivethanother\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,ﬁlter\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\nwithotherformsofregularizationtoyieldafurtherimprovement.\nOneadvantageofdropoutisthatitisverycomputationally cheap.Using",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "dropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\nexamples.\nAnothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimit",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "thetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\nthearchitectureofthemodel.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "thearchitectureofthemodel.\nThoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,\nthecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropout\nisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀset\nthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "datasets,regularizationconferslittlereductioningeneralization error. Inthese\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\nthebeneﬁtofregularization.\nWhenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\neﬀective.Bayesian neuralnetworks(, )outperform dropout onthe Neal1996\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedfeaturelearningcangainanadvantageoverdropout.\nWager2013etal.()showedthat,whenappliedtolinearregression,dropout\nisequivalentto L2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor\n2 6 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\neachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientis\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\nmodels,dropoutisnotequivalenttoweightdecay.\nThestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\napproach’ssuccess.Itisjustameansofapproximating thesumoverallsub-\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\nmarginalization. Theirapproximation,knownasfastdropoutresultedinfaster",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "convergencetimeduetothereducedstochasticityinthecomputationofthe\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\n(butalsomorecomputationally expensive)approximation totheaverageoverall\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\nproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoa\nlargeproblem.\nJustasstochasticityisnotnecessarytoachievetheregularizing eﬀect of",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "dropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\nitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointly\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\ndropoutisanalogoustobagging, this approachisanalogoustoboosting.As\nintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀect",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "comparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\ndropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleis\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\nperformwellindependently ofeachother.\nDropouthasinspiredotherstochasticapproachestotrainingexponentially\nlargeensemblesofmodelsthatshareweights. DropConnectisaspecialcaseof",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "dropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerent\nspatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidely\nusedimplicitensemblemethod.\nOneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "behaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\nimplementsaformofbaggingwithparametersharing.Earlier, wedescribed\n2 6 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ndropoutas bagginganensembleofmodelsformedbyincludingor excluding\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\ninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.\nInpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareable\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\napproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrized",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "byavectorµastraininganensembleconsistingof p( y ,|xµ)forallpossible\nvaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.For\nexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\nweightsbyµ∼N( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\nE[µ] = 1thestandardnetworkautomatically implementsapproximate inference\nintheensemble,withoutneedinganyweightscaling.\nSofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "approximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.\n()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\nswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressurefor",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "genestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerent\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts. Warde-\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\nconcludedthatdropoutoﬀersadditionalimprovementstogeneralization error",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "beyondthoseobtainedbyensemblesofindependentmodels.\nItisimportanttounderstandthatalargeportionofthepowerofdropout\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\nexample,ifthemodellearnsahiddenunit h ithatdetectsafacebyﬁndingthenose,\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "theimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.\nTraditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\n2 6 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\ndistributionthatthemodelhasacquiredsofar.\nAnotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\nnoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunit h iwith\naddednoise couldsimplylearntohave h ibecomeverylargeinordertomake\ntheaddednoise insigniﬁcantbycomparison.Multiplicativenoisedoesnotallow",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "suchapathologicalsolutiontothenoiserobustnessproblem.\nAnotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\noptimization, butthenoisecanhavearegularizingeﬀect,andsometimesmakes\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\n7.13AdversarialTraining\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "evaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\nsearchforexamplesthatthemodelmisclassiﬁes. ()foundthat Szegedy etal.2014b\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\nproceduretosearchforaninputxnearadatapointxsuchthatthemodel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "outputisverydiﬀerentatx.Inmanycases,xcanbesosimilartoxthata\nhumanobservercannottellthediﬀerencebetweentheoriginalexampleandthe\nadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.See\nﬁgureforanexample.7.8\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\nthatarebeyondthescopeofthischapter. However,theyareinterestinginthe\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "testsetviaadversarialtraining—trainingonadversariallyperturbedexamples\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,).\nGoodfellow2014betal.()showedthatoneoftheprimarycausesofthese\nadversarial examplesis excessive linearity.Neural networks arebuilt out of\nprimarilylinearbuildingblocks. Insomeexperimentstheoverallfunctionthey\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\n2 6 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n+ .007× =\nx sign(∇ x J(θx , , y))x+\nsign(∇ x J(θx , , y))\ny=“panda” “nematode”“gibbon”\nw/57.7%\nconﬁdencew/8.2%\nconﬁdencew/99.3%\nconﬁdence\nFigure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\nrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproduced",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "withpermissionfrom (). Goodfellow e t a l .2014b\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\nifithasnumerousinputs.Ifwechangeeachinputby ,thenalinearfunction\nwithweightswcanchangebyasmuchas ||||w 1,whichcanbeaverylarge\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "introducingalocalconstancypriorintosupervisedneuralnets.\nAdversarialtraininghelpstoillustratethepowerofusingalargefunction\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\nfromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapture\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\nmodelitselfassignssomelabel ˆ y.Themodel’slabel ˆ ymaynotbethetruelabel,\nbutifthemodelishighquality,thenˆ yhasahighprobabilityofprovidingthe\ntruelabel.Wecanseekanadversarialexamplexthatcausestheclassiﬁerto\noutputalabel ywith y=ˆ y.Adversarialexamplesgeneratedusingnotthetrue\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "examples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthe\nsamelabeltoxandx.Thisencouragestheclassiﬁertolearnafunctionthatis\n2 6 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\nlies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylie\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\nfromoneclassmanifoldtoanotherclassmanifold.\n7.14Tangent Distance, TangentProp,and Manifold\nTangentClassiﬁer\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "section.5.11.3\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁer\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "onthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\nrespectivelybelong.Althoughthatmaybecomputationally diﬃcult(itwould\nrequiresolvinganoptimization problem,toﬁndthenearestpairofpointson M 1\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "linearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\nonetospecifythetangentvectors.\nInarelatedspirit,thetangentpropalgorithm( ,)(ﬁgure) Simardetal.19927.9\ntrainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutput f(x)of\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\nsameclassconcentrate.Localinvarianceisachievedbyrequiring ∇ x f(x)tobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "orthogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\nregularizationpenalty:Ω\nΩ() = f\ni\n(∇ x f())xv( ) i2\n. (7.67)\n2 7 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\ntheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.\nTangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "butalsointhecontextofreinforcementlearning(,). Thrun1995\nTangentpropagation is closelyrelated todataset augmentation.In both\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\nnetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation, thenetworkis\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\nmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "doesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\nthe speciﬁed transformation.While thisanalytical approac h isintellectually\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\ninﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\nlargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodels",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "basedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivatives\nbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheir\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\nunitscan.Datasetaugmentation workswellwithrectiﬁedlinearunitsbecause\ndiﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsof\neachoriginalinput.\nTangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\nﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\naugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthe\nmodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.\nDoublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "invarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\nasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,\nadversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.\nThemanifoldtangentclassiﬁer(,),eliminatestheneedto Rifaietal.2011c\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\n2 7 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nx 1x 2N o r m a lT a ng e nt\nFigure7.9: Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l .\n1992 Rifai2011c )andmanifoldtangentclassiﬁer( e t a l .,),whichbothregularizethe\nclassiﬁeroutputfunction f(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\nOnonecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "classmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\ntangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctionto\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\nclassiﬁerregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "propagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\nmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirections\nbytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimate\nmanifoldswillbedescribedinchapter.14\nestimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuse\nofthistechniquetoavoidneedinguser-speciﬁedtangentvectors. Asillustrated",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "inﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\nandincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchas\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁer\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁer\nasintangentprop(equation).7.67",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "asintangentprop(equation).7.67\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\n2 7 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\nthemeofmachinelearningisoptimization, describednext.\n2 7 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "P a rt I I I\nDeepLearningResearch\n486",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes\nt o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y .\nIn t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d\nlearning problems — how t o learn t o map one v e c t or t o another, given e nough\ne x amples of t he mapping.\nN ot all problems w e might w ant t o s olve f all in t o t his c ategory . W e ma y",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "wis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle\nmis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples\nf r om r e lated t as k s . A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial\napplications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d\ndata t o ac hieve go o d accuracy . In t his part of t he b o ok, w e dis c us s s ome of",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "t he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary\nf or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of\nt as k s . A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or\ns e mi-s up e r v is e d learning.\nMan y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d\nlearning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "deep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of\nt as k s . In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d\nlearning and s ome of t he p opular t hought ab out how w e c an make progres s in t his\nﬁ e ld.\nA c e ntral c aus e of t he diﬃculties with uns upervis e d learning is t he high di-\nmens iona lit y of t he r andom v ariables being mo deled. This brings t wo dis t inct",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "c hallenges : a s t atis t ical c hallenge and a c omputational c hallenge. The s t a t i s t i c a l\nc h a l l e ng e r e gards generalization: t he num b e r of c onﬁgurations we may wan t t o\ndis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and\nt his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "ha v e ( or us e with b ounded c omputational r e s ources ) . The c o m p u t a t i o na l c h a l l e ng e\nas s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or\nlearning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit\nprobabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially\nwith t he n um b e r of dimens ion s .",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "with t he n um b e r of dimens ion s .\nWith probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o\np e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on.\n• I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter . It r e gards 19\nt he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "v ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r\n4 8 7",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "a , b and c . In order t o e v e n c ompute s uch c onditional probabilities one needs\nt o s um ov e r t he v alues of t he v ariables c , as well as c ompute a normalization\nc ons t an t whic h s ums o v e r t he v alues of a and c .\n• I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition\nf unction is dis c us s e d mos t ly in c hapter . N ormalizing c ons t ants of proba- 18",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "bilit y f unctions c ome up in inference ( ab o v e ) as well as in learning. Man y\nprobabilis t ic mo dels in v olve s uc h a normalizing c ons t ant. U nfortun ately ,\nlearning s uc h a mo del often r e q uires c omputing t he gradient of t he loga-\nr ithm of t he partition f unction with r e s p e c t t o t he mo del parameters . That\nc omputation is generally as intractable as c omputing t he partition f unction\nits e lf. Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "t e n us e d t o deal with t he partition f unction ( c omputing it or its gradient).\nU nfortun ately , MCMC metho ds s uﬀer when t he mo des of t he mo del dis t r ibu-\nt ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces\n( s e c t ion ) . 17.5\nOne wa y t o c onfront t hes e intractable c omputations is t o approximate t hem,\nand many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "b o ok. A nother interes t in g w ay , als o dis c us s e d here, w ould b e t o av oid t hes e\nin t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire\ns uc h c omputations are t hus v e r y app e aling. Several generativ e mo dels ha v e b e e n\nprop os e d in r e c e nt y e ars , with t hat motiv ation. A wide v ariety of c ontemporary\napproac hes t o generativ e mo deling are dis c us s e d in c hapter . 20",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "P art is t he mos t imp ortant f or a r e s e arc her—s om e one who wan t s t o un- I I I\nders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he ﬁ e ld of deep\nlearning, and pus h t he ﬁ e ld f orward t ow ards t r ue artiﬁcial intelligence.\n4 8 8",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  }
]